{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Evaluation - Modular Version\n",
    "\n",
    "**Purpose**: Evaluate trained GRPO checkpoints with proper optimization direction handling.\n",
    "\n",
    "**Key Features**:\n",
    "- ‚úÖ **Checkpoint-first approach** - load any checkpoint to evaluate\n",
    "- ‚úÖ **Auto-detect optimization** - reads direction from checkpoint metadata\n",
    "- ‚úÖ **Independent cells** - no need to run training first\n",
    "- ‚úÖ **Correct metrics** - handles both minimization and maximization\n",
    "- ‚úÖ **Multiple modes** - single checkpoint, compare checkpoints, compare objectives\n",
    "\n",
    "**Workflow**:\n",
    "1. Select evaluation mode and checkpoints\n",
    "2. Load checkpoint(s) and validate metadata\n",
    "3. Generate or load test SCMs\n",
    "4. Run evaluation with baselines\n",
    "5. Generate visualizations with correct labels\n",
    "6. Export results for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-07-28 16:33:05,379:jax._src.xla_bridge:749: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "[2025-07-28 16:33:05,379][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Setup Complete\n",
      "üìÅ Project root: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt\n",
      "üîß JAX devices: [CpuDevice(id=0)]\n",
      "üìÖ Date: 2025-07-28 16:33:05\n",
      "\n",
      "üìÅ Checkpoint directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cell 1: Import base components and configure environment\n",
    "\n",
    "This cell sets up the evaluation environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"experiments\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import base components\n",
    "from scripts.notebooks.base_components import (\n",
    "    NotebookError, CheckpointManager, SCMGenerator,\n",
    "    OptimizationConfig, CheckpointMetadata, validate_environment,\n",
    "    format_results_summary\n",
    ")\n",
    "from scripts.notebooks.config_templates import create_evaluation_config\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import pyrsistent as pyr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Validate environment\n",
    "try:\n",
    "    env_info = validate_environment()\n",
    "    print(\"‚úÖ Environment Setup Complete\")\n",
    "    print(f\"üìÅ Project root: {project_root}\")\n",
    "    print(f\"üîß JAX devices: {env_info['jax_devices']}\")\n",
    "    print(f\"üìÖ Date: {env_info['timestamp']}\")\n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Environment validation failed: {e}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_dir = project_root / \"checkpoints\" / \"grpo_training\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_dir)\n",
    "print(f\"\\nüìÅ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Evaluation Configuration\n",
      "==================================================\n",
      "Mode: PHASE2_ACTIVE_LEARNING\n",
      "Test SCMs: 10\n",
      "Runs per method: 3\n",
      "Intervention budget: 50\n",
      "Random seed: 42\n",
      "Learning rate: 0.001\n",
      "Observation samples: 30\n",
      "\n",
      "üìö Phase 2 combines:\n",
      "  ‚Ä¢ GRPO policy (guides interventions)\n",
      "  ‚Ä¢ Active learning (discovers structure)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 2: Configure evaluation parameters\n",
    "\n",
    "Set evaluation mode and parameters for the run.\n",
    "\"\"\"\n",
    "\n",
    "# EVALUATION MODE SELECTION\n",
    "EVALUATION_MODE = \"PHASE2_ACTIVE_LEARNING\"  # Options: \"SINGLE_CHECKPOINT\", \"COMPARE_CHECKPOINTS\", \"COMPARE_OBJECTIVES\", \"PHASE2_ACTIVE_LEARNING\"\n",
    "\n",
    "# Configuration for different modes\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Evaluate one checkpoint against baselines\n",
    "    NUM_TEST_SCMS = 10\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Compare multiple checkpoints\n",
    "    COMPARISON_COUNT = 3  # Number of checkpoints to compare\n",
    "    NUM_TEST_SCMS = 5  # Fewer SCMs for faster comparison\n",
    "    RUNS_PER_METHOD = 2\n",
    "    INTERVENTION_BUDGET = 8\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Compare minimization vs maximization\n",
    "    NUM_TEST_SCMS = 8\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "\n",
    "elif EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "    # Phase 2: Use GRPO policy with active learning surrogate\n",
    "    NUM_TEST_SCMS = 10\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 50  # More interventions for structure learning\n",
    "    LEARNING_RATE = 1e-3  # For active surrogate\n",
    "    OBSERVATION_SAMPLES = 30  # Initial observational data\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# General settings\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"üéØ Evaluation Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "if EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    print(f\"Checkpoints to compare: {COMPARISON_COUNT}\")\n",
    "elif EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"Observation samples: {OBSERVATION_SAMPLES}\")\n",
    "    print(f\"\\nüìö Phase 2 combines:\")\n",
    "    print(f\"  ‚Ä¢ GRPO policy (guides interventions)\")\n",
    "    print(f\"  ‚Ä¢ Active learning (discovers structure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Mode and Checkpoint Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Intelligent Checkpoint Selection\n",
      "==================================================\n",
      "Found 4 total checkpoints\n",
      "Usable checkpoints:\n",
      "  MINIMIZE: 4\n",
      "  MAXIMIZE: 0\n",
      "  Total usable: 4\n",
      "üéØ Selected checkpoint for Phase 2: grpo_full_minimize_20250728_163157\n",
      "  This GRPO policy will guide active learning\n",
      "  ‚úÖ Checkpoint is valid and ready for Phase 2\n",
      "\n",
      "‚úÖ Final Selection (1 checkpoint(s)):\n",
      "1. grpo_full_minimize_20250728_163157\n",
      "     Optimization: MINIMIZE\n",
      "     Training mode: FULL\n",
      "     Path: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_full_minimize_20250728_163157\n",
      "     Status: ‚úÖ Ready for evaluation\n",
      "\n",
      "üöÄ All selected checkpoints validated and ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Select checkpoints for evaluation\n",
    "\n",
    "This cell handles checkpoint selection based on the evaluation mode.\n",
    "Uses intelligent discovery instead of hardcoded names.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Intelligent Checkpoint Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get available checkpoints\n",
    "try:\n",
    "    available_checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    if not available_checkpoints:\n",
    "        raise NotebookError(\"No checkpoints found\")\n",
    "        \n",
    "    print(f\"Found {len(available_checkpoints)} total checkpoints\")\n",
    "    \n",
    "    # Show usable checkpoints by direction\n",
    "    usable_minimize = checkpoint_manager.find_usable_checkpoints('MINIMIZE')\n",
    "    usable_maximize = checkpoint_manager.find_usable_checkpoints('MAXIMIZE')\n",
    "    \n",
    "    print(f\"Usable checkpoints:\")\n",
    "    print(f\"  MINIMIZE: {len(usable_minimize)}\")\n",
    "    print(f\"  MAXIMIZE: {len(usable_maximize)}\")\n",
    "    print(f\"  Total usable: {len(usable_minimize) + len(usable_maximize)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to analyze checkpoints: {e}\")\n",
    "\n",
    "# SELECT CHECKPOINTS BASED ON MODE\n",
    "selected_checkpoints = []\n",
    "\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Find best MINIMIZE checkpoint (preferred for comparison with PARENT_SCALE)\n",
    "    best_checkpoint = checkpoint_manager.find_best_checkpoint({\n",
    "        'optimization_direction': 'MINIMIZE',\n",
    "        'training_mode': 'FULL'\n",
    "    })\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        selected_checkpoints = [best_checkpoint]\n",
    "        print(f\"üéØ Selected best MINIMIZE checkpoint: {best_checkpoint.name}\")\n",
    "        \n",
    "        # Validate the selected checkpoint\n",
    "        validation = checkpoint_manager.validate_checkpoint(best_checkpoint)\n",
    "        if validation['is_valid']:\n",
    "            print(f\"  ‚úÖ Checkpoint is valid and ready for evaluation\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Checkpoint issues: {validation['issues']}\")\n",
    "    else:\n",
    "        # Fallback: try any usable checkpoint\n",
    "        all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "        if all_usable:\n",
    "            selected_checkpoints = [all_usable[0]]\n",
    "            print(f\"üéØ No MINIMIZE checkpoint found, using: {all_usable[0].name}\")\n",
    "            print(f\"  Optimization: {all_usable[0].optimization_config.direction}\")\n",
    "        else:\n",
    "            raise NotebookError(\"No usable checkpoints found. Please ensure checkpoints have both metadata.json and checkpoint.pkl files.\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Get multiple usable checkpoints\n",
    "    all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "    comparison_count = min(COMPARISON_COUNT, len(all_usable))\n",
    "    selected_checkpoints = all_usable[:comparison_count]\n",
    "    print(f\"üìä Selected {comparison_count} checkpoints for comparison\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Get best from each optimization direction\n",
    "    best_minimize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MINIMIZE'})\n",
    "    best_maximize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MAXIMIZE'})\n",
    "    \n",
    "    selected_checkpoints = []\n",
    "    if best_minimize:\n",
    "        selected_checkpoints.append(best_minimize)\n",
    "    if best_maximize:\n",
    "        selected_checkpoints.append(best_maximize)\n",
    "    \n",
    "    if not selected_checkpoints:\n",
    "        raise NotebookError(\"Need checkpoints from both MINIMIZE and MAXIMIZE directions for objective comparison\")\n",
    "    \n",
    "    print(f\"üîÑ Selected checkpoints for objective comparison:\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        print(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\")\n",
    "\n",
    "elif EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "    # Phase 2: Select best GRPO checkpoint to use with active learning\n",
    "    best_checkpoint = checkpoint_manager.find_best_checkpoint({\n",
    "        'optimization_direction': 'MINIMIZE',\n",
    "        'training_mode': 'FULL'\n",
    "    })\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        selected_checkpoints = [best_checkpoint]\n",
    "        print(f\"üéØ Selected checkpoint for Phase 2: {best_checkpoint.name}\")\n",
    "        print(f\"  This GRPO policy will guide active learning\")\n",
    "        \n",
    "        # Validate the selected checkpoint\n",
    "        validation = checkpoint_manager.validate_checkpoint(best_checkpoint)\n",
    "        if validation['is_valid']:\n",
    "            print(f\"  ‚úÖ Checkpoint is valid and ready for Phase 2\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Checkpoint issues: {validation['issues']}\")\n",
    "    else:\n",
    "        # Fallback: try any usable checkpoint\n",
    "        all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "        if all_usable:\n",
    "            selected_checkpoints = [all_usable[0]]\n",
    "            print(f\"üéØ Using checkpoint for Phase 2: {all_usable[0].name}\")\n",
    "            print(f\"  Optimization: {all_usable[0].optimization_config.direction}\")\n",
    "        else:\n",
    "            raise NotebookError(\"No usable checkpoints found for Phase 2. Please train a GRPO policy first.\")\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# Final validation\n",
    "if not selected_checkpoints:\n",
    "    raise NotebookError(\"No checkpoints selected for evaluation\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final Selection ({len(selected_checkpoints)} checkpoint(s)):\")\n",
    "for i, checkpoint in enumerate(selected_checkpoints, 1):\n",
    "    print(f\"{i}. {checkpoint.name}\")\n",
    "    print(f\"     Optimization: {checkpoint.optimization_config.direction}\")\n",
    "    print(f\"     Training mode: {checkpoint.training_config.get('mode', 'unknown')}\")\n",
    "    print(f\"     Path: {checkpoint.path}\")\n",
    "    \n",
    "    # Final validation\n",
    "    validation = checkpoint_manager.validate_checkpoint(checkpoint)\n",
    "    if validation['is_valid']:\n",
    "        print(f\"     Status: ‚úÖ Ready for evaluation\")\n",
    "    else:\n",
    "        print(f\"     Status: ‚ùå Issues found: {validation['issues']}\")\n",
    "        raise NotebookError(f\"Selected checkpoint {checkpoint.name} has validation issues: {validation['issues']}\")\n",
    "\n",
    "print(f\"\\nüöÄ All selected checkpoints validated and ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Checkpoint Metadata\n",
      "============================================================\n",
      "\n",
      "Loading: grpo_full_minimize_20250728_163157\n",
      "  ‚úì Optimization: MINIMIZE\n",
      "  ‚úì Training mode: FULL\n",
      "  ‚úì Episodes completed: 512\n",
      "  ‚úì Duration: 44.5 minutes\n",
      "  ‚úì Reward weights: opt=0.8, struct=0.1, eff=0.1\n",
      "\n",
      "‚úÖ Loaded 1 checkpoint(s) successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 3: Load checkpoint metadata and validate\n",
    "\n",
    "This cell loads the selected checkpoints and validates their metadata.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• Loading Checkpoint Metadata\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store loaded checkpoint info\n",
    "loaded_checkpoints = {}\n",
    "\n",
    "for ckpt in selected_checkpoints:\n",
    "    print(f\"\\nLoading: {ckpt.name}\")\n",
    "    try:\n",
    "        # For now, we're using the metadata we already have\n",
    "        # In production, this would load the actual model parameters\n",
    "        loaded_checkpoints[ckpt.name] = {\n",
    "            'metadata': ckpt,\n",
    "            'optimization_config': ckpt.optimization_config,\n",
    "            'training_config': ckpt.training_config,\n",
    "            'model_params': None  # TODO: Load actual model parameters\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Optimization: {ckpt.optimization_config.direction}\")\n",
    "        print(f\"  ‚úì Training mode: {ckpt.training_config.get('mode', 'unknown')}\")\n",
    "        print(f\"  ‚úì Episodes completed: {ckpt.training_results.get('episodes_completed', 'unknown')}\")\n",
    "        print(f\"  ‚úì Duration: {ckpt.training_results.get('duration_minutes', 0):.1f} minutes\")\n",
    "        \n",
    "        # Show reward weights if available\n",
    "        if 'reward_weights' in ckpt.training_config:\n",
    "            weights = ckpt.training_config['reward_weights']\n",
    "            print(f\"  ‚úì Reward weights: opt={weights.get('optimization', 0):.1f}, \"\n",
    "                  f\"struct={weights.get('discovery', 0):.1f}, \"\n",
    "                  f\"eff={weights.get('efficiency', 0):.1f}\")\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed to load: {e}\")\n",
    "        raise NotebookError(f\"Failed to load checkpoint {ckpt.name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(loaded_checkpoints)} checkpoint(s) successfully\")\n",
    "\n",
    "# Check optimization compatibility for comparison modes\n",
    "if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    directions = [ckpt.optimization_config.direction for ckpt in selected_checkpoints]\n",
    "    if len(set(directions)) == 1:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: All checkpoints have same optimization direction: {directions[0]}\")\n",
    "        print(\"   Objective comparison may not be meaningful.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Comparing optimization directions: {set(directions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Test SCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Generating Test SCMs\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 16:33:05,831][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-28 16:33:05,832][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-28 16:33:05,847][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-28 16:33:05,848][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-28 16:33:05,864][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "[2025-07-28 16:33:05,864][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "[2025-07-28 16:33:05,867][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "[2025-07-28 16:33:05,867][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "[2025-07-28 16:33:05,882][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "[2025-07-28 16:33:05,882][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "[2025-07-28 16:33:05,884][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "[2025-07-28 16:33:05,884][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "[2025-07-28 16:33:05,886][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "[2025-07-28 16:33:05,887][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "[2025-07-28 16:33:05,888][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "[2025-07-28 16:33:05,889][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "[2025-07-28 16:33:05,890][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-28 16:33:05,890][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-28 16:33:05,892][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-28 16:33:05,892][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-28 16:33:05,892][scripts.notebooks.base_components][INFO] - Generated 10 SCMs\n",
      "[2025-07-28 16:33:05,893][scripts.notebooks.base_components][INFO] - Distribution: {'structure_types': {'fork': 4, 'chain': 4, 'collider': 2}, 'variable_counts': {3: 3, 4: 3, 5: 2, 6: 2}, 'total': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Generated 10 test SCMs\n",
      "\n",
      "üìä Test Set Distribution:\n",
      "  Structure types: {'fork': 4, 'chain': 4, 'collider': 2}\n",
      "  Variable counts: {3: 3, 4: 3, 5: 2, 6: 2}\n",
      "\n",
      "üíæ Saved test SCM metadata to: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/test_scms/test_scms_1042.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Generate test SCMs for evaluation\n",
    "\n",
    "Create a balanced set of test SCMs different from training.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî¨ Generating Test SCMs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SCM generator\n",
    "scm_generator = SCMGenerator()\n",
    "\n",
    "# Generate test SCMs with different seed than training\n",
    "test_seed = RANDOM_SEED + 1000  # Ensure different from training\n",
    "\n",
    "try:\n",
    "    test_scms, test_metadata = scm_generator.generate_balanced_scms(\n",
    "        num_scms=NUM_TEST_SCMS,\n",
    "        variable_range=(3, 6),\n",
    "        structure_types=['fork', 'chain', 'collider', 'mixed'],\n",
    "        seed=test_seed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(test_scms)} test SCMs\")\n",
    "    \n",
    "    # Analyze distribution\n",
    "    distribution = scm_generator._summarize_distribution(test_metadata)\n",
    "    print(f\"\\nüìä Test Set Distribution:\")\n",
    "    print(f\"  Structure types: {distribution['structure_types']}\")\n",
    "    print(f\"  Variable counts: {distribution['variable_counts']}\")\n",
    "    \n",
    "    # Save test SCM metadata\n",
    "    test_scm_path = project_root / \"results\" / \"test_scms\" / f\"test_scms_{test_seed}.json\"\n",
    "    test_scm_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(test_scm_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'metadata': test_metadata,\n",
    "            'config': {\n",
    "                'num_scms': len(test_scms),\n",
    "                'seed': test_seed,\n",
    "                'variable_range': [3, 6],\n",
    "                'structure_types': ['fork', 'chain', 'collider', 'mixed']\n",
    "            },\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved test SCM metadata to: {test_scm_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to generate test SCMs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Running Evaluation\n",
      "============================================================\n",
      "üìÅ Output directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_phase2_active_learning_20250728_163305\n",
      "\n",
      "Evaluation parameters:\n",
      "  Mode: PHASE2_ACTIVE_LEARNING\n",
      "  Test SCMs: 10\n",
      "  Runs per method: 3\n",
      "  Intervention budget: 50\n",
      "\n",
      "üöÄ Running Phase 2: Active Learning with GRPO Policy\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 16:33:08,126][src.causal_bayes_opt.training.grpo_policy_loader][INFO] - Loaded variable-agnostic GRPO policy\n",
      "[2025-07-28 16:33:08,126][src.causal_bayes_opt.training.grpo_policy_loader][INFO] - Loaded GRPO policy from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_full_minimize_20250728_163157/policy_params.pkl\n",
      "[2025-07-28 16:33:08,127][src.causal_bayes_opt.training.grpo_policy_loader][INFO] - Policy config: {'architecture': {'hidden_dim': 128, 'num_layers': 2, 'num_heads': 4, 'key_size': 32, 'widening_factor': 4, 'dropout': 0.1, 'policy_intermediate_dim': None, 'variable_agnostic': True}, 'state_config': {'max_history_size': 100, 'num_channels': 5, 'standardize_values': True, 'include_temporal_features': True, 'use_global_standardization': True}, 'grpo_config': {'group_size': 64, 'interventions_per_state': 8, 'clip_ratio': 0.2, 'entropy_coeff': 0.1, 'kl_penalty_coeff': 0.0, 'max_grad_norm': 1.0, 'scale_rewards': True}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading GRPO policy from: grpo_full_minimize_20250728_163157\n",
      "‚úÖ Loaded GRPO policy successfully\n",
      "  Variables: None\n",
      "  Target: None\n",
      "\n",
      "========================================\n",
      "SCM 1/10\n",
      "  Run 1/3... üß† Progressive Learning Demo\n",
      "============================================================\n",
      "üß† Using learnable surrogate model\n",
      "üéØ Using pretrained acquisition policy\n",
      "\n",
      "‚ùå Phase 2 evaluation failed: Unable to retrieve parameter 'w' for module 'EnrichedAcquisitionPolicyNetwork/EnrichedAttentionEncoder/~_project_enriched_input_with_roles/RoleBasedProjection/target_projection' All parameters must be created as part of `init`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/2f/7z7glsfj1fd22nlr6wj56z5w0000gn/T/ipykernel_79305/1762222707.py\", line 85, in <module>\n",
      "    result = run_progressive_learning_demo_with_scm(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/examples/complete_workflow_demo.py\", line 185, in run_progressive_learning_demo_with_scm\n",
      "    intervention = intervention_fn(key=keys[step])\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/training/grpo_policy_loader.py\", line 257, in select_intervention\n",
      "    policy_output = loaded_policy.apply_fn(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/transform.py\", line 183, in apply_fn\n",
      "    out, state = f.apply(params, None, *args, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/transform.py\", line 456, in apply_fn\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/training/grpo_policy_loader.py\", line 127, in policy_fn\n",
      "    return policy(\n",
      "           ^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 464, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 305, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/policy_heads.py\", line 306, in __call__\n",
      "    variable_embeddings = encoder(\n",
      "                          ^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 464, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 305, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/enriched_policy.py\", line 75, in __call__\n",
      "    x = self._project_enriched_input_with_roles(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 464, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 305, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/enriched_policy.py\", line 152, in _project_enriched_input_with_roles\n",
      "    x = role_projection(enriched_history, target_mask, intervention_mask)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 464, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 305, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/role_based_projection.py\", line 98, in __call__\n",
      "    proj_target = target_projection(flat_history)  # [T*n_vars, hidden_dim]\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 464, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py\", line 305, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/basic.py\", line 178, in __call__\n",
      "    w = hk.get_parameter(\"w\", [input_size, output_size], dtype, init=w_init)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/harellidar/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/base.py\", line 675, in get_parameter\n",
      "    raise param_missing_error\n",
      "ValueError: Unable to retrieve parameter 'w' for module 'EnrichedAcquisitionPolicyNetwork/EnrichedAttentionEncoder/~_project_enriched_input_with_roles/RoleBasedProjection/target_projection' All parameters must be created as part of `init`.\n"
     ]
    },
    {
     "ename": "NotebookError",
     "evalue": "Phase 2 evaluation failed: Unable to retrieve parameter 'w' for module 'EnrichedAcquisitionPolicyNetwork/EnrichedAttentionEncoder/~_project_enriched_input_with_roles/RoleBasedProjection/target_projection' All parameters must be created as part of `init`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Run Phase 2: GRPO policy + active learning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m result = \u001b[43mrun_progressive_learning_demo_with_scm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mphase2_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_surrogate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Active learning from scratch\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_acquisition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrpo_intervention_fn\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GRPO policy\u001b[39;49;00m\n\u001b[32m     90\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m scm_results.append(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/examples/complete_workflow_demo.py:185\u001b[39m, in \u001b[36mrun_progressive_learning_demo_with_scm\u001b[39m\u001b[34m(scm, config, pretrained_surrogate, pretrained_acquisition)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Execute random intervention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m intervention = \u001b[43mintervention_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m _, outcome_key = random.split(keys[step])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/training/grpo_policy_loader.py:257\u001b[39m, in \u001b[36mcreate_grpo_intervention_fn.<locals>.select_intervention\u001b[39m\u001b[34m(state, key)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Apply policy network (inference mode)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m policy_output = \u001b[43mloaded_policy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloaded_policy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# is_training=False for inference\u001b[39;49;00m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Extract intervention decision\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/transform.py:183\u001b[39m, in \u001b[36mwithout_state.<locals>.apply_fn\u001b[39m\u001b[34m(params, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    178\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mIf the functions you are transforming use the same names you must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m out, state = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/transform.py:456\u001b[39m, in \u001b[36mtransform_with_state.<locals>.apply_fn\u001b[39m\u001b[34m(params, state, rng, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m   out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m jax.errors.UnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/training/grpo_policy_loader.py:127\u001b[39m, in \u001b[36mload_grpo_policy.<locals>.policy_fn\u001b[39m\u001b[34m(enriched_history, target_variable_idx, is_training)\u001b[39m\n\u001b[32m    117\u001b[39m policy = EnrichedAcquisitionPolicyNetwork(\n\u001b[32m    118\u001b[39m     num_layers=arch_config.get(\u001b[33m'\u001b[39m\u001b[33mnum_layers\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m4\u001b[39m),\n\u001b[32m    119\u001b[39m     num_heads=arch_config.get(\u001b[33m'\u001b[39m\u001b[33mnum_heads\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m8\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m     use_role_based_projection=arch_config.get(\u001b[33m'\u001b[39m\u001b[33muse_role_based_projection\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    126\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_variable_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_variable_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_training\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/policy_heads.py:306\u001b[39m, in \u001b[36mEnrichedAcquisitionPolicyNetwork.__call__\u001b[39m\u001b[34m(self, enriched_history, target_variable_idx, is_training)\u001b[39m\n\u001b[32m    304\u001b[39m intervention_mask = jnp.any(intervention_indicators > \u001b[32m0\u001b[39m, axis=\u001b[32m0\u001b[39m).astype(jnp.float32)  \u001b[38;5;66;03m# [n_vars]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m variable_embeddings = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_mask\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [n_vars, hidden_dim]\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# Apply simplified policy heads\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/enriched_policy.py:75\u001b[39m, in \u001b[36mEnrichedAttentionEncoder.__call__\u001b[39m\u001b[34m(self, enriched_history, is_training, target_mask, intervention_mask)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_role_based_projection:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_project_enriched_input_with_roles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_mask\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [T, n_vars, hidden_dim]\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/enriched_policy.py:152\u001b[39m, in \u001b[36mEnrichedAttentionEncoder._project_enriched_input_with_roles\u001b[39m\u001b[34m(self, enriched_history, target_mask, intervention_mask)\u001b[39m\n\u001b[32m    147\u001b[39m role_projection = RoleBasedProjection(\n\u001b[32m    148\u001b[39m     hidden_dim=\u001b[38;5;28mself\u001b[39m.hidden_dim,\n\u001b[32m    149\u001b[39m     w_init=\u001b[38;5;28mself\u001b[39m.w_init\n\u001b[32m    150\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m x = \u001b[43mrole_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43menriched_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Apply layer normalization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Imperial/Individual_Project/causal_bayes_opt/src/causal_bayes_opt/acquisition/enriched/role_based_projection.py:98\u001b[39m, in \u001b[36mRoleBasedProjection.__call__\u001b[39m\u001b[34m(self, enriched_history, target_mask, intervention_mask)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Apply projections\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m proj_target = \u001b[43mtarget_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_history\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [T*n_vars, hidden_dim]\u001b[39;00m\n\u001b[32m     99\u001b[39m proj_intervention = intervention_projection(flat_history)  \u001b[38;5;66;03m# [T*n_vars, hidden_dim]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/basic.py:178\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, inputs, precision)\u001b[39m\n\u001b[32m    177\u001b[39m   w_init = hk.initializers.TruncatedNormal(stddev=stddev)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m w = \u001b[43mhk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m out = jnp.dot(inputs, w, precision=precision)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/causal-bayes-opt-9Aj1r1ec-py3.12/lib/python3.12/site-packages/haiku/_src/base.py:675\u001b[39m, in \u001b[36mget_parameter\u001b[39m\u001b[34m(name, shape, dtype, init)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame.params_frozen:\n\u001b[32m    674\u001b[39m   \u001b[38;5;66;03m# Throw if we needed to re-init the parameter during apply.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m param_missing_error\n\u001b[32m    677\u001b[39m param = check_not_none(param, \u001b[33m\"\u001b[39m\u001b[33mParameters cannot be `None`.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Unable to retrieve parameter 'w' for module 'EnrichedAcquisitionPolicyNetwork/EnrichedAttentionEncoder/~_project_enriched_input_with_roles/RoleBasedProjection/target_projection' All parameters must be created as part of `init`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotebookError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 282\u001b[39m\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n\u001b[32m    281\u001b[39m         traceback.print_exc()\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotebookError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPhase 2 evaluation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# Original evaluation code for other modes\u001b[39;00m\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Import the unified evaluation function\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcausal_bayes_opt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_evaluation\n",
      "\u001b[31mNotebookError\u001b[39m: Phase 2 evaluation failed: Unable to retrieve parameter 'w' for module 'EnrichedAcquisitionPolicyNetwork/EnrichedAttentionEncoder/~_project_enriched_input_with_roles/RoleBasedProjection/target_projection' All parameters must be created as part of `init`."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 5: Run evaluation with proper optimization handling\n",
    "\n",
    "Evaluate checkpoints against baselines with correct metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üèÅ Running Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = project_root / \"results\" / f\"evaluation_{EVALUATION_MODE.lower()}_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"\\nEvaluation parameters:\")\n",
    "print(f\"  Mode: {EVALUATION_MODE}\")\n",
    "print(f\"  Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"  Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"  Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "\n",
    "# Store results\n",
    "evaluation_results = {}\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Phase 2 Active Learning Implementation\n",
    "if EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "    print(\"\\nüöÄ Running Phase 2: Active Learning with GRPO Policy\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Import Phase 2 modules\n",
    "    from src.causal_bayes_opt.training.grpo_policy_loader import (\n",
    "        load_grpo_policy, create_grpo_intervention_fn\n",
    "    )\n",
    "    from examples.demo_learning import (\n",
    "        DemoConfig, create_learnable_surrogate_model,\n",
    "        create_oracle_intervention_policy\n",
    "    )\n",
    "    from examples.complete_workflow_demo import run_progressive_learning_demo_with_scm\n",
    "    \n",
    "    # Ensure we have a checkpoint selected\n",
    "    if not selected_checkpoints:\n",
    "        raise NotebookError(\"No checkpoint selected for Phase 2 evaluation\")\n",
    "    \n",
    "    ckpt = selected_checkpoints[0]  # Use first checkpoint\n",
    "    print(f\"\\nLoading GRPO policy from: {ckpt.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load GRPO policy\n",
    "        grpo_policy = load_grpo_policy(str(ckpt.path))\n",
    "        print(f\"‚úÖ Loaded GRPO policy successfully\")\n",
    "        print(f\"  Variables: {grpo_policy.variables}\")\n",
    "        print(f\"  Target: {grpo_policy.target_variable}\")\n",
    "        \n",
    "        # Run Phase 2 for each test SCM\n",
    "        phase2_results = {}\n",
    "        \n",
    "        for scm_idx, scm in enumerate(test_scms):\n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"SCM {scm_idx + 1}/{len(test_scms)}\")\n",
    "            \n",
    "            # Create GRPO intervention function for this SCM\n",
    "            grpo_intervention_fn = create_grpo_intervention_fn(\n",
    "                loaded_policy=grpo_policy,\n",
    "                scm=scm,\n",
    "                intervention_range=(-2.0, 2.0)\n",
    "            )\n",
    "            \n",
    "            # Run multiple seeds\n",
    "            scm_results = []\n",
    "            for seed_idx in range(RUNS_PER_METHOD):\n",
    "                print(f\"  Run {seed_idx + 1}/{RUNS_PER_METHOD}\", end=\"... \")\n",
    "                \n",
    "                # Create new config for each run with updated seed\n",
    "                phase2_config = DemoConfig(\n",
    "                    n_observational_samples=OBSERVATION_SAMPLES,\n",
    "                    n_intervention_steps=INTERVENTION_BUDGET,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    intervention_value_range=(-2.0, 2.0),\n",
    "                    random_seed=RANDOM_SEED + scm_idx * 100 + seed_idx,\n",
    "                    scoring_method=\"bic\"\n",
    "                )\n",
    "                \n",
    "                # Run Phase 2: GRPO policy + active learning\n",
    "                result = run_progressive_learning_demo_with_scm(\n",
    "                    scm=scm,\n",
    "                    config=phase2_config,\n",
    "                    pretrained_surrogate=None,  # Active learning from scratch\n",
    "                    pretrained_acquisition=grpo_intervention_fn  # GRPO policy\n",
    "                )\n",
    "                \n",
    "                scm_results.append(result)\n",
    "                \n",
    "                # Extract final value from target_progress if available\n",
    "                if 'target_progress' in result and result['target_progress']:\n",
    "                    final_value = result['target_progress'][-1]\n",
    "                    print(f\"Final value: {final_value:.4f}\")\n",
    "                else:\n",
    "                    # Handle intervention_mean which might be a JAX array\n",
    "                    final_value = result.get('intervention_mean', 'N/A')\n",
    "                    if hasattr(final_value, 'item'):\n",
    "                        # Convert JAX array to scalar\n",
    "                        final_value = float(final_value.item())\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    elif isinstance(final_value, (int, float)):\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Final value: {final_value}\")\n",
    "            \n",
    "            phase2_results[f\"scm_{scm_idx}\"] = scm_results\n",
    "        \n",
    "        # Also run baseline: Random + Active Learning\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Running baseline: Random + Active Learning\")\n",
    "        \n",
    "        random_active_results = {}\n",
    "        for scm_idx, scm in enumerate(test_scms):\n",
    "            print(f\"\\nSCM {scm_idx + 1}/{len(test_scms)}\")\n",
    "            \n",
    "            scm_results = []\n",
    "            for seed_idx in range(RUNS_PER_METHOD):\n",
    "                print(f\"  Run {seed_idx + 1}/{RUNS_PER_METHOD}\", end=\"... \")\n",
    "                \n",
    "                # Create new config for baseline with different seed offset\n",
    "                phase2_config = DemoConfig(\n",
    "                    n_observational_samples=OBSERVATION_SAMPLES,\n",
    "                    n_intervention_steps=INTERVENTION_BUDGET,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    intervention_value_range=(-2.0, 2.0),\n",
    "                    random_seed=RANDOM_SEED + scm_idx * 100 + seed_idx + 1000,\n",
    "                    scoring_method=\"bic\"\n",
    "                )\n",
    "                \n",
    "                result = run_progressive_learning_demo_with_scm(\n",
    "                    scm=scm,\n",
    "                    config=phase2_config,\n",
    "                    pretrained_surrogate=None,  # Active learning\n",
    "                    pretrained_acquisition=None  # Random interventions\n",
    "                )\n",
    "                \n",
    "                scm_results.append(result)\n",
    "                \n",
    "                # Extract final value from target_progress if available\n",
    "                if 'target_progress' in result and result['target_progress']:\n",
    "                    final_value = result['target_progress'][-1]\n",
    "                    print(f\"Final value: {final_value:.4f}\")\n",
    "                else:\n",
    "                    # Handle intervention_mean which might be a JAX array\n",
    "                    final_value = result.get('intervention_mean', 'N/A')\n",
    "                    if hasattr(final_value, 'item'):\n",
    "                        # Convert JAX array to scalar\n",
    "                        final_value = float(final_value.item())\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    elif isinstance(final_value, (int, float)):\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Final value: {final_value}\")\n",
    "            \n",
    "            random_active_results[f\"scm_{scm_idx}\"] = scm_results\n",
    "        \n",
    "        # NEW: Run Oracle + Active Learning baseline\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Running oracle baseline: Oracle + Active Learning\")\n",
    "        print(\"(Oracle knows true parents and tests interventions)\")\n",
    "        \n",
    "        oracle_active_results = {}\n",
    "        for scm_idx, scm in enumerate(test_scms):\n",
    "            print(f\"\\nSCM {scm_idx + 1}/{len(test_scms)}\")\n",
    "            \n",
    "            # Get variables and target from SCM\n",
    "            from causal_bayes_opt.data_structures import get_variables, get_target\n",
    "            variables = sorted(get_variables(scm))\n",
    "            target = get_target(scm)\n",
    "            \n",
    "            # Create oracle intervention function for this SCM\n",
    "            oracle_intervention_fn = create_oracle_intervention_policy(\n",
    "                variables=variables,\n",
    "                target_variable=target,\n",
    "                scm=scm,\n",
    "                intervention_value_range=(-2.0, 2.0)\n",
    "            )\n",
    "            \n",
    "            scm_results = []\n",
    "            for seed_idx in range(RUNS_PER_METHOD):\n",
    "                print(f\"  Run {seed_idx + 1}/{RUNS_PER_METHOD}\", end=\"... \")\n",
    "                \n",
    "                # Create new config for oracle with different seed offset\n",
    "                phase2_config = DemoConfig(\n",
    "                    n_observational_samples=OBSERVATION_SAMPLES,\n",
    "                    n_intervention_steps=INTERVENTION_BUDGET,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    intervention_value_range=(-2.0, 2.0),\n",
    "                    random_seed=RANDOM_SEED + scm_idx * 100 + seed_idx + 2000,\n",
    "                    scoring_method=\"bic\"\n",
    "                )\n",
    "                \n",
    "                result = run_progressive_learning_demo_with_scm(\n",
    "                    scm=scm,\n",
    "                    config=phase2_config,\n",
    "                    pretrained_surrogate=None,  # Active learning\n",
    "                    pretrained_acquisition=oracle_intervention_fn  # Oracle interventions\n",
    "                )\n",
    "                \n",
    "                scm_results.append(result)\n",
    "                \n",
    "                # Extract final value from target_progress if available\n",
    "                if 'target_progress' in result and result['target_progress']:\n",
    "                    final_value = result['target_progress'][-1]\n",
    "                    print(f\"Final value: {final_value:.4f}\")\n",
    "                else:\n",
    "                    # Handle intervention_mean which might be a JAX array\n",
    "                    final_value = result.get('intervention_mean', 'N/A')\n",
    "                    if hasattr(final_value, 'item'):\n",
    "                        # Convert JAX array to scalar\n",
    "                        final_value = float(final_value.item())\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    elif isinstance(final_value, (int, float)):\n",
    "                        print(f\"Final value: {final_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Final value: {final_value}\")\n",
    "            \n",
    "            oracle_active_results[f\"scm_{scm_idx}\"] = scm_results\n",
    "        \n",
    "        # Store results\n",
    "        evaluation_results['phase2_grpo_active'] = {\n",
    "            'results': phase2_results,\n",
    "            'method': 'GRPO + Active Learning',\n",
    "            'checkpoint': ckpt.name\n",
    "        }\n",
    "        \n",
    "        evaluation_results['phase2_random_active'] = {\n",
    "            'results': random_active_results,\n",
    "            'method': 'Random + Active Learning',\n",
    "            'checkpoint': 'baseline'\n",
    "        }\n",
    "        \n",
    "        evaluation_results['phase2_oracle_active'] = {\n",
    "            'results': oracle_active_results,\n",
    "            'method': 'Oracle + Active Learning',\n",
    "            'checkpoint': 'oracle'\n",
    "        }\n",
    "        \n",
    "        # Also run standard GRPO evaluation for comparison\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Running standard GRPO (Phase 1 only) for comparison...\")\n",
    "        \n",
    "        # Import the unified evaluation function\n",
    "        from src.causal_bayes_opt.evaluation import run_evaluation\n",
    "        \n",
    "        eval_config = {\n",
    "            'n_scms': NUM_TEST_SCMS,\n",
    "            'n_seeds': RUNS_PER_METHOD,\n",
    "            'parallel': False,\n",
    "            'experiment': {\n",
    "                'runs_per_method': RUNS_PER_METHOD,\n",
    "                'target': {\n",
    "                    'max_interventions': INTERVENTION_BUDGET,\n",
    "                    'n_observational_samples': OBSERVATION_SAMPLES,\n",
    "                    'optimization_direction': ckpt.optimization_config.direction\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        standard_results = run_evaluation(\n",
    "            checkpoint_path=ckpt.path,\n",
    "            output_dir=output_dir / \"grpo_standard\",\n",
    "            config=eval_config,\n",
    "            test_scms=test_scms,\n",
    "            methods=['grpo']\n",
    "        )\n",
    "        \n",
    "        evaluation_results['phase1_grpo_bootstrap'] = {\n",
    "            'results': standard_results,\n",
    "            'method': 'GRPO + Bootstrap (Phase 1)',\n",
    "            'checkpoint': ckpt.name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Phase 2 evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise NotebookError(f\"Phase 2 evaluation failed: {e}\")\n",
    "\n",
    "else:\n",
    "    # Original evaluation code for other modes\n",
    "    # Import the unified evaluation function\n",
    "    from src.causal_bayes_opt.evaluation import run_evaluation\n",
    "\n",
    "    # Run evaluation for each checkpoint\n",
    "    for ckpt in selected_checkpoints:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {ckpt.name}\")\n",
    "        print(f\"Optimization: {ckpt.optimization_config.direction}\")\n",
    "        \n",
    "        # Create checkpoint-specific output directory\n",
    "        ckpt_output_dir = output_dir / ckpt.name\n",
    "        ckpt_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Prepare evaluation config\n",
    "        eval_config = {\n",
    "            'n_scms': NUM_TEST_SCMS,\n",
    "            'n_seeds': RUNS_PER_METHOD,  # This is the correct parameter name\n",
    "            'parallel': False,  # Disable parallel to avoid serialization issues\n",
    "            'experiment': {\n",
    "                'runs_per_method': RUNS_PER_METHOD,\n",
    "                'target': {\n",
    "                    'max_interventions': INTERVENTION_BUDGET,\n",
    "                    'n_observational_samples': 100,\n",
    "                    'optimization_direction': ckpt.optimization_config.direction\n",
    "                },\n",
    "                'scm_generation': {\n",
    "                    'use_variable_factory': True,\n",
    "                    'variable_range': [3, 6],\n",
    "                    'structure_types': ['fork', 'chain', 'collider', 'mixed']\n",
    "                }\n",
    "            },\n",
    "            'visualization': {\n",
    "                'enabled': True,\n",
    "                'plot_types': ['target_trajectory', 'f1_trajectory', 'shd_trajectory', 'method_comparison']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Run evaluation using the unified system\n",
    "        print(f\"\\nRunning evaluation with unified system...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Use our test SCMs if already generated\n",
    "            comparison_results = run_evaluation(\n",
    "                checkpoint_path=ckpt.path,\n",
    "                output_dir=ckpt_output_dir,\n",
    "                config=eval_config,\n",
    "                test_scms=test_scms if 'test_scms' in locals() else None,\n",
    "                methods=['grpo', 'random', 'learning', 'oracle']  # Compare against all baselines\n",
    "            )\n",
    "            \n",
    "            duration = (time.time() - start_time) / 60\n",
    "            print(f\"\\n‚úÖ Evaluation completed in {duration:.1f} minutes\")\n",
    "            \n",
    "            # Store results\n",
    "            evaluation_results[ckpt.name] = {\n",
    "                'results': comparison_results,\n",
    "                'optimization_direction': ckpt.optimization_config.direction,\n",
    "                'checkpoint_metadata': ckpt.to_dict(),\n",
    "                'duration_minutes': duration\n",
    "            }\n",
    "            \n",
    "            # Quick summary - FIXED: use method_metrics instead of method_results\n",
    "            print(\"\\nüìä Quick Summary:\")\n",
    "            for method_name, metrics in comparison_results.method_metrics.items():\n",
    "                improvement = metrics.mean_improvement\n",
    "                # Format based on optimization direction\n",
    "                if ckpt.optimization_config.is_minimizing:\n",
    "                    display_val = f\"{-improvement:.4f} (lower is better)\"\n",
    "                else:\n",
    "                    display_val = f\"{improvement:.4f} (higher is better)\"\n",
    "                print(f\"  {method_name}: {display_val}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            evaluation_results[ckpt.name] = {'error': str(e)}\n",
    "\n",
    "total_duration = (time.time() - evaluation_start_time) / 60\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ All evaluations complete!\")\n",
    "print(f\"  Total duration: {total_duration:.1f} minutes\")\n",
    "print(f\"  Successful: {sum(1 for r in evaluation_results.values() if 'error' not in r)}/{len(evaluation_results)}\")\n",
    "print(f\"\\nüìÅ Results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 6: Generate visualizations with trajectory plots\n",
    "\n",
    "Create comprehensive plots showing method performance over time.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Generating Visualizations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have results to visualize\n",
    "valid_results = {k: v for k, v in evaluation_results.items() if 'error' not in v}\n",
    "\n",
    "if not valid_results:\n",
    "    print(\"‚ùå No valid results to visualize\")\n",
    "else:\n",
    "    if EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "        # Phase 2 specific visualizations\n",
    "        print(\"\\nüìà Phase 2 Active Learning Visualizations\")\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Phase 2: GRPO + Active Learning Results', fontsize=16)\n",
    "        \n",
    "        # Extract results\n",
    "        grpo_active_results = evaluation_results.get('phase2_grpo_active', {}).get('results', {})\n",
    "        random_active_results = evaluation_results.get('phase2_random_active', {}).get('results', {})\n",
    "        oracle_active_results = evaluation_results.get('phase2_oracle_active', {}).get('results', {})\n",
    "        \n",
    "        # 1. Target Value Trajectory\n",
    "        ax = axes[0, 0]\n",
    "        ax.set_title('Target Value Over Time')\n",
    "        ax.set_xlabel('Interventions')\n",
    "        ax.set_ylabel('Target Value')\n",
    "        \n",
    "        # Plot trajectories for each method\n",
    "        for method_name, method_results, color in [\n",
    "            ('GRPO + Active', grpo_active_results, 'blue'),\n",
    "            ('Random + Active', random_active_results, 'orange'),\n",
    "            ('Oracle + Active', oracle_active_results, 'green')\n",
    "        ]:\n",
    "            all_trajectories = []\n",
    "            for scm_results in method_results.values():\n",
    "                for result in scm_results:\n",
    "                    if 'target_progress' in result:\n",
    "                        all_trajectories.append(result['target_progress'])\n",
    "            \n",
    "            if all_trajectories:\n",
    "                # Average across runs\n",
    "                import numpy as np\n",
    "                max_len = max(len(t) for t in all_trajectories)\n",
    "                padded = np.array([t + [t[-1]]*(max_len-len(t)) for t in all_trajectories])\n",
    "                mean_traj = np.mean(padded, axis=0)\n",
    "                std_traj = np.std(padded, axis=0)\n",
    "                \n",
    "                x = np.arange(len(mean_traj))\n",
    "                ax.plot(x, mean_traj, label=method_name, linewidth=2, color=color)\n",
    "                ax.fill_between(x, mean_traj - std_traj, mean_traj + std_traj, alpha=0.2, color=color)\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Structure Learning (F1 Score)\n",
    "        ax = axes[0, 1]\n",
    "        ax.set_title('Structure Learning (F1 Score)')\n",
    "        ax.set_xlabel('Interventions')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        \n",
    "        for method_name, method_results, color in [\n",
    "            ('GRPO + Active', grpo_active_results, 'blue'),\n",
    "            ('Random + Active', random_active_results, 'orange'),\n",
    "            ('Oracle + Active', oracle_active_results, 'green')\n",
    "        ]:\n",
    "            f1_trajectories = []\n",
    "            for scm_results in method_results.values():\n",
    "                for result in scm_results:\n",
    "                    if 'learning_history' in result:\n",
    "                        # Extract F1 scores from learning history\n",
    "                        f1_scores = []\n",
    "                        for step in result['learning_history']:\n",
    "                            if 'f1_score' in step:\n",
    "                                f1_scores.append(step['f1_score'])\n",
    "                        if f1_scores:\n",
    "                            f1_trajectories.append(f1_scores)\n",
    "            \n",
    "            if f1_trajectories:\n",
    "                # Average across runs\n",
    "                max_len = max(len(t) for t in f1_trajectories)\n",
    "                padded = np.array([t + [t[-1]]*(max_len-len(t)) for t in f1_trajectories])\n",
    "                mean_f1 = np.mean(padded, axis=0)\n",
    "                std_f1 = np.std(padded, axis=0)\n",
    "                \n",
    "                x = np.arange(len(mean_f1))\n",
    "                ax.plot(x, mean_f1, label=method_name, linewidth=2, color=color)\n",
    "                ax.fill_between(x, mean_f1 - std_f1, mean_f1 + std_f1, alpha=0.2, color=color)\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        # 3. Final Performance Comparison\n",
    "        ax = axes[1, 0]\n",
    "        ax.set_title('Final Performance Comparison')\n",
    "        \n",
    "        # Collect final values\n",
    "        method_final_values = {}\n",
    "        for method_key, method_data, display_name in [\n",
    "            ('phase2_grpo_active', grpo_active_results, 'GRPO + Active'),\n",
    "            ('phase2_random_active', random_active_results, 'Random + Active'),\n",
    "            ('phase2_oracle_active', oracle_active_results, 'Oracle + Active')\n",
    "        ]:\n",
    "            final_values = []\n",
    "            for scm_results in method_data.values():\n",
    "                for result in scm_results:\n",
    "                    # Extract final value from target_progress if available\n",
    "                    if 'target_progress' in result and result['target_progress']:\n",
    "                        final_values.append(result['target_progress'][-1])\n",
    "                    elif 'intervention_mean' in result:\n",
    "                        # Handle intervention_mean which might be a JAX array\n",
    "                        val = result['intervention_mean']\n",
    "                        if hasattr(val, 'item'):\n",
    "                            final_values.append(float(val.item()))\n",
    "                        elif isinstance(val, (int, float)):\n",
    "                            final_values.append(float(val))\n",
    "            if final_values:\n",
    "                method_final_values[display_name] = final_values\n",
    "        \n",
    "        # Also add Phase 1 GRPO if available\n",
    "        if 'phase1_grpo_bootstrap' in evaluation_results:\n",
    "            phase1_results = evaluation_results['phase1_grpo_bootstrap'].get('results', {})\n",
    "            if hasattr(phase1_results, 'method_metrics'):\n",
    "                for method, metrics in phase1_results.method_metrics.items():\n",
    "                    if 'grpo' in method.lower():\n",
    "                        method_final_values['GRPO + Bootstrap'] = [metrics.mean_final_value]\n",
    "        \n",
    "        # Create box plot\n",
    "        if method_final_values:\n",
    "            labels = list(method_final_values.keys())\n",
    "            values = list(method_final_values.values())\n",
    "            positions = range(len(labels))\n",
    "            \n",
    "            bp = ax.boxplot(values, positions=positions, labels=labels, patch_artist=True)\n",
    "            \n",
    "            # Color boxes\n",
    "            colors = ['blue', 'orange', 'green', 'red'][:len(labels)]\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            ax.set_ylabel('Final Target Value')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add sample size annotations\n",
    "            for i, (label, vals) in enumerate(method_final_values.items()):\n",
    "                ax.text(i, ax.get_ylim()[1] * 0.95, f'n={len(vals)}', \n",
    "                        ha='center', va='top', fontsize=10)\n",
    "        \n",
    "        # 4. Structure Learning Summary\n",
    "        ax = axes[1, 1]\n",
    "        ax.set_title('Structure Learning Performance')\n",
    "        \n",
    "        # Calculate average final F1 scores\n",
    "        method_f1_scores = {}\n",
    "        for method_name, method_results, display_name in [\n",
    "            ('GRPO + Active', grpo_active_results, 'GRPO + Active'),\n",
    "            ('Random + Active', random_active_results, 'Random + Active'),\n",
    "            ('Oracle + Active', oracle_active_results, 'Oracle + Active')\n",
    "        ]:\n",
    "            f1_scores = []\n",
    "            for scm_results in method_results.values():\n",
    "                for result in scm_results:\n",
    "                    if 'structure_learning_metrics' in result:\n",
    "                        final_f1 = result['structure_learning_metrics'].get('final_f1', 0)\n",
    "                        f1_scores.append(final_f1)\n",
    "                    elif 'learning_history' in result and result['learning_history']:\n",
    "                        # Try to get from last step\n",
    "                        last_step = result['learning_history'][-1]\n",
    "                        if 'f1_score' in last_step:\n",
    "                            f1_scores.append(last_step['f1_score'])\n",
    "            \n",
    "            if f1_scores:\n",
    "                method_f1_scores[display_name] = np.mean(f1_scores)\n",
    "        \n",
    "        # Create bar plot\n",
    "        if method_f1_scores:\n",
    "            methods = list(method_f1_scores.keys())\n",
    "            scores = list(method_f1_scores.values())\n",
    "            \n",
    "            bars = ax.bar(methods, scores, alpha=0.7, color=['blue', 'orange', 'green'][:len(methods)])\n",
    "            ax.set_ylabel('Average Final F1 Score')\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, score in zip(bars, scores):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = output_dir / \"phase2_comparison.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Saved Phase 2 comparison plot: {plot_path}\")\n",
    "        \n",
    "        # Display the plot\n",
    "        display(Image(filename=str(plot_path)))\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nüìä Phase 2 Summary Statistics:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Method':<25} {'Mean Final Value':>15} {'Std Dev':>15} {'F1 Score':>15}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for method_name, final_vals in method_final_values.items():\n",
    "            if final_vals:\n",
    "                mean_val = np.mean(final_vals)\n",
    "                std_val = np.std(final_vals)\n",
    "                f1_val = method_f1_scores.get(method_name, 'N/A')\n",
    "                \n",
    "                print(f\"{method_name:<25} {mean_val:>15.4f} {std_val:>15.4f} \", end=\"\")\n",
    "                if isinstance(f1_val, float):\n",
    "                    print(f\"{f1_val:>15.3f}\")\n",
    "                else:\n",
    "                    print(f\"{'N/A':>15}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Add intervention diversity analysis\n",
    "        print(\"\\nüìä Intervention Diversity Analysis:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for method_name, method_results in [\n",
    "            ('GRPO + Active', grpo_active_results),\n",
    "            ('Random + Active', random_active_results),\n",
    "            ('Oracle + Active', oracle_active_results)\n",
    "        ]:\n",
    "            intervention_counts = {}\n",
    "            total_interventions = 0\n",
    "            \n",
    "            for scm_results in method_results.values():\n",
    "                for result in scm_results:\n",
    "                    if 'intervention_counts' in result:\n",
    "                        for var, count in result['intervention_counts'].items():\n",
    "                            if var != 'observational':\n",
    "                                intervention_counts[var] = intervention_counts.get(var, 0) + count\n",
    "                                total_interventions += count\n",
    "            \n",
    "            if total_interventions > 0:\n",
    "                print(f\"\\n{method_name}:\")\n",
    "                for var, count in sorted(intervention_counts.items()):\n",
    "                    percentage = (count / total_interventions) * 100\n",
    "                    print(f\"  {var}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    else:\n",
    "        # Original visualization code for other modes\n",
    "        # Process results for each checkpoint\n",
    "        for ckpt_name, eval_data in valid_results.items():\n",
    "            print(f\"\\nüìà Visualizations for: {ckpt_name}\")\n",
    "            \n",
    "            comparison_results = eval_data['results']\n",
    "            opt_direction = eval_data['optimization_direction']\n",
    "            \n",
    "            # The plots should already be generated by run_evaluation\n",
    "            # Let's check what was created\n",
    "            ckpt_output_dir = output_dir / ckpt_name\n",
    "            plot_files = list(ckpt_output_dir.glob(\"*.png\"))\n",
    "            plot_files.extend(list((ckpt_output_dir / \"plots\").glob(\"*.png\")))\n",
    "            \n",
    "            if plot_files:\n",
    "                print(f\"Found {len(plot_files)} plots:\")\n",
    "                for plot_file in plot_files:\n",
    "                    print(f\"  - {plot_file.name}\")\n",
    "                    # Display the plot\n",
    "                    try:\n",
    "                        display(Image(filename=str(plot_file)))\n",
    "                    except:\n",
    "                        print(f\"    (Could not display {plot_file.name})\")\n",
    "            \n",
    "            # Also create a summary table - FIXED: use method_metrics instead of method_results\n",
    "            print(f\"\\nüìä Performance Summary for {ckpt_name}:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'Method':<30} {'Improvement':>15} {'Final Value':>15} {'Success Rate':>15}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for method_name, metrics in comparison_results.method_metrics.items():\n",
    "                improvement = metrics.mean_improvement\n",
    "                final_value = metrics.mean_final_value\n",
    "                success_rate = metrics.success_rate if hasattr(metrics, 'success_rate') else metrics.n_successful / metrics.n_runs if metrics.n_runs > 0 else 0.0\n",
    "                \n",
    "                if opt_direction == \"MINIMIZE\":\n",
    "                    # For minimization, negative improvement is good\n",
    "                    improvement_str = f\"{-improvement:>15.4f} ‚Üì\"\n",
    "                else:\n",
    "                    improvement_str = f\"{improvement:>15.4f} ‚Üë\"\n",
    "                \n",
    "                print(f\"{method_name:<30} {improvement_str} {final_value:>15.4f} {success_rate:>14.1%}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Statistical significance - check if it exists\n",
    "            if hasattr(comparison_results, 'statistical_tests') and comparison_results.statistical_tests:\n",
    "                print(\"\\nüîç Statistical Significance:\")\n",
    "                for test_name, test_result in comparison_results.statistical_tests.items():\n",
    "                    if isinstance(test_result, dict):\n",
    "                        p_value = test_result.get('p_value', 1.0)\n",
    "                        significant = p_value < 0.05\n",
    "                        print(f\"  {test_name}: p={p_value:.4f} {'‚úÖ' if significant else '‚ùå'}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualization complete!\")\n",
    "    print(f\"üìÅ All plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 7: Export results and generate summary\n",
    "\n",
    "Export evaluation results with proper handling for different result types.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Exporting Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a helper function to safely convert results\n",
    "def convert_result_to_dict(result):\n",
    "    \"\"\"Safely convert any result type to dictionary.\"\"\"\n",
    "    if isinstance(result, dict):\n",
    "        return result\n",
    "    elif hasattr(result, 'to_dict'):\n",
    "        return result.to_dict()\n",
    "    elif hasattr(result, '__dict__'):\n",
    "        # Convert object attributes to dict\n",
    "        return {k: v for k, v in result.__dict__.items() if not k.startswith('_')}\n",
    "    else:\n",
    "        # Return as-is if we can't convert\n",
    "        return result\n",
    "\n",
    "# Fix export for all types of results\n",
    "export_results = {}\n",
    "for ckpt_name, result_data in evaluation_results.items():\n",
    "    if 'error' in result_data:\n",
    "        # Keep error results as-is\n",
    "        export_results[ckpt_name] = result_data\n",
    "    elif 'phase2_' in ckpt_name or 'phase1_' in ckpt_name:\n",
    "        # Phase 2 and Phase 1 results are already dicts\n",
    "        export_results[ckpt_name] = result_data\n",
    "    else:\n",
    "        # Standard results - handle ComparisonResults object\n",
    "        results = result_data.get('results')\n",
    "        \n",
    "        # Convert ComparisonResults to dict if needed\n",
    "        if results and hasattr(results, 'method_metrics'):\n",
    "            # This is a ComparisonResults object\n",
    "            results_dict = {\n",
    "                'method_metrics': {},\n",
    "                'scm_results': getattr(results, 'scm_results', {}),\n",
    "                'statistical_tests': getattr(results, 'statistical_tests', {}),\n",
    "                'metadata': getattr(results, 'metadata', {})\n",
    "            }\n",
    "            \n",
    "            # Convert method metrics\n",
    "            for method_name, metrics in results.method_metrics.items():\n",
    "                if hasattr(metrics, '__dict__'):\n",
    "                    results_dict['method_metrics'][method_name] = {\n",
    "                        k: v for k, v in metrics.__dict__.items() \n",
    "                        if not k.startswith('_')\n",
    "                    }\n",
    "                else:\n",
    "                    results_dict['method_metrics'][method_name] = metrics\n",
    "            \n",
    "            results = results_dict\n",
    "        \n",
    "        export_results[ckpt_name] = {\n",
    "            'results': convert_result_to_dict(results),\n",
    "            'optimization_direction': result_data.get('optimization_direction'),\n",
    "            'checkpoint_metadata': result_data.get('checkpoint_metadata'),\n",
    "            'duration_minutes': result_data.get('duration_minutes')\n",
    "        }\n",
    "\n",
    "# Prepare export data\n",
    "export_data = {\n",
    "    'evaluation_config': {\n",
    "        'mode': EVALUATION_MODE,\n",
    "        'num_test_scms': NUM_TEST_SCMS,\n",
    "        'runs_per_method': RUNS_PER_METHOD,\n",
    "        'intervention_budget': INTERVENTION_BUDGET,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'timestamp': timestamp\n",
    "    },\n",
    "    'checkpoints_evaluated': [\n",
    "        {\n",
    "            'name': ckpt.name,\n",
    "            'optimization_direction': ckpt.optimization_config.direction,\n",
    "            'path': str(ckpt.path),\n",
    "            'training_mode': ckpt.training_config.get('mode', 'unknown')\n",
    "        }\n",
    "        for ckpt in selected_checkpoints\n",
    "    ],\n",
    "    'results': export_results,\n",
    "    'duration_minutes': total_duration if 'total_duration' in locals() else 0\n",
    "}\n",
    "\n",
    "# Save JSON results with custom encoder for numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if hasattr(obj, 'item'):\n",
    "            return obj.item()\n",
    "        return super().default(obj)\n",
    "\n",
    "json_path = output_dir / \"evaluation_results.json\"\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, cls=NumpyEncoder)\n",
    "    print(f\"‚úÖ Results saved to: {json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error saving JSON: {e}\")\n",
    "    # Try to save a simplified version\n",
    "    try:\n",
    "        simplified_data = {\n",
    "            'evaluation_config': export_data['evaluation_config'],\n",
    "            'checkpoints_evaluated': export_data['checkpoints_evaluated'],\n",
    "            'error': 'Full results could not be serialized',\n",
    "            'duration_minutes': export_data['duration_minutes']\n",
    "        }\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(simplified_data, f, indent=2)\n",
    "        print(f\"‚úÖ Saved simplified results to: {json_path}\")\n",
    "    except:\n",
    "        print(f\"‚ùå Could not save results to JSON\")\n",
    "\n",
    "# Generate text summary\n",
    "summary_path = output_dir / \"evaluation_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"GRPO Evaluation Summary\\n\")\n",
    "    f.write(f\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Evaluation Mode: {EVALUATION_MODE}\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "    f.write(f\"Duration: {total_duration:.1f} minutes\\n\\n\" if 'total_duration' in locals() else \"\\n\")\n",
    "    \n",
    "    f.write(f\"Configuration:\\n\")\n",
    "    f.write(f\"  Test SCMs: {NUM_TEST_SCMS}\\n\")\n",
    "    f.write(f\"  Runs per method: {RUNS_PER_METHOD}\\n\")\n",
    "    f.write(f\"  Intervention budget: {INTERVENTION_BUDGET}\\n\")\n",
    "    f.write(f\"  Random seed: {RANDOM_SEED}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Checkpoints Evaluated:\\n\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        f.write(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\\n\")\n",
    "    \n",
    "    f.write(f\"\\nKey Findings:\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Summarize results based on evaluation mode\n",
    "    if EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "        # Phase 2 specific summary\n",
    "        f.write(\"\\nPhase 2 Active Learning Results:\\n\\n\")\n",
    "        \n",
    "        # Extract method final values if available\n",
    "        if 'method_final_values' in locals():\n",
    "            for method_name, values in method_final_values.items():\n",
    "                if values:\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values)\n",
    "                    f.write(f\"{method_name}:\\n\")\n",
    "                    f.write(f\"  Mean final value: {mean_val:.4f} ¬± {std_val:.4f}\\n\")\n",
    "                    f.write(f\"  Number of runs: {len(values)}\\n\\n\")\n",
    "        \n",
    "        # Add structure learning summary if available\n",
    "        if 'method_f1_scores' in locals():\n",
    "            f.write(\"\\nStructure Learning Performance:\\n\")\n",
    "            for method, score in method_f1_scores.items():\n",
    "                f.write(f\"  {method}: F1 = {score:.3f}\\n\")\n",
    "    \n",
    "    else:\n",
    "        # Standard evaluation summary\n",
    "        for ckpt_name, result_data in export_results.items():\n",
    "            if 'error' not in result_data and 'results' in result_data:\n",
    "                f.write(f\"\\n{ckpt_name}:\\n\")\n",
    "                \n",
    "                results = result_data['results']\n",
    "                if isinstance(results, dict) and 'method_metrics' in results:\n",
    "                    # Sort methods by performance\n",
    "                    method_metrics = results['method_metrics']\n",
    "                    is_minimizing = result_data.get('optimization_direction') == 'MINIMIZE'\n",
    "                    \n",
    "                    sorted_methods = sorted(\n",
    "                        method_metrics.items(),\n",
    "                        key=lambda x: x[1].get('mean_final_value', float('inf')),\n",
    "                        reverse=not is_minimizing\n",
    "                    )\n",
    "                    \n",
    "                    for rank, (method_name, metrics) in enumerate(sorted_methods[:5], 1):\n",
    "                        final_value = metrics.get('mean_final_value', 'N/A')\n",
    "                        marker = \"*\" if 'grpo' in method_name.lower() else \" \"\n",
    "                        f.write(f\"  {rank}. {marker} {method_name}: {final_value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\\nOutput Files:\\n\")\n",
    "    f.write(f\"  Results JSON: {json_path.name}\\n\")\n",
    "    f.write(f\"  Summary: {summary_path.name}\\n\")\n",
    "    \n",
    "    # List all plots\n",
    "    plot_locations = []\n",
    "    \n",
    "    # Check for plots in main directory\n",
    "    main_plots = list(output_dir.glob(\"*.png\"))\n",
    "    for plot in main_plots:\n",
    "        f.write(f\"  Plot: {plot.name}\\n\")\n",
    "        plot_locations.append(plot)\n",
    "    \n",
    "    # Check for plots in checkpoint subdirectories\n",
    "    if 'selected_checkpoints' in locals():\n",
    "        for ckpt in selected_checkpoints:\n",
    "            ckpt_plot_dir = output_dir / ckpt.name / \"plots\"\n",
    "            if ckpt_plot_dir.exists():\n",
    "                ckpt_plots = list(ckpt_plot_dir.glob(\"*.png\"))\n",
    "                for plot in ckpt_plots:\n",
    "                    f.write(f\"  Plot ({ckpt.name}): {plot.relative_to(output_dir)}\\n\")\n",
    "                    plot_locations.append(plot)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_path}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\nüéâ Evaluation Complete!\")\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Checkpoints: {len(selected_checkpoints)}\")\n",
    "print(f\"Duration: {total_duration:.1f} minutes\" if 'total_duration' in locals() else \"\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "\n",
    "# Show plot locations clearly\n",
    "if 'plot_locations' in locals() and plot_locations:\n",
    "    print(f\"\\nüìä Generated Plots ({len(plot_locations)} total):\")\n",
    "    for plot in plot_locations[:5]:  # Show first 5\n",
    "        print(f\"  - {plot.relative_to(output_dir)}\")\n",
    "    if len(plot_locations) > 5:\n",
    "        print(f\"  ... and {len(plot_locations) - 5} more\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No plots were found in the output directory.\")\n",
    "\n",
    "print(f\"\\nüìä Key Insights:\")\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    print(f\"- Evaluated {selected_checkpoints[0].name} with {selected_checkpoints[0].optimization_config.direction} objective\")\n",
    "    print(f\"- Compare against baselines to see if training improved performance\")\n",
    "    print(f\"- Check trajectory plots to see learning progress over time\")\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    print(f\"- Compared {len(selected_checkpoints)} checkpoints with different optimization directions\")\n",
    "    print(f\"- Minimization policies optimize for lower target values\")\n",
    "    print(f\"- Maximization policies optimize for higher target values\")\n",
    "    print(f\"- Check plots to see which direction performs better\")\n",
    "elif EVALUATION_MODE == \"PHASE2_ACTIVE_LEARNING\":\n",
    "    print(f\"- Evaluated GRPO policy with active learning surrogate\")\n",
    "    print(f\"- Compared GRPO+Active vs Random+Active vs Oracle+Active\")\n",
    "    print(f\"- Oracle baseline shows theoretical best performance\")\n",
    "    print(f\"- Check if GRPO guidance improves structure learning (F1 scores)\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"1. Review the trajectory plots for learning progress\")\n",
    "print(f\"2. Check the summary plots for method comparison\")\n",
    "print(f\"3. Read detailed results in {json_path.name}\")\n",
    "print(f\"4. Consider running with FULL mode for more thorough evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we've accomplished:**\n",
    "1. ‚úÖ Loaded checkpoints with optimization direction metadata\n",
    "2. ‚úÖ Ran evaluation with proper metric handling\n",
    "3. ‚úÖ Generated visualizations that show min/max correctly\n",
    "4. ‚úÖ Exported comprehensive results\n",
    "\n",
    "**Key improvements over original notebook:**\n",
    "- Checkpoint-first approach - no need to run training\n",
    "- Auto-detects optimization direction from metadata\n",
    "- Correctly displays minimization vs maximization results\n",
    "- All cells are independent and can be re-run\n",
    "- No silent failures - explicit errors throughout\n",
    "\n",
    "**Understanding the results:**\n",
    "- For MINIMIZE checkpoints: Lower target values are better\n",
    "- For MAXIMIZE checkpoints: Higher target values are better\n",
    "- The plots show \"(‚Üì better)\" or \"(‚Üë better)\" to clarify\n",
    "- PARENT_SCALE baseline uses minimization by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-bayes-opt-9Aj1r1ec-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
