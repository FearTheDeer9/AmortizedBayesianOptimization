# ACBO 4-Method Comparison Configuration (Including Trained Policy)
# @package _global_

defaults:
  - _self_

# Global settings
seed: 42
project_name: "causal_bayes_opt"

# Experiment configuration
experiment:
  name: "acbo_4method_comparison"
  description: "Compare 4 ACBO methods including trained enriched policy"
  
  # Methods to compare (3 baselines + trained policy)
  methods:
    "Random + Untrained": "random_untrained"
    "Random + Learning": "random_learning"
    "Oracle + Learning": "oracle_learning"
    "Trained Policy + Learning": "learned_enriched_policy"
  
  # SCM generation configuration
  scm_generation:
    use_variable_factory: true
    variable_range: [3, 6]  # Test on 3-6 variable SCMs
    structure_types: ["fork", "chain", "collider", "mixed"]  # Include mixed for policy testing
    rotation_frequency: 1  # Each run uses different SCM
    target_selection: "random"
    edge_density_range: [0.3, 0.7]
  
  # Experimental parameters
  runs_per_method: 5  # Statistical power
  intervention_budget: 15  # Standard intervention budget
  
  # Target configuration
  target:
    max_interventions: 15
    optimization_threshold: 1.0
  
  # Environment settings
  environment:
    noise_scale: 1.0
    intervention_value_range: [-2.0, 2.0]

# Policy configuration for learned enriched policy method
policy_checkpoint_path: "checkpoints/grpo_training/grpo_quick_minimize_20250723_101252_fixed"  # Trained checkpoint path

# Enriched policy settings
enriched_policy:
  enabled: true
  fallback_to_random: true  # Fallback if policy fails
  intervention_value_range: [-2.0, 2.0]
  
  # Validation settings
  require_enriched_architecture: true
  validate_checkpoint: true
  
  # Performance settings
  jit_policy_inference: true
  cache_policy_outputs: false

# Analysis configuration
analysis:
  significance_level: 0.05
  multiple_comparisons_correction: "bonferroni"
  effect_size_threshold: 0.2
  
  # Enhanced metrics for policy evaluation
  primary_metrics:
    - "target_improvement"
    - "structure_accuracy"
    - "sample_efficiency"
    - "convergence_steps"
    - "intervention_efficiency"
  
  # Policy-specific analysis
  policy_analysis:
    intervention_diversity: true
    convergence_analysis: true
    structure_discovery_rate: true
    policy_consistency: true

# Logging configuration
logging:
  level: "INFO"
  
  # WandB integration with policy tracking
  wandb:
    enabled: true
    project: "causal_bayes_opt_4method_comparison"
    entity: null
    tags: ["4method_comparison", "policy_evaluation", "acbo", "enriched_grpo"]
    group: "policy_vs_baselines"
  
  # Enhanced logging for policy analysis
  track_policy_metrics: true
  log_policy_decisions: false  # Set to true for detailed debugging
  save_policy_trajectories: true
  
  # Save detailed results
  save_results: true
  results_format: ["json", "csv"]

# Visualization configuration
visualization:
  enabled: true
  plot_types:
    - "method_comparison"
    - "learning_curves"
    - "statistical_comparison"
    - "performance_distribution"
    - "intervention_efficiency"
    - "f1_trajectory"          # New standalone F1 trajectory plot
    - "shd_trajectory"         # New standalone SHD trajectory plot
    - "target_trajectory"      # New standalone target trajectory plot
  
  # Enhanced visualizations for policy
  policy_specific_plots:
    - "policy_consistency"
    - "intervention_patterns"
    - "policy_vs_baselines"
  
  # Plot settings (improved for better readability)
  figure_size: [15, 10]  # Increased from [12, 8] for better readability
  dpi: 300               # High DPI for crisp plots
  format: "png"

# Validation configuration
validation:
  # Pre-experiment validation
  validate_policy_checkpoint: true
  validate_scm_generation: true
  
  # Post-experiment validation
  validate_results_consistency: true
  check_statistical_assumptions: true
  
  # Performance requirements
  min_success_rate: 0.8  # Minimum 80% successful runs
  max_policy_failure_rate: 0.1  # Max 10% policy failures

# Performance settings
performance:
  parallel_execution: false  # Sequential for policy debugging
  cache_scms: true
  cache_policy_states: false  # Don't cache for policy analysis
  profile_performance: false
  
  # Memory management
  clear_policy_cache_frequency: 10  # Clear every 10 runs
  max_trajectory_length: 100

# Error handling
error_handling:
  policy_failure_action: "fallback"  # "fallback", "skip", "fail"
  scm_generation_retries: 3
  method_execution_timeout: 300  # 5 minutes per method run
  
  # Debugging
  save_failure_logs: true
  detailed_error_reporting: true

# Output configuration
output_dir: results/acbo_comparison

# Hydra configuration (minimal - just for parameter management)
hydra:
  job_logging:
    level: INFO