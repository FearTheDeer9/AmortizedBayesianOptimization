# ACBO Comparison Experiment with Enriched Policy
# Configuration for comparing learned enriched GRPO policy against baseline methods

# @package _global_

defaults:
  - base_config
  - _self_

# Override default methods to include enriched policy
experiment:
  name: "acbo_enriched_policy_comparison"
  description: "Compare learned enriched GRPO policy against baseline methods"
  
  # Enable enriched policy method
  methods:
    "Random + Untrained Surrogate": "random_untrained"
    "Random + Learning Surrogate": "random_learning" 
    "Oracle + Learning Surrogate": "oracle_learning"
    "Learned Enriched Policy + Learning Surrogate": "learned_enriched_policy"
  
  # Enriched policy configuration
  enriched_policy:
    enabled: true
    checkpoint_path: "checkpoints/enriched_grpo_acbo_latest"  # Update with actual path
    fallback_to_random: true
    intervention_value_range: [-2.0, 2.0]
    
    # Policy architecture validation
    require_enriched_architecture: true
    validate_checkpoint: true
  
  # Problem configuration optimized for policy comparison
  problem:
    difficulty: "moderate"
    n_variables: 3  # Match training SCMs
    edge_density: 0.6
    intervention_budget: 30
    target_effect_size: 1.5
  
  # Comparison settings
  comparison:
    n_runs: 5  # Statistical significance
    statistical_tests: true
    plot_trajectories: true
    save_detailed_results: true

# Enhanced logging for policy analysis
logging:
  wandb:
    enabled: true
    project: "causal_bayes_opt_enriched_comparison"
    tags: ["enriched_policy", "acbo_comparison", "policy_evaluation"]
    group: "enriched_vs_baselines"
  
  # Track policy-specific metrics
  track_policy_metrics: true
  policy_analysis:
    intervention_diversity: true
    convergence_analysis: true
    structure_discovery_rate: true

# Validation settings
validation:
  # Validate enriched policy before experiment
  pre_experiment_validation: true
  checkpoint_compatibility_check: true
  
  # Performance requirements
  min_policy_performance:
    mean_reward_threshold: 0.3
    structure_accuracy_threshold: 0.6
  
  # Fallback behavior
  fallback_on_policy_failure: true
  fallback_method: "random_learning"

# Analysis configuration
analysis:
  # Compare policy learning curves
  learning_curve_comparison: true
  
  # Intervention pattern analysis
  intervention_analysis:
    pattern_recognition: true
    strategy_classification: true
    temporal_consistency: true
  
  # Statistical analysis
  statistical_comparison:
    significance_level: 0.05
    multiple_comparisons_correction: true
    effect_size_reporting: true

# Performance optimization
performance:
  # Policy inference optimization
  jit_policy_inference: true
  batch_policy_calls: false  # Process individually for analysis
  
  # Memory management
  clear_policy_cache: true
  checkpoint_caching: false  # Load fresh each time

# Output configuration
output:
  # Save policy-specific results
  save_policy_trajectories: true
  save_intervention_patterns: true
  save_comparison_plots: true
  
  # Export formats
  export_formats: ["json", "csv", "plots"]
  detailed_logging: true

# Safety and debugging
debug:
  # Policy debugging
  validate_policy_outputs: true
  log_policy_failures: true
  save_fallback_events: true
  
  # Experiment monitoring
  monitor_memory_usage: true
  checkpoint_experiment_state: true