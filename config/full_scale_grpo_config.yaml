# Full-Scale GRPO Training Configuration
# @package _global_

defaults:
  - _self_

# Global settings
seed: 42
project_name: "causal_bayes_opt"

# Experiment configuration
experiment:
  n_variables: 4
  name: "full_scale_grpo"
  description: "Full-scale GRPO policy training with continuous rewards"

# Training configuration  
training:
  # Episodes and learning
  n_episodes: 300
  episode_length: 20
  learning_rate: 0.0003
  gamma: 0.99
  
  # Network architecture
  hidden_size: 64
  num_layers: 2
  state_dim: 10
  action_dim: 1
  max_intervention_value: 3.0
  
  # Reward weights
  reward_weights:
    optimization: 1.0
    structure: 0.5
    parent: 0.3
    exploration: 0.1

# Logging configuration
logging:
  level: "INFO"
  wandb:
    enabled: false
    project: "causal_bayes_opt_grpo"
    entity: null
    tags: ["grpo", "policy_training", "continuous_rewards"]

# Hydra settings
hydra:
  run:
    dir: outputs/full_scale_grpo/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/full_scale_grpo/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra:job.num}