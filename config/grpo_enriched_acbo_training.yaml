# Enriched GRPO Policy Training for ACBO Experiments
# @package _global_

defaults:
  - _self_

# Global settings
seed: 42
project_name: "causal_bayes_opt"

# Experiment configuration
experiment:
  name: "enriched_grpo_acbo_training"
  description: "Train enriched GRPO policy on benchmark SCMs for ACBO comparison"
  
  # SCM generation configuration for variable sizes
  scm_generation:
    # Use variable SCM factory instead of hardcoded 3-var SCMs
    use_variable_factory: true
    variable_range: [3, 6]  # Train on 3-6 variable SCMs
    structure_types: ["fork", "chain", "collider", "mixed"]
    rotation_frequency: 50  # Switch SCM every 50 episodes
    target_selection: "random"  # "random", "fixed", "largest_indegree"
    edge_density_range: [0.3, 0.7]  # For random structures
    
    # Backward compatibility: fallback to specific SCMs if factory fails
    fallback_scms:
      - "fork_3var"
      - "chain_3var" 
      - "collider_3var"

# Training configuration  
training:
  # Episodes for ~20 minutes of training
  n_episodes: 1200
  episode_length: 30  # Match experiment intervention budget
  learning_rate: 0.0005  # Slightly higher for enriched architecture
  gamma: 0.99
  
  # Enriched network architecture (variable-agnostic)
  architecture:
    type: "enriched"
    level: "simplified"  # "full", "simplified", "baseline"
    hidden_dim: 128
    num_layers: 4
    num_heads: 8
    key_size: 32
    widening_factor: 4  # FFN expansion factor
    dropout: 0.1
    policy_intermediate_dim: null  # null uses hidden_dim // 2
    
  # State representation
  state_config:
    max_history_size: 100
    include_temporal_features: true
    standardize_values: true
    num_channels: 5  # Simplified to 5 meaningful channels (all variable-specific)
  
  # Action space (intervention values)
  max_intervention_value: 2.0  # Match experiment range
  
  # Reward weights (optimized for structure learning)
  reward_weights:
    optimization: 1.2      # Slightly higher for target optimization
    structure: 0.8         # Structure discovery
    parent: 0.5            # Parent identification
    exploration: 0.2       # Exploration bonus

# Logging configuration
logging:
  level: "INFO"
  wandb:
    enabled: false  # Can be overridden via CLI
    project: "causal_bayes_opt_enriched_grpo"
    entity: null
    tags: ["enriched_grpo", "acbo_training", "benchmark_scms"]
    group: "enriched_policy_training"
    
  # Checkpoint configuration
  checkpointing:
    enabled: true
    frequency: 100  # Save every 100 episodes
    keep_best: 3    # Keep top 3 checkpoints by performance
    metrics_to_track: ["mean_reward", "structure_accuracy", "convergence_rate"]

# Performance optimization
performance:
  jit_compilation: true
  batch_episodes: false  # Process episodes individually for SCM rotation
  profile_performance: false

# Validation configuration
validation:
  enabled: true
  frequency: 50  # Validate every 50 episodes
  n_validation_episodes: 10
  # Use variable SCMs for validation too
  use_variable_validation: true
  validation_variable_range: [3, 5]  # Smaller range for faster validation
  validation_structure_types: ["fork", "chain"]

# Hydra settings
hydra:
  run:
    dir: outputs/enriched_grpo_training/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/enriched_grpo_training/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra:job.num}