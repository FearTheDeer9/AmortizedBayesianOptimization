# ACBO Phase 3: Acquisition Model (RL with GRPO)
# Implementation Plan

## ðŸŽ¯ **Overview**

**Phase 1** âœ… **COMPLETE**: All core data structures, experience buffer, and intervention framework implemented  
**Phase 2** âœ… **COMPLETE**: Sophisticated AVICI integration with ParentSetPosterior and prediction models  
**Phase 3** âœ… **COMPLETE**: Acquisition Model using Group Relative Policy Optimization (GRPO)

### **Phase 3 Progress** âœ… **100% COMPLETE**
- âœ… **Component 1**: AcquisitionState (Rich state representation with optimization tracking)
- âœ… **Component 2**: AcquisitionPolicyNetwork (Two-headed policy with alternating attention)
- âœ… **Component 3**: RewardComponents (Multi-objective verifiable rewards)  
- âœ… **Component 4**: Enhanced GRPO Algorithm (Literature-compliant with open-r1 features, 33 passing tests)
- âœ… **Component 5**: UncertaintyGuidedExploration (Expected information gain, 28 passing tests)

**Phase 3 successfully delivers**: A complete acquisition model using GRPO for intelligent intervention selection based on structural uncertainty and optimization progress. All components are implemented, tested, and integrated.

## ðŸ”¬ **Architectural Design Decisions**

After analyzing recent work in amortized intervention design, we've made informed decisions about our architecture:

### **Key Insight: Dual Objectives**
- **Pure Structure Learning**: Single objective (like many existing approaches)
- **ACBO**: Optimization + structure learning (dual objective)

### **Architectural Decisions**:
- âœ… **ADOPT**: Alternating attention transformer architecture (Component 2)
- âœ… **DESIGN**: GRPO algorithm (optimal for multi-objective rewards)  
- âœ… **DESIGN**: Rich AcquisitionState (essential for optimization tracking)
- âœ… **DESIGN**: Multi-component rewards (required for dual objectives)
- âœ… **DESIGN**: Two-headed policy (leverages our uncertainty infrastructure)
- âœ… **INTEGRATE**: Deep integration with Phase 1&2 components

## ðŸ—ï¸ **Foundation Assessment**

### **Available Infrastructure:**
- âœ… **ExperienceBuffer**: Efficient storage with O(1) appends, batch processing, comprehensive filtering
- âœ… **ParentSetPosterior**: Rich uncertainty quantification with marginal probabilities, entropy, concentration
- âœ… **Intervention Framework**: Perfect interventions with registry pattern, extensible design
- âœ… **AVICI Integration**: Complete data bridge, standardization, validation, and training infrastructure
- âœ… **Sampling Capabilities**: Mixed datasets, intervention grids, effect comparison, batch generation

### **Key APIs Available:**
```python
# Experience Buffer Management
buffer = ExperienceBuffer()
buffer.add_observation(sample)
buffer.add_intervention(intervention, outcome)
stats = buffer.get_statistics()

# Parent Set Prediction  
posterior = predict_parent_posterior(net, params, data, vars, target)
summary = summarize_posterior(posterior)
marginals = get_marginal_parent_probabilities(posterior, vars)

# Intervention Creation & Application
intervention = create_perfect_intervention(targets, values)
modified_scm = apply_intervention(scm, intervention)
samples = sample_with_intervention(scm, intervention, n_samples)
```

---

## ðŸŽ¯ **Phase 3 Implementation Plan**

**Estimated Effort**: 12-15 focused hours  
**Key Dependencies**: Phases 1 & 2 complete âœ…  
**Target**: Production-ready GRPO-based acquisition model with proven transformer architecture

---

## ðŸ“ **Module Structure**

```
src/causal_bayes_opt/
â”œâ”€â”€ acquisition/                           # ðŸ†• NEW MODULE
â”‚   â”œâ”€â”€ __init__.py                       # Clean public API
â”‚   â”œâ”€â”€ state.py                          # State representation for RL
â”‚   â”œâ”€â”€ policy.py                         # Policy network with alternating attention  
â”‚   â”œâ”€â”€ rewards.py                        # Verifiable reward system
â”‚   â”œâ”€â”€ grpo.py                          # GRPO algorithm implementation
â”‚   â”œâ”€â”€ exploration.py                    # Exploration strategies
â”‚   â””â”€â”€ training.py                       # Training utilities
â”‚
â”œâ”€â”€ training/                              # ðŸ†• NEW MODULE  
â”‚   â”œâ”€â”€ __init__.py                       # Training infrastructure
â”‚   â”œâ”€â”€ curriculum.py                     # Curriculum learning
â”‚   â”œâ”€â”€ pipeline.py                       # End-to-end training pipeline
â”‚   â””â”€â”€ utils.py                          # Training utilities
â”‚
â””â”€â”€ evaluation/                            # ðŸ†• NEW MODULE
    â”œâ”€â”€ __init__.py                       # Evaluation framework
    â”œâ”€â”€ metrics.py                        # Performance metrics
    â””â”€â”€ visualization.py                  # Results visualization
```

---

## ðŸ”§ **Component 1: State Representation** 
**Priority**: HIGH | **Effort**: 2-3 hours

### **File**: `src/causal_bayes_opt/acquisition/state.py`

```python
@dataclass(frozen=True)
class AcquisitionState:
    """
    Rich state representation for dual-objective RL-based acquisition.
    
    Unlike CAASL's simple history encoding, we need to track both
    optimization progress AND structural uncertainty for effective
    decision making in our dual-objective setting.
    """
    # Structural uncertainty from Phase 2
    posterior: ParentSetPosterior
    
    # Experience from Phase 1  
    buffer: ExperienceBuffer
    
    # Optimization progress (not present in CAASL)
    best_value: float
    current_target: str
    
    # Additional context
    step: int
    metadata: pyr.PMap[str, Any] = pyr.m()
    
    # Derived properties (computed on creation)
    uncertainty_bits: float = field(init=False)  
    buffer_statistics: BufferStatistics = field(init=False)
    marginal_parent_probs: Dict[str, float] = field(init=False)
    
    def __post_init__(self):
        """Compute derived properties for efficient access."""
        object.__setattr__(self, 'uncertainty_bits', 
                          self.posterior.uncertainty / jnp.log(2))
        object.__setattr__(self, 'buffer_statistics', 
                          self.buffer.get_statistics())
        
        # Get all variables for marginal computation
        all_vars = list(self.buffer.get_variable_coverage())
        object.__setattr__(self, 'marginal_parent_probs',
                          get_marginal_parent_probabilities(self.posterior, all_vars))
    
    def to_history_format(self) -> jnp.ndarray:
        """Convert to standardized history format for transformer input."""
        # Extract observations and intervention indicators
        observations = []
        intervention_masks = []
        
        # Get observational data
        obs_samples = self.buffer.get_observations()
        for sample in obs_samples:
            observations.append(list(sample.values.values()))
            intervention_masks.append([0] * len(sample.values))  # No interventions
        
        # Get interventional data  
        int_samples = self.buffer.get_interventions()
        for intervention, outcome in int_samples:
            observations.append(list(outcome.values.values()))
            # Create intervention mask
            mask = [1 if var in intervention.variables else 0 
                   for var in outcome.values.keys()]
            intervention_masks.append(mask)
        
        # Stack into [n_samples, n_vars, 2] format
        obs_array = jnp.array(observations)  # [n_samples, n_vars]
        mask_array = jnp.array(intervention_masks)  # [n_samples, n_vars]
        
        return jnp.stack([obs_array, mask_array], axis=2)  # [n_samples, n_vars, 2]

# Factory functions
def create_acquisition_state(
    scm: pyr.PMap,
    buffer: ExperienceBuffer, 
    surrogate_model: Any,
    surrogate_params: Any,
    target_variable: str,
    step: int = 0
) -> AcquisitionState:
    """Create state from current buffer and surrogate model predictions."""

def update_state_with_intervention(
    state: AcquisitionState,
    intervention: pyr.PMap,
    outcome: pyr.PMap,
    new_posterior: ParentSetPosterior
) -> AcquisitionState:
    """Create new state after intervention application."""
```

### **Key Features:**
- **Rich state representation** essential for dual objectives
- **Optimization tracking** for target variable progress
- **Uncertainty integration** leveraging our Phase 2 infrastructure
- **History format conversion** for transformer compatibility

---

## ðŸ§  **Component 2: Policy Network with Alternating Attention**
**Priority**: HIGH | **Effort**: 4-5 hours

### **File**: `src/causal_bayes_opt/acquisition/policy.py`

```python
class AlternatingAttentionEncoder(hk.Module):
    """
    Alternating attention transformer encoder for intervention data.
    
    Applies self-attention alternately over samples and variables
    to properly encode permutation symmetries in intervention data.
    """
    
    def __init__(self,
                 num_layers: int = 4,
                 num_heads: int = 8,
                 hidden_dim: int = 128,
                 dropout: float = 0.1,
                 name="AlternatingAttentionEncoder"):
        super().__init__(name=name)
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        self.dropout = dropout

    def __call__(self, 
                 history: jnp.ndarray,  # [n_samples, n_vars, 2]
                 is_training: bool = True) -> jnp.ndarray:
        """
        Apply alternating attention following CAASL architecture.
        
        Args:
            history: Intervention history in [n_samples, n_vars, 2] format
            is_training: Training mode flag
            
        Returns:
            State embedding of shape [n_vars, hidden_dim]
        """
        # Input projection
        x = hk.Linear(self.hidden_dim)(history)  # [n_samples, n_vars, hidden_dim]
        
        # Alternating attention layers
        for layer in range(self.num_layers):
            # Attention over samples (axis=0)
            x = self._sample_attention_layer(x, is_training, f"sample_attn_{layer}")
            
            # Attention over variables (axis=1)  
            x = self._variable_attention_layer(x, is_training, f"var_attn_{layer}")
        
        # Max pooling over samples dimension -> [n_vars, hidden_dim]
        state_embedding = jnp.max(x, axis=0)
        
        return state_embedding
    
    def _sample_attention_layer(self, 
                               x: jnp.ndarray,
                               is_training: bool,
                               layer_name: str) -> jnp.ndarray:
        """Apply self-attention over samples dimension."""
        with hk.experimental.name_scope(layer_name):
            # Reshape for attention: [n_vars, n_samples, hidden_dim]
            x_transposed = jnp.transpose(x, (1, 0, 2))
            n_vars, n_samples, hidden_dim = x_transposed.shape
            
            # Apply attention over samples for each variable independently
            attended = []
            for var_idx in range(n_vars):
                var_samples = x_transposed[var_idx]  # [n_samples, hidden_dim]
                
                # Multi-head self-attention
                attn_output = hk.MultiHeadAttention(
                    num_heads=self.num_heads,
                    key_size=hidden_dim // self.num_heads,
                    w_init_scale=2.0
                )(var_samples, var_samples, var_samples)
                
                attended.append(attn_output)
            
            x_attended = jnp.stack(attended, axis=0)  # [n_vars, n_samples, hidden_dim]
            
            # Residual connection and layer norm
            x_transposed = x_transposed + x_attended
            x_transposed = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x_transposed)
            
            # Feedforward network
            ff_output = hk.nets.MLP([4 * hidden_dim, hidden_dim])(x_transposed)
            x_transposed = x_transposed + ff_output
            x_transposed = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x_transposed)
            
            # Apply dropout if training
            if is_training:
                x_transposed = hk.dropout(hk.next_rng_key(), self.dropout, x_transposed)
            
            # Transpose back: [n_samples, n_vars, hidden_dim]
            return jnp.transpose(x_transposed, (1, 0, 2))
    
    def _variable_attention_layer(self,
                                 x: jnp.ndarray, 
                                 is_training: bool,
                                 layer_name: str) -> jnp.ndarray:
        """Apply self-attention over variables dimension."""
        with hk.experimental.name_scope(layer_name):
            n_samples, n_vars, hidden_dim = x.shape
            
            # Apply attention over variables for each sample independently
            attended = []
            for sample_idx in range(n_samples):
                sample_vars = x[sample_idx]  # [n_vars, hidden_dim]
                
                # Multi-head self-attention
                attn_output = hk.MultiHeadAttention(
                    num_heads=self.num_heads,
                    key_size=hidden_dim // self.num_heads,
                    w_init_scale=2.0
                )(sample_vars, sample_vars, sample_vars)
                
                attended.append(attn_output)
            
            x_attended = jnp.stack(attended, axis=0)  # [n_samples, n_vars, hidden_dim]
            
            # Residual connection and layer norm
            x = x + x_attended
            x = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)
            
            # Feedforward network
            ff_output = hk.nets.MLP([4 * hidden_dim, hidden_dim])(x)
            x = x + ff_output
            x = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x)
            
            # Apply dropout if training
            if is_training:
                x = hk.dropout(hk.next_rng_key(), self.dropout, x)
            
            return x


class AcquisitionPolicyNetwork(hk.Module):
    """
    Two-headed policy network for intervention selection with alternating attention.
    
    Combines proven transformer architecture with our dual-objective
    design for optimization + structure learning.
    """
    
    def __init__(self,
                 hidden_dim: int = 128,
                 num_layers: int = 4,
                 num_heads: int = 8,
                 dropout: float = 0.1,
                 name="AcquisitionPolicyNetwork"):
        super().__init__(name=name)
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.dropout = dropout

    def __call__(self, state: AcquisitionState, is_training: bool = True) -> Dict[str, jnp.ndarray]:
        """
        Forward pass for intervention selection.
        
        Args:
            state: Current acquisition state
            is_training: Training mode flag
            
        Returns:
            {
                'variable_logits': [n_vars] - Which variable to intervene on
                'value_params': [n_vars, 2] - (mean, log_std) for intervention values
                'state_value': [] - State value estimate (for GRPO baseline)
            }
        """
        # Convert state to history format and encode with alternating attention
        history = state.to_history_format()  # [n_samples, n_vars, 2]
        
        encoder = AlternatingAttentionEncoder(
            num_layers=self.num_layers,
            num_heads=self.num_heads, 
            hidden_dim=self.hidden_dim,
            dropout=self.dropout
        )
        
        # Get state embedding: [n_vars, hidden_dim]
        state_embedding = encoder(history, is_training)
        
        # Variable selection head (leverages uncertainty information)
        variable_logits = self._variable_selection_head(state_embedding, state)
        
        # Value selection head
        value_params = self._value_selection_head(state_embedding, state)
        
        # State value estimation (for GRPO)
        state_value = self._state_value_head(state_embedding)
        
        return {
            'variable_logits': variable_logits,
            'value_params': value_params,  
            'state_value': state_value
        }

    def _variable_selection_head(self, 
                                state_emb: jnp.ndarray,  # [n_vars, hidden_dim]
                                state: AcquisitionState) -> jnp.ndarray:
        """
        Select which variable to intervene on using uncertainty information.
        
        Leverages rich uncertainty information from our ParentSetPosterior
        rather than simple thresholding approaches.
        """
        # Incorporate marginal parent probabilities as features
        marginal_probs = jnp.array(list(state.marginal_parent_probs.values()))  # [n_vars]
        
        # Combine state embedding with uncertainty features
        uncertainty_features = jnp.expand_dims(marginal_probs, axis=1)  # [n_vars, 1]
        combined_features = jnp.concatenate([state_emb, uncertainty_features], axis=1)
        
        # MLP for variable selection
        x = hk.Linear(self.hidden_dim)(combined_features)
        x = jax.nn.relu(x)
        x = hk.Linear(self.hidden_dim // 2)(x) 
        x = jax.nn.relu(x)
        variable_logits = hk.Linear(1)(x).squeeze(-1)  # [n_vars]
        
        return variable_logits

    def _value_selection_head(self,
                             state_emb: jnp.ndarray,  # [n_vars, hidden_dim] 
                             state: AcquisitionState) -> jnp.ndarray:
        """
        Select intervention values using optimization context.
        
        Returns parameters for normal distribution over intervention values.
        """
        # Add optimization context (current best value)
        best_value_feature = jnp.full((state_emb.shape[0], 1), state.best_value)
        augmented_features = jnp.concatenate([state_emb, best_value_feature], axis=1)
        
        # MLP for value parameters
        x = hk.Linear(self.hidden_dim)(augmented_features)
        x = jax.nn.relu(x)
        x = hk.Linear(2)(x)  # [n_vars, 2] - mean and log_std for each variable
        
        # Apply constraints: log_std should be reasonable
        means = x[:, 0]
        log_stds = jnp.clip(x[:, 1], -2, 2)  # Reasonable variance range
        
        return jnp.stack([means, log_stds], axis=1)

    def _state_value_head(self, state_emb: jnp.ndarray) -> jnp.ndarray:
        """State value estimation for GRPO baseline."""
        # Global pooling over variables
        global_features = jnp.mean(state_emb, axis=0)  # [hidden_dim]
        
        # MLP for state value
        x = hk.Linear(self.hidden_dim // 2)(global_features)
        x = jax.nn.relu(x)
        state_value = hk.Linear(1)(x).squeeze(-1)  # []
        
        return state_value


# Factory functions
def create_acquisition_policy(
    config: Dict[str, Any],
    example_state: AcquisitionState
) -> Any:
    """Create and initialize acquisition policy network."""
    
    def policy_fn(state: AcquisitionState, is_training: bool = True):
        policy = AcquisitionPolicyNetwork(
            hidden_dim=config.get('hidden_dim', 128),
            num_layers=config.get('num_layers', 4),
            num_heads=config.get('num_heads', 8),
            dropout=config.get('dropout', 0.1)
        )
        return policy(state, is_training)
    
    return hk.transform(policy_fn)

def sample_intervention_from_policy(
    policy_output: Dict[str, jnp.ndarray],
    state: AcquisitionState,
    key: jax.Array,
    exploration_noise: float = 0.1
) -> pyr.PMap:
    """Sample intervention from policy network output."""
    variable_logits = policy_output['variable_logits']
    value_params = policy_output['value_params']
    
    # Sample variable to intervene on
    var_key, val_key = jax.random.split(key)
    
    # Add exploration noise to variable selection
    noisy_logits = variable_logits + exploration_noise * jax.random.normal(var_key, variable_logits.shape)
    selected_var_idx = jnp.argmax(noisy_logits)
    
    # Sample intervention value for selected variable
    mean, log_std = value_params[selected_var_idx]
    std = jnp.exp(log_std)
    intervention_value = mean + std * jax.random.normal(val_key)
    
    # Get variable names
    all_vars = list(state.buffer.get_variable_coverage())
    selected_var = all_vars[selected_var_idx]
    
    return create_perfect_intervention(
        targets=frozenset([selected_var]),
        values={selected_var: float(intervention_value)}
    )
```

### **Key Features:**
- **Alternating attention architecture** for proper symmetry encoding
- **Two-headed design** leveraging our uncertainty infrastructure
- **Optimization awareness** using target value context
- **Rich uncertainty integration** via marginal parent probabilities

---

## ðŸŽ **Component 3: Multi-Component Verifiable Rewards**
**Priority**: HIGH | **Effort**: 2-3 hours

### **File**: `src/causal_bayes_opt/acquisition/rewards.py`

```python
@dataclass(frozen=True)
class RewardComponents:
    """Decomposed reward components for dual-objective learning."""
    optimization_reward: float        # Target variable improvement (not in CAASL)
    structure_discovery_reward: float # AVICI improvement (like CAASL)
    parent_intervention_reward: float # Bonus for intervening on likely parents
    exploration_bonus: float          # Encourage diverse interventions
    total_reward: float
    metadata: pyr.PMap[str, Any] = pyr.m()

def compute_verifiable_reward(
    state_before: AcquisitionState,
    intervention: pyr.PMap,
    outcome: pyr.PMap,
    state_after: AcquisitionState,
    config: pyr.PMap
) -> RewardComponents:
    """
    Compute verifiable reward for dual objectives.
    
    Balances multiple objectives:
    1. Target variable optimization (key for our use case)
    2. Structure discovery (information gain) 
    3. Intervention quality bonuses
    4. Exploration incentives
    """
    # 1. Optimization reward: target variable improvement (CRITICAL for our use case)
    opt_reward = _compute_optimization_reward(
        state_before, outcome, config.get('target_variable')
    )
    
    # 2. Structure discovery reward: AVICI improvement (like CAASL)
    struct_reward = _compute_structure_discovery_reward(
        state_before.posterior, state_after.posterior
    )
    
    # 3. Parent intervention reward: guide structure learning
    parent_reward = _compute_parent_intervention_reward(
        intervention, state_before.marginal_parent_probs
    )
    
    # 4. Exploration bonus: prevent mode collapse
    exploration_bonus = _compute_exploration_bonus(
        intervention, state_before.buffer, config.get('exploration_weight', 0.1)
    )
    
    # Combine with learnable weights
    weights = config.get('reward_weights', {
        'optimization': 1.0,      # Primary objective
        'structure': 0.5,         # Secondary objective  
        'parent': 0.3,           # Learning guidance
        'exploration': 0.1       # Diversity maintenance
    })
    
    total = (
        weights['optimization'] * opt_reward +
        weights['structure'] * struct_reward + 
        weights['parent'] * parent_reward +
        weights['exploration'] * exploration_bonus
    )
    
    return RewardComponents(
        optimization_reward=opt_reward,
        structure_discovery_reward=struct_reward,
        parent_intervention_reward=parent_reward,
        exploration_bonus=exploration_bonus,
        total_reward=total,
        metadata=pyr.m({
            'weights_used': weights,
            'target_variable': config.get('target_variable'),
            'intervention_type': intervention['type'],
            'intervention_targets': intervention['targets']
        })
    )

def _compute_optimization_reward(
    state_before: AcquisitionState,
    outcome: pyr.PMap,
    target_variable: str
) -> float:
    """
    Reward based on target variable improvement.
    
    This is a key component for optimization objectives
    that pure structure learning approaches don't include.
    """
    target_value = outcome['values'][target_variable]
    improvement = target_value - state_before.best_value
    
    # Tanh normalization for bounded rewards
    return float(jnp.tanh(improvement))

def _compute_structure_discovery_reward(
    posterior_before: ParentSetPosterior,
    posterior_after: ParentSetPosterior  
) -> float:
    """
    Reward based on information gain using our posterior representation.
    
    Uses uncertainty reduction rather than simple accuracy metrics
    for more nuanced structure discovery guidance.
    """
    if posterior_before.target_variable != posterior_after.target_variable:
        return 0.0
    
    uncertainty_reduction = posterior_before.uncertainty - posterior_after.uncertainty
    
    # Normalize by maximum possible uncertainty reduction
    max_uncertainty = posterior_before.uncertainty
    if max_uncertainty > 0:
        normalized_reduction = uncertainty_reduction / max_uncertainty
        return float(jnp.clip(normalized_reduction, 0.0, 1.0))
    else:
        return 0.0

def _compute_parent_intervention_reward(
    intervention: pyr.PMap,
    marginal_parent_probs: Dict[str, float]
) -> float:
    """Reward for intervening on variables likely to be parents."""
    if intervention['type'] != 'perfect':
        return 0.0
    
    targets = intervention['targets']
    if not targets:
        return 0.0
    
    # Average parent probability of intervened variables
    target_probs = [marginal_parent_probs.get(var, 0.0) for var in targets]
    avg_parent_prob = sum(target_probs) / len(target_probs)
    
    return float(avg_parent_prob)

def _compute_exploration_bonus(
    intervention: pyr.PMap,
    buffer: ExperienceBuffer, 
    weight: float
) -> float:
    """Bonus for exploring under-sampled intervention types."""
    if intervention['type'] != 'perfect':
        return 0.0
    
    targets = intervention['targets']
    
    # Count previous interventions on these targets
    previous_count = len(buffer.filter_interventions_by_targets(targets))
    total_interventions = buffer.num_interventions()
    
    if total_interventions == 0:
        return weight
    
    # Inverse frequency bonus
    frequency = previous_count / total_interventions
    bonus = weight * (1.0 - frequency)
    
    return float(jnp.clip(bonus, 0.0, weight))
```

### **Key Features:**
- **Dual objectives**: Optimization + structure learning
- **Verifiable components**: No human feedback required
- **Decomposed analysis**: Understand what drives learning
- **Configurable weights**: Adapt to different problem requirements

---

## âœ… **Component 4: GRPO Algorithm Implementation** 
**STATUS**: COMPLETE âœ…
**Priority**: CRITICAL | **Effort**: 4-5 hours

### **File**: `src/causal_bayes_opt/acquisition/grpo.py`

```python
@dataclass
class GRPOConfig:
    """Configuration for GRPO algorithm - better than SAC for multi-objective."""
    group_size: int = 8
    kl_coeff: float = 0.1
    clip_ratio: float = 0.2
    value_loss_coeff: float = 0.5
    entropy_coeff: float = 0.01
    max_grad_norm: float = 1.0
    learning_rate: float = 3e-4

@dataclass  
class GRPOUpdate:
    """Results from a GRPO update step."""
    policy_loss: float
    value_loss: float
    entropy_loss: float
    total_loss: float
    kl_divergence: float
    explained_variance: float
    grad_norm: float

def create_grpo_trainer(
    policy_network: Any,
    config: GRPOConfig
) -> Tuple[Any, Any]:
    """
    Create GRPO training infrastructure.
    
    GRPO is well-suited for our multi-objective setting because:
    - Group-based advantages handle multi-objective variance well
    - No separate value network needed (simpler)
    - Designed for heterogeneous reward settings
    """
    optimizer = optax.chain(
        optax.clip_by_global_norm(config.max_grad_norm),
        optax.adam(learning_rate=config.learning_rate)
    )
    
    def grpo_update_step(
        params: Any,
        opt_state: Any,
        batch_data: Dict[str, jnp.ndarray],
        config: GRPOConfig
    ) -> Tuple[Any, Any, GRPOUpdate]:
        """Single GRPO update step."""
        
        def loss_fn(params):
            return _compute_grpo_loss(params, batch_data, policy_network, config)
        
        # Compute loss and gradients
        (loss_value, loss_info), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)
        
        # Apply updates
        updates, new_opt_state = optimizer.update(grads, opt_state, params)
        new_params = optax.apply_updates(params, updates)
        
        # Compute gradient norm
        grad_norm = optax.global_norm(grads)
        
        return new_params, new_opt_state, GRPOUpdate(
            policy_loss=loss_info['policy_loss'],
            value_loss=loss_info['value_loss'], 
            entropy_loss=loss_info['entropy_loss'],
            total_loss=loss_value,
            kl_divergence=loss_info['kl_divergence'],
            explained_variance=loss_info['explained_variance'],
            grad_norm=grad_norm
        )
    
    return grpo_update_step, optimizer.init

def _compute_grpo_loss(
    params: Any,
    batch_data: Dict[str, jnp.ndarray],
    policy_network: Any,
    config: GRPOConfig
) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:
    """
    Compute GRPO loss using group-based advantage estimation.
    
    Key advantage for multi-objective: uses group statistics as baseline
    instead of learned value function, which is more stable when 
    rewards have multiple conflicting components.
    """
    states = batch_data['states']  # [group_size, ...]
    actions = batch_data['actions']  # [group_size, ...]  
    rewards = batch_data['rewards']  # [group_size] - our multi-component rewards
    old_log_probs = batch_data['old_log_probs']  # [group_size]
    
    # Forward pass through policy
    policy_outputs = jax.vmap(policy_network.apply, in_axes=(None, 0))(params, states)
    
    # Compute action log probabilities
    new_log_probs = _compute_action_log_probs(policy_outputs, actions)
    state_values = policy_outputs['state_value']  # [group_size]
    
    # GRPO: Use group mean as baseline (handles multi-objective variance)
    group_baseline = jnp.mean(rewards)
    advantages = rewards - group_baseline
    
    # Normalize advantages within group
    advantages = (advantages - jnp.mean(advantages)) / (jnp.std(advantages) + 1e-8)
    
    # Policy loss: clipped probability ratio
    ratio = jnp.exp(new_log_probs - old_log_probs)
    clipped_ratio = jnp.clip(ratio, 1 - config.clip_ratio, 1 + config.clip_ratio)
    policy_loss = -jnp.mean(jnp.minimum(ratio * advantages, clipped_ratio * advantages))
    
    # Value loss: state values should predict group-adjusted rewards
    value_targets = rewards - group_baseline + jnp.mean(rewards)
    value_loss = jnp.mean((state_values - value_targets) ** 2)
    
    # Entropy loss: encourage exploration  
    entropy = _compute_entropy(policy_outputs)
    entropy_loss = -jnp.mean(entropy)
    
    # KL divergence for monitoring
    kl_div = jnp.mean(new_log_probs - old_log_probs)
    
    # Explained variance for monitoring
    var_y = jnp.var(value_targets)
    explained_var = 1 - jnp.var(value_targets - state_values) / (var_y + 1e-8)
    
    # Total loss
    total_loss = (
        policy_loss + 
        config.value_loss_coeff * value_loss + 
        config.entropy_coeff * entropy_loss
    )
    
    loss_info = {
        'policy_loss': policy_loss,
        'value_loss': value_loss,
        'entropy_loss': entropy_loss,
        'kl_divergence': kl_div,
        'explained_variance': explained_var
    }
    
    return total_loss, loss_info

def collect_grpo_batch(
    policy_network: Any,
    params: Any,
    scm: pyr.PMap,
    state: AcquisitionState,
    config: GRPOConfig,
    reward_config: pyr.PMap,
    key: jax.Array
) -> Dict[str, jnp.ndarray]:
    """Collect a batch of experience for GRPO training."""
    keys = jax.random.split(key, config.group_size)
    
    batch_states = []
    batch_actions = []
    batch_rewards = []
    batch_log_probs = []
    
    current_state = state
    
    for i in range(config.group_size):
        # Get policy output for current state
        policy_output = policy_network.apply(params, current_state)
        
        # Sample intervention from policy
        intervention = sample_intervention_from_policy(policy_output, current_state, keys[i])
        
        # Apply intervention and get outcome
        outcome = sample_with_intervention(scm, intervention, n_samples=1, seed=int(keys[i][0]))[0]
        
        # Update state with new outcome (this requires surrogate model prediction)
        new_buffer = current_state.buffer.copy()
        new_buffer.add_intervention(intervention, outcome)
        
        # Get new posterior (would need surrogate model prediction)
        # new_posterior = predict_parent_posterior(...)
        # new_state = create_acquisition_state(...)
        
        # Compute multi-component reward
        reward_components = compute_verifiable_reward(
            current_state, intervention, outcome, new_state, reward_config
        )
        
        # Store batch data
        batch_states.append(current_state)
        batch_actions.append(intervention)
        batch_rewards.append(reward_components.total_reward)
        batch_log_probs.append(_compute_action_log_prob(policy_output, intervention))
        
        # Update state for next iteration
        current_state = new_state
    
    return {
        'states': jnp.array(batch_states),
        'actions': jnp.array(batch_actions),
        'rewards': jnp.array(batch_rewards),
        'old_log_probs': jnp.array(batch_log_probs)
    }
```

### **Key Features:**
- **Multi-objective optimized**: Group-based advantages handle conflicting reward components
- **Stable training**: Clipped probability ratios prevent large updates
- **No value network required**: Simpler architecture for our setting
- **Comprehensive monitoring**: Track KL divergence, explained variance, etc.

---

## âœ… **Component 5: Exploration Strategies**
**STATUS**: COMPLETE âœ…
**Priority**: MEDIUM | **Effort**: 2 hours

### **File**: `src/causal_bayes_opt/acquisition/exploration.py`

```python
class UncertaintyGuidedExploration:
    """
    Exploration strategy leveraging our rich uncertainty infrastructure.
    
    Uses ParentSetPosterior uncertainty plus optimization progress 
    to guide exploration vs. exploitation decisions.
    """
    
    def __init__(self, 
                 uncertainty_weight: float = 1.0,
                 count_weight: float = 0.1,
                 temperature: float = 1.0):
        self.uncertainty_weight = uncertainty_weight
        self.count_weight = count_weight 
        self.temperature = temperature
    
    def compute_exploration_bonus(
        self,
        state: AcquisitionState,
        candidate_intervention: pyr.PMap
    ) -> float:
        """Compute exploration bonus for candidate intervention."""
        
        # Epistemic uncertainty: encourage exploration when posterior is uncertain
        epistemic_bonus = self.uncertainty_weight * state.uncertainty_bits
        
        # Count-based bonus: encourage under-explored variable combinations
        count_bonus = self._compute_count_bonus(candidate_intervention, state.buffer)
        
        # Variable uncertainty: prefer variables with uncertain parent status
        var_uncertainty_bonus = self._compute_variable_uncertainty_bonus(
            candidate_intervention, state.marginal_parent_probs
        )
        
        total_bonus = epistemic_bonus + count_bonus + var_uncertainty_bonus
        
        return total_bonus / self.temperature
    
    def _compute_count_bonus(self, intervention: pyr.PMap, buffer: ExperienceBuffer) -> float:
        """Bonus inversely proportional to intervention frequency."""
        targets = intervention['targets']
        count = len(buffer.filter_interventions_by_targets(targets))
        total = buffer.num_interventions()
        
        if total == 0:
            return self.count_weight
        
        frequency = count / total
        return self.count_weight * (1.0 - frequency)
    
    def _compute_variable_uncertainty_bonus(
        self, 
        intervention: pyr.PMap, 
        marginal_probs: Dict[str, float]
    ) -> float:
        """Bonus for variables with uncertain parent status (prob ~0.5)."""
        targets = intervention['targets']
        uncertainties = []
        
        for var in targets:
            prob = marginal_probs.get(var, 0.0)
            # Maximum uncertainty at prob=0.5
            uncertainty = 1.0 - 2.0 * abs(prob - 0.5)
            uncertainties.append(uncertainty)
        
        return float(jnp.mean(jnp.array(uncertainties)))

class AdaptiveExploration:
    """
    Adaptive exploration balancing optimization and structure discovery.
    
    Adapts exploration based on both optimization progress and 
    structural uncertainty - our dual-objective advantage.
    """
    
    def __init__(self,
                 initial_temperature: float = 2.0,
                 final_temperature: float = 0.1,
                 adaptation_steps: int = 1000):
        self.initial_temp = initial_temperature
        self.final_temp = final_temperature 
        self.adaptation_steps = adaptation_steps
    
    def get_exploration_temperature(self, step: int, state: AcquisitionState) -> float:
        """Get temperature based on step and optimization progress."""
        base_progress = min(step / self.adaptation_steps, 1.0)
        
        # Adjust based on optimization stagnation
        if hasattr(state, 'optimization_stagnation_steps'):
            stagnation_bonus = min(state.optimization_stagnation_steps / 100, 0.5)
            base_progress = max(0, base_progress - stagnation_bonus)
        
        temperature = self.initial_temp * (1 - base_progress) + self.final_temp * base_progress
        return temperature
    
    def should_explore(self, state: AcquisitionState, step: int) -> bool:
        """Decide whether to prioritize exploration vs exploitation."""
        temperature = self.get_exploration_temperature(step, state)
        
        # High uncertainty OR optimization stagnation -> explore
        if state.uncertainty_bits > 2.0:
            return True
        
        # Check for optimization progress stagnation
        recent_improvements = getattr(state, 'recent_target_improvements', [])
        if len(recent_improvements) > 5 and max(recent_improvements[-5:]) < 0.01:
            return True  # Explore if optimization has stagnated
        
        # Temperature-dependent exploration
        explore_prob = temperature / self.initial_temp
        return jax.random.bernoulli(jax.random.PRNGKey(step), explore_prob)
```

### **Key Features:**
- **Expected Information Gain**: Fixed epistemic bonus to predict intervention-specific information gain
- **Count-Based Exploration**: Encourages under-explored intervention combinations
- **Variable Uncertainty Bonuses**: Maximized at marginal probability ~0.5
- **Adaptive Temperature Scheduling**: Adjusts exploration based on optimization progress
- **Stagnation Detection**: Increases exploration when optimization plateaus

### **Implementation Notes:**
- Fixed critical design flaw where epistemic bonus was state-only (now intervention-specific)
- Removed hacky dummy state in `get_exploration_bonus_schedule` 
- Comprehensive test coverage with 28 passing tests
- Clean integration with `AcquisitionState` and `ExperienceBuffer`

---

## â­ï¸ **Next: Phase 4 Training Infrastructure**

**Now that Phase 3 is complete**, the next phase focuses on:

### **Enhanced Training Configuration System** âœ… **STARTED**
- Pydantic-based configuration with comprehensive validation
- Open-r1 enhancements: sample reuse, configurable scaling, zero KL penalty
- Immutable configuration objects with type safety
- **File**: `src/causal_bayes_opt/training/config.py`

### **Multi-Stage Training Pipeline** 
- PARENT_SCALE expert demonstration collection
- Curriculum learning with progressive difficulty
- End-to-end training with dual objectives
- Integration of exploration strategies

```python
@dataclass
class TrainingConfig:
    """Configuration for dual-objective training pipeline."""
    # GRPO settings
    grpo_config: GRPOConfig
    
    # Reward settings for dual objectives
    reward_weights: Dict[str, float] = field(default_factory=lambda: {
        'optimization': 1.0,    # Primary objective 
        'structure': 0.5,       # Secondary objective
        'parent': 0.3,         # Learning guidance
        'exploration': 0.1     # Diversity
    })
    
    # Training schedule
    total_steps: int = 10000
    surrogate_update_frequency: int = 50
    evaluation_frequency: int = 100
    checkpoint_frequency: int = 500
    
    # Exploration (adapted for dual objectives)
    exploration_strategy: str = "uncertainty_guided"
    exploration_kwargs: Dict[str, Any] = None
    
    # Curriculum learning
    curriculum_enabled: bool = True
    curriculum_schedule: Dict[str, Any] = None

class ACBOTrainingPipeline:
    """
    End-to-end training pipeline for dual-objective ACBO.
    
    Manages both optimization and structure learning objectives
    with sophisticated reward balancing and exploration strategies.
    """
    
    def __init__(self,
                 config: TrainingConfig,
                 scm: pyr.PMap,
                 initial_buffer: ExperienceBuffer,
                 surrogate_model: Any,
                 acquisition_model: Any):
        self.config = config
        self.scm = scm
        self.buffer = initial_buffer
        self.surrogate_model = surrogate_model
        self.acquisition_model = acquisition_model
        
        # Initialize training components
        self.grpo_trainer, self.grpo_opt_state = create_grpo_trainer(
            acquisition_model, config.grpo_config
        )
        
        self.exploration_strategy = create_exploration_strategy(
            config.exploration_strategy, 
            **(config.exploration_kwargs or {})
        )
        
        # Training state
        self.step = 0
        self.surrogate_params = None
        self.acquisition_params = None
        self.training_history = []
        
        # Dual-objective tracking
        self.best_optimization_value = float('-inf')
        self.optimization_stagnation_steps = 0
    
    def train(self, num_steps: int) -> Dict[str, Any]:
        """Run dual-objective training for specified steps."""
        logger.info(f"Starting dual-objective ACBO training for {num_steps} steps")
        
        for step in range(num_steps):
            self.step = step
            
            # 1. Update surrogate model if needed
            if step % self.config.surrogate_update_frequency == 0:
                self._update_surrogate_model()
            
            # 2. Create current acquisition state
            current_state = self._create_current_state()
            
            # 3. Track optimization progress
            self._update_optimization_tracking(current_state)
            
            # 4. Collect GRPO batch and update acquisition model
            update_info = self._grpo_update_step(current_state)
            
            # 5. Log progress
            if step % 10 == 0:
                self._log_dual_objective_progress(step, update_info, current_state)
            
            # 6. Evaluate periodically
            if step % self.config.evaluation_frequency == 0:
                eval_results = self._evaluate_dual_objectives()
                self.training_history.append({
                    'step': step,
                    'update_info': update_info,
                    'eval_results': eval_results,
                    'optimization_value': current_state.best_value,
                    'structure_uncertainty': current_state.uncertainty_bits
                })
            
            # 7. Checkpoint if needed
            if step % self.config.checkpoint_frequency == 0:
                self._save_checkpoint(step)
        
        return self._create_training_summary()
    
    def _update_optimization_tracking(self, state: AcquisitionState):
        """Track optimization progress for adaptive exploration."""
        if state.best_value > self.best_optimization_value:
            self.best_optimization_value = state.best_value
            self.optimization_stagnation_steps = 0
        else:
            self.optimization_stagnation_steps += 1
    
    def _evaluate_dual_objectives(self) -> Dict[str, float]:
        """Evaluate both optimization and structure learning performance."""
        state = self._create_current_state()
        
        # Optimization metrics
        target_improvement = state.best_value - self._get_baseline_target_value()
        
        # Structure learning metrics
        structure_accuracy = self._evaluate_structure_discovery()
        uncertainty_reduction = self._evaluate_uncertainty_reduction()
        
        # Intervention quality metrics
        intervention_diversity = self._evaluate_intervention_diversity()
        
        return {
            'target_improvement': target_improvement,
            'structure_accuracy': structure_accuracy,
            'uncertainty_reduction': uncertainty_reduction,
            'intervention_diversity': intervention_diversity,
            'optimization_stagnation_steps': self.optimization_stagnation_steps,
            'buffer_size': self.buffer.size()
        }
    
    def _log_dual_objective_progress(self, step: int, update_info: GRPOUpdate, state: AcquisitionState):
        """Log progress for both objectives."""
        logger.info(
            f"Step {step}: "
            f"Target={state.best_value:.3f}, "
            f"Uncertainty={state.uncertainty_bits:.3f}, "
            f"Policy Loss={update_info.policy_loss:.3f}, "
            f"Stagnation={self.optimization_stagnation_steps}"
        )
```

---

### **Evaluation Framework** (Moved to Phase 5)
Comprehensive evaluation moved to Phase 5 for proper system integration:
- Dual-objective performance metrics
- Pareto efficiency assessment  
- Training diagnostics and visualization
- Benchmark comparisons with exact methods

---

## âš¡ **Implementation Timeline**

### **Week 1: Core Architecture (6-8 hours)**
- **Day 1-2**: Rich state representation and alternating attention architecture
- **Day 3-4**: Multi-component rewards and GRPO implementation  
- **Day 5**: Integration testing and dual-objective validation

### **Week 2: Training & Evaluation (4-6 hours)**
- **Day 1-2**: Dual-objective training pipeline and exploration strategies
- **Day 3**: Evaluation framework for both optimization and structure learning
- **Day 4**: Comprehensive testing and validation

### **Week 3: Optimization & Polish (2-3 hours)**
- **Day 1**: Performance optimization and hyperparameter tuning
- **Day 2**: Documentation and final integration tests

---

## âœ… **Phase 3 Success Criteria: ACHIEVED**

### **Functional Requirements: ALL COMPLETE**
- âœ… **Rich State Integration**: Clean integration of optimization tracking with structural uncertainty
- âœ… **Alternating Attention**: CAASL-proven transformer architecture for symmetry encoding
- âœ… **Enhanced GRPO Training**: Group-based learning for multi-objective rewards with open-r1 features
- âœ… **Dual-Objective Rewards**: Balanced optimization and structure discovery with verifiable components
- âœ… **Exploration Strategies**: Expected information gain with intervention-specific bonuses

### **Implementation Achievements:**
- **Rich State Representation**: AcquisitionState with optimization tracking and uncertainty integration
- **Policy Architecture**: Two-headed network with alternating attention transformer
- **Verifiable Rewards**: Multi-component system balancing optimization + structure learning
- **Literature-Compliant GRPO**: Enhanced with sample reuse, configurable scaling, 33 passing tests
- **Expected Information Gain**: Fixed epistemic bonus design for intervention-specific exploration

### **Integration Requirements: ALL SATISFIED**
- âœ… **Phase 1 Integration**: Seamless ExperienceBuffer usage for dual objectives
- âœ… **Phase 2 Integration**: ParentSetPosterior uncertainty guides both optimization and structure learning
- âœ… **Architectural Innovation**: Alternating attention + rich state + multi-component rewards
- âœ… **Extensibility**: Easy to add new objectives and reward components
- âœ… **Open-r1 Features**: Enhanced GRPO with modern training improvements

---

## ðŸŽ¯ **Phase 3: COMPLETE & READY FOR PHASE 4**

**Phase 3 successfully delivered a complete acquisition model with:**
- âœ… **Rich State Representation** with optimization tracking and uncertainty integration
- âœ… **Advanced Policy Architecture** using alternating attention transformers  
- âœ… **Multi-Component Verifiable Rewards** balancing optimization + structure learning
- âœ… **Enhanced GRPO Algorithm** with open-r1 features and comprehensive testing
- âœ… **Intelligent Exploration** using expected information gain per intervention

**Built on solid foundation from Phases 1 & 2:**
- âœ… **Robust data management** via ExperienceBuffer
- âœ… **Sophisticated uncertainty quantification** via ParentSetPosterior  
- âœ… **Clean intervention framework** with perfect interventions
- âœ… **Comprehensive AVICI integration** with validated training infrastructure

**Key innovations successfully implemented:**
- **Dual-objective design**: Balances optimization + structure learning
- **Literature-compliant GRPO**: Group-based advantages for multi-objective RL
- **Expected information gain**: Intervention-specific exploration bonuses
- **Enhanced training**: Open-r1 features for improved performance

**Ready to proceed to Phase 4: Training Infrastructure!** ðŸš€