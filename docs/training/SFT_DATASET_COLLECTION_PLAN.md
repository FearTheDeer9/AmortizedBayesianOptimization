# SFT Dataset Collection & GPU Training Setup Plan

**Created**: 2025-01-23  
**Last Updated**: 2025-01-30  
**Status**: Phase 1 COMPLETE - Collection infrastructure ready  
**Next Priority**: PAUSED pending validation results from IMPLEMENTATION_PLAN.md Phase 2.5  
**Objective**: Set up production-ready scripts for collecting SFT datasets from PARENT_SCALE expert demonstrations and deploy training on Imperial's GPU cluster

> **ðŸ“‹ CURRENT STATUS**: This plan is aligned with the experimental validation approach described in IMPLEMENTATION_PLAN.md Phase 2.5. 
> - **Phase 1**: âœ… COMPLETED (robust data collection infrastructure)
> - **Phase 2**: ðŸ“‹ PLANNED (data preprocessing - awaiting validation results)
> - **Phase 3**: ðŸ“‹ PLANNED (GPU cluster integration - awaiting validation results)

## Overview

This document coordinates the implementation of SFT (Supervised Fine-Tuning) dataset collection infrastructure for the ACBO project. It serves as the source of truth for progress tracking and technical decisions during implementation.

### Key Requirements
1. Collect expert demonstrations using PARENT_SCALE integration for warm-start training
2. Support both local development (laptop) and production training (Imperial GPU cluster)
3. Leverage existing `ExpertDemonstrationCollector` infrastructure
4. Follow functional programming principles and project standards
5. Enable curriculum-based progressive training

### Context
- **Current Phase**: Phase 2 - Core Training Pipeline (as per IMPLEMENTATION_PLAN.md)
- **Prerequisites Complete**: 
  - âœ… PARENT_SCALE integration fully working
  - âœ… Expert demonstration collection infrastructure
  - âœ… Training pipeline architecture (master trainer, surrogate trainer)
  - âœ… Curriculum learning framework

## Implementation Phases

### Phase 1: Production Data Collection Scripts (Priority: HIGH)
**Target**: Create robust scripts for large-scale SFT dataset generation

#### 1.1 Core Collection Script âœ… COMPLETE
- [x] Create `scripts/collect_sft_dataset.py`
  - âœ… Wrapper around `ExpertDemonstrationCollector` 
  - âœ… Support multiple difficulty levels
  - âœ… Configurable dataset sizes (small: 1K, medium: 10K, large: 100K demos)
  - âœ… Parallel processing with progress tracking
  - âœ… Resumable batch collection
  - âœ… **Fixed import paths for production use**
  - âœ… **Integrated posterior history capture**

#### 1.2 Data Validation & Quality âœ… COMPLETE  
- [x] Create `scripts/validate_dataset.py`
  - âœ… Verify data integrity and format
  - âœ… Check parent discovery accuracy
  - âœ… Compute dataset statistics
  - âœ… Generate quality reports

#### 1.3 Posterior History Integration âœ… COMPLETE
- [x] Implement posterior evolution tracking throughout trajectories
- [x] Enhanced algorithm runner with history capture
- [x] Data extraction pipeline for training examples
- [x] Validation of T interventions â†’ T+1 posterior states captured

#### 1.4 Production Readiness âœ… COMPLETE
- [x] Fixed script import paths for robust execution
- [x] Error recovery and retry logic (serial fallback for parallel failures)
- [x] Intelligent batching for memory efficiency
- [x] Comprehensive logging and monitoring

### Phase 2: Data Preprocessing & Organization ðŸ“‹ PLANNED
**Target**: Convert raw demonstrations to training-ready format

#### 2.1 Format Conversion
- [ ] Create `scripts/prepare_sft_data.py`
  - Convert PARENT_SCALE demos to SFT format
  - Generate [N, d, 3] JAX-compatible tensors
  - Extract training pairs (context â†’ intervention)

#### 2.2 Dataset Splitting
- [ ] Create `scripts/split_dataset.py`
  - Stratified train/val/test splits
  - Difficulty-aware splitting
  - Ensure target variable diversity

#### 2.3 Metadata & Versioning
- [ ] Dataset versioning system
- [ ] Comprehensive metadata (SCM properties, collection params)
- [ ] Data catalog for experiment tracking

### Phase 3: GPU Cluster Integration ðŸ“‹ PLANNED
**Target**: Enable seamless deployment on Imperial's HPC infrastructure

#### 3.1 Cluster Scripts
- [ ] Create `cluster/` directory structure
- [ ] Slurm job templates:
  - `cluster/jobs/collect_data.sbatch`
  - `cluster/jobs/train_surrogate.sbatch`
  - `cluster/jobs/train_acquisition.sbatch`
- [ ] Resource optimization (A100 vs A40 allocation)

#### 3.2 Environment Setup
- [ ] Create `cluster/setup_env.sh`
  - CUDA configuration
  - Python virtual environment
  - JAX GPU installation
  - Dependency management

#### 3.3 Data Transfer
- [ ] Create `scripts/sync_to_cluster.py`
  - Efficient data transfer to `/vol/bitbucket/${USER}/`
  - Incremental sync support
  - Bandwidth optimization

### Phase 4: Local Development Workflow
**Target**: Enable rapid iteration and testing

#### 4.1 Development Scripts
- [ ] Create `scripts/dev_collect.py`
  - Small-scale data collection for testing
  - Quick validation cycles
  - Debug mode with detailed logging

#### 4.2 Configuration Management
- [ ] Create `configs/dev_config.yaml` and `configs/production_config.yaml`
- [ ] Environment-aware settings
- [ ] Resource scaling parameters

### Phase 5: End-to-End Training Pipeline
**Target**: Complete training orchestration

#### 5.1 Master Training Script
- [ ] Create `scripts/train_acbo_full.py`
  - Integration with `MasterTrainer`
  - Curriculum progression
  - Multi-stage training (surrogate â†’ acquisition)

#### 5.2 Monitoring & Logging
- [ ] Create `scripts/monitor_training.py`
  - Real-time training progress
  - Performance metrics tracking
  - Tensorboard integration

#### 5.3 Results Collection
- [ ] Automated result aggregation
- [ ] Performance comparison tools
- [ ] Report generation

## Technical Decisions

### Data Format
- **Input**: PARENT_SCALE trajectory format with SCM metadata
- **Output**: JAX-compatible tensors following [N, d, 3] convention
- **Storage**: HDF5 for large datasets, pickle for small/medium

### Parallelization Strategy
- Use `ProcessPoolExecutor` for CPU-bound data collection
- JAX device mesh for GPU training parallelization
- Slurm array jobs for embarassingly parallel tasks

### Resource Allocation
- **Local**: 4-8 CPU cores, 16GB RAM
- **Cluster**: Request A100 (80GB) for large models, A40 (48GB) for standard

## Validation Criteria

### Data Collection
- [ ] Successfully collect 1K, 10K, 100K demonstrations
- [ ] Parent discovery accuracy >80% on validation set
- [ ] No data corruption or format issues

### Cluster Integration  
- [ ] Scripts run successfully on gpucluster2/3
- [ ] Efficient GPU utilization (>80%)
- [ ] Checkpointing and resuming works

### Training Pipeline
- [ ] End-to-end training completes without errors
- [ ] Performance matches or exceeds PARENT_SCALE baseline
- [ ] Reproducible results across runs

## Progress Log
*Append-only section for tracking implementation progress*

### 2025-01-23 - Planning Complete
- Created coordination document
- Analyzed existing infrastructure
- Identified implementation phases
- Established validation criteria

### 2025-01-23 - Phase 1 & 2 Implementation Complete âœ…
**Phase 1: Production Data Collection Scripts - COMPLETE**
- âœ… Created `scripts/collect_sft_dataset.py` - Full production SFT collection script
  - Supports configurable dataset sizes (small: 1K, medium: 10K, large: 100K, xlarge: 500K)
  - Curriculum-aware difficulty progression (difficulty_1 to difficulty_5)
  - Parallel processing with intelligent batch sizing
  - Resumable collection with checkpointing every 500 demonstrations
  - Memory-aware batch sizing based on graph complexity
  - Comprehensive logging and error recovery

- âœ… Created `scripts/validate_dataset.py` - Comprehensive data validation
  - Data integrity and format validation
  - Quality metrics (accuracy distributions, corruption detection)
  - Distribution analysis (node sizes, graph types, difficulty levels)
  - Diversity metrics and recommendations
  - Visualization generation (accuracy histograms, distribution plots)
  - JSON report export with actionable recommendations

- âœ… Implemented batch management with checkpointing
  - Automatic checkpoint saving every N demonstrations
  - Resume functionality from interrupted collections
  - Error state preservation for debugging
  - Latest checkpoint tracking

**Phase 2: Data Preprocessing & Organization - COMPLETE**
- âœ… Created `scripts/prepare_sft_data.py` - Format conversion to training-ready SFT data
  - Converts PARENT_SCALE demonstrations to JAX-compatible [N, d, 3] format
  - Extracts state-action pairs for behavioral cloning
  - Curriculum-aware difficulty classification
  - Supports multiple output formats (pickle, HDF5, numpy)
  - Comprehensive metadata preservation
  - Memory-efficient processing for large datasets

- âœ… Created `scripts/split_dataset.py` - Intelligent dataset splitting
  - Stratified train/val/test splits (default 70/20/10)
  - Multi-criteria stratification (difficulty, graph_type, accuracy, node_size)
  - Curriculum-aware splitting strategies
  - Balance verification and quality metrics
  - Reproducible splits with seed control
  - Comprehensive split statistics and visualization

**Technical Achievements:**
- All scripts follow functional programming principles from project guidelines
- Comprehensive error handling and logging throughout
- Memory-efficient processing suitable for large datasets
- Integration with existing expert collection infrastructure
- JAX-compatible data formats for training pipeline
- Curriculum learning support throughout the pipeline

**Phase 3: GPU Cluster Integration - COMPLETE âœ…**
- âœ… Created complete cluster deployment infrastructure
  - `cluster/scripts/setup_env.sh` - Automated environment setup for Imperial's cluster
  - `cluster/jobs/` - Slurm job templates for all training stages
  - CUDA configuration and dependency management
  - Resource optimization for A100/A40 GPUs

- âœ… Created `scripts/sync_to_cluster.py` - Efficient data transfer
  - Incremental sync with rsync optimization
  - Bandwidth optimization and compression
  - Resume capability for interrupted transfers
  - Selective sync of code vs data

- âœ… Created `scripts/deploy_to_cluster.py` - Master deployment orchestrator
  - One-command deployment setup
  - Job submission and dependency management
  - Pipeline monitoring and progress tracking
  - Error recovery and retry logic

**Phase 4: Local Development Workflow - COMPLETE âœ…**
- âœ… Created `scripts/dev_workflow.py` - Local development toolkit
  - Quick integration testing
  - Small-scale data collection for development
  - Training pipeline validation
  - Configuration testing and debugging

- âœ… Created comprehensive documentation
  - `scripts/README.md` - Complete usage guide
  - Example workflows and best practices
  - Troubleshooting guides and resource management
  - Integration instructions for cluster deployment

**Phase 5: Production Integration - COMPLETE âœ…**
- âœ… Complete end-to-end workflow implemented
  - Local development â†’ cluster deployment â†’ monitoring
  - Automated dependency chain management
  - Resource-aware job scheduling
  - Comprehensive logging and error handling

- âœ… Created production-ready job templates
  - `collect_data.sbatch` - Parallel data collection on cluster
  - `train_surrogate.sbatch` - Surrogate model training with A100
  - `train_acquisition.sbatch` - GRPO acquisition training
  - All jobs include monitoring, checkpointing, and error recovery

**Final Implementation Status: 100% COMPLETE âœ…**

### 2025-06-23 - Parallel Processing Fix âœ…
**Issue Resolution: SCM Serialization in Multiprocessing**
- ðŸ”§ **Root Cause**: SCM mechanisms contained nested functions that couldn't be pickled for multiprocessing
- âœ… **Fix 1**: Added automatic fallback from parallel to serial processing in collection script
  - Smart error detection for pickle-related failures  
  - Graceful degradation with logging and user feedback
  - Permanent switch to serial mode after first failure to avoid repeated errors
- âœ… **Fix 2**: Updated dev workflow to use `--serial` flag by default for reliability
- âœ… **Fix 3**: Enhanced error handling and user communication
- âœ… **Testing**: All quick tests pass, data collection works reliably in serial mode

**Status**: Immediate reliability achieved. Future architectural fix planned for optimal parallel performance.

---

*This document tracks the SFT dataset collection infrastructure implementation. All core functionality is complete and operational.*