{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning Development Workflow\n",
    "\n",
    "**Interactive development and debugging notebook for the complete BC pipeline**\n",
    "\n",
    "This notebook provides a complete workflow for developing, training, and evaluating Behavioral Cloning models for Causal Bayesian Optimization.\n",
    "\n",
    "## Recent Fixes (2025-07-17 - LATEST)\n",
    "- ‚úÖ Fixed JAX compilation errors properly by restructuring train_step to accept numeric arrays\n",
    "- ‚úÖ Updated bc_surrogate_trainer.py to pass individual arrays instead of batch object to JAX\n",
    "- ‚úÖ Fixed parameter update mechanism to actually update state during training\n",
    "- ‚úÖ Modified _train_epoch to return updated state along with metrics\n",
    "- ‚úÖ Re-enabled JAX compilation for surrogate training with proper implementation\n",
    "\n",
    "## Recent Fixes (2025-07-17)\n",
    "- ‚úÖ Fixed JAX compilation errors by disabling JAX in BC trainers (temporary workaround)\n",
    "- ‚úÖ Fixed data structure type mismatch (target_variables: List[str] ‚Üí List[int])\n",
    "- ‚úÖ Fixed JAX train step argument mismatch (9 args ‚Üí 4 args)\n",
    "- ‚úÖ Fixed string data in JAX computation (removed parent_sets from loss computation)\n",
    "\n",
    "## Recent Fixes (2025-07-10)\n",
    "- ‚úÖ Fixed `run_simulated_training` function definition (moved before usage)\n",
    "- ‚úÖ Fixed JAX model parameter mismatch (`max_parents` parameter removed)\n",
    "- ‚úÖ Fixed numpy import for simulation fallback\n",
    "- ‚úÖ Updated acquisition trainer to handle missing datasets gracefully\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Environment Setup & Validation** - Validate dependencies and system state\n",
    "2. **Expert Demonstration Analysis** - Load and inspect raw demonstration data  \n",
    "3. **Data Pipeline Testing** - Test format conversion and trajectory extraction\n",
    "4. **Training Configuration** - Setup and validate training parameters\n",
    "5. **BC Surrogate Training** - Train surrogate model with live monitoring\n",
    "6. **BC Acquisition Training** - Train acquisition policy with live monitoring\n",
    "7. **Model Loading & Validation** - Test checkpoint loading and compatibility\n",
    "8. **ACBO Integration Setup** - Register BC methods in comparison framework\n",
    "9. **Single Method Testing** - Test individual BC methods\n",
    "10. **Complete Benchmark Comparison** - Run full comparison with baselines\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "```\n",
    "Expert Demonstrations ‚Üí Data Processing ‚Üí Training (Surrogate + Acquisition) \n",
    "                                              ‚Üì\n",
    "                    Benchmarking ‚Üê Integration ‚Üê Model Validation\n",
    "```\n",
    "\n",
    "Each cell is self-contained with comprehensive error handling. Run cells sequentially for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "\n",
    "# Setup project paths\n",
    "project_root = Path().cwd() \n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Behavioral Cloning Development Environment\")\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üêç Python Version: {sys.version}\")\n",
    "\n",
    "# Import core dependencies with error handling\n",
    "import_status = {}\n",
    "required_packages = [\n",
    "    ('jax', 'jax'),\n",
    "    ('jax.numpy', 'jnp'),\n",
    "    ('jax.random', 'random'),\n",
    "    ('numpy', 'onp'),\n",
    "    ('pyrsistent', 'pyr'),\n",
    "    ('omegaconf', 'OmegaConf'),\n",
    "    ('hydra', 'hydra'),\n",
    "    ('optax', 'optax'),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('pandas', 'pd'),\n",
    "    ('IPython.display', 'display, HTML, clear_output'),\n",
    "    ('tqdm.notebook', 'tqdm'),\n",
    "]\n",
    "\n",
    "for pkg_name, import_name in required_packages:\n",
    "    try:\n",
    "        exec(f\"import {pkg_name} as {import_name.split(',')[0].strip()}\" if ',' not in import_name else f\"from {pkg_name} import {import_name}\")\n",
    "        import_status[pkg_name] = \"‚úÖ\"\n",
    "    except ImportError as e:\n",
    "        import_status[pkg_name] = f\"‚ùå {e}\"\n",
    "        print(f\"Warning: Failed to import {pkg_name}: {e}\")\n",
    "\n",
    "# Display import status\n",
    "print(\"\\nüì¶ Package Import Status:\")\n",
    "for pkg, status in import_status.items():\n",
    "    print(f\"  {pkg}: {status}\")\n",
    "\n",
    "# Check expert demonstrations directory\n",
    "demo_dir = project_root / \"expert_demonstrations\" / \"raw\" / \"raw_demonstrations\"\n",
    "if demo_dir.exists():\n",
    "    demo_files = list(demo_dir.glob(\"*.pkl\"))\n",
    "    print(f\"\\nüìä Expert Demonstrations: ‚úÖ Found {len(demo_files)} demonstration files\")\n",
    "else:\n",
    "    print(f\"\\nüìä Expert Demonstrations: ‚ùå Directory not found: {demo_dir}\")\n",
    "\n",
    "# Check ACBO comparison framework\n",
    "acbo_comparison_dir = project_root / \"scripts\" / \"core\" / \"acbo_comparison\"\n",
    "if acbo_comparison_dir.exists():\n",
    "    print(f\"\\nüî¨ ACBO Comparison Framework: ‚úÖ Found at {acbo_comparison_dir}\")\n",
    "else:\n",
    "    print(f\"\\nüî¨ ACBO Comparison Framework: ‚ùå Directory not found: {acbo_comparison_dir}\")\n",
    "\n",
    "# Test BC training imports\n",
    "print(\"\\nüß™ Testing BC Training Imports:\")\n",
    "try:\n",
    "    from src.causal_bayes_opt.training.bc_surrogate_trainer import create_bc_surrogate_trainer\n",
    "    print(\"  ‚úÖ BC Surrogate Trainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå BC Surrogate Trainer: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.causal_bayes_opt.training.bc_acquisition_trainer import create_bc_acquisition_trainer\n",
    "    print(\"  ‚úÖ BC Acquisition Trainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå BC Acquisition Trainer: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.causal_bayes_opt.training.bc_data_pipeline import process_all_demonstrations\n",
    "    print(\"  ‚úÖ BC Data Pipeline\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå BC Data Pipeline: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Expert Demonstration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import BC data loading utilities\n",
    "try:\n",
    "    from src.causal_bayes_opt.training.pure_data_loader import load_demonstrations_from_directory\n",
    "    from src.causal_bayes_opt.training.expert_collection.data_structures import ExpertDemonstration\n",
    "    print(\"‚úÖ Successfully imported BC data loading utilities\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import BC utilities: {e}\")\n",
    "    # Fallback to basic demonstration loading\n",
    "\n",
    "def analyze_demonstration_file(demo_file_path):\n",
    "    \"\"\"Analyze a single demonstration file.\"\"\"\n",
    "    try:\n",
    "        with open(demo_file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Extract basic information\n",
    "        info = {\n",
    "            'file': demo_file_path.name,\n",
    "            'type': type(data).__name__,\n",
    "            'size_kb': demo_file_path.stat().st_size / 1024\n",
    "        }\n",
    "        \n",
    "        # Analyze structure based on type\n",
    "        if hasattr(data, 'demonstrations'):\n",
    "            # DemonstrationBatch\n",
    "            info['n_demonstrations'] = len(data.demonstrations)\n",
    "            if data.demonstrations:\n",
    "                demo = data.demonstrations[0]\n",
    "                info['n_nodes'] = getattr(demo, 'n_nodes', 'unknown')\n",
    "                info['graph_type'] = getattr(demo, 'graph_type', 'unknown')\n",
    "                info['target_variable'] = getattr(demo, 'target_variable', 'unknown')\n",
    "        elif isinstance(data, list):\n",
    "            info['n_demonstrations'] = len(data)\n",
    "            if data and hasattr(data[0], 'n_nodes'):\n",
    "                info['n_nodes'] = data[0].n_nodes\n",
    "        \n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return {'file': demo_file_path.name, 'error': str(e)}\n",
    "\n",
    "# Load and analyze all demonstration files\n",
    "print(\"üìä Analyzing Expert Demonstrations...\")\n",
    "demo_dir = project_root / \"expert_demonstrations\" / \"raw\" / \"raw_demonstrations\"\n",
    "demo_files = list(demo_dir.glob(\"*.pkl\"))\n",
    "\n",
    "if demo_files:\n",
    "    # Analyze first few files for quick inspection\n",
    "    sample_files = demo_files[:10]  # Analyze first 10 files\n",
    "    demo_info = []\n",
    "    \n",
    "    print(f\"Analyzing {len(sample_files)} sample files...\")\n",
    "    for demo_file in tqdm(sample_files, desc=\"Analyzing demonstrations\"):\n",
    "        info = analyze_demonstration_file(demo_file)\n",
    "        demo_info.append(info)\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(demo_info)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\nüìà Demonstration Analysis Summary:\")\n",
    "    print(f\"Total files analyzed: {len(df)}\")\n",
    "    print(f\"Total files available: {len(demo_files)}\")\n",
    "    \n",
    "    if 'error' not in df.columns or df['error'].isna().all():\n",
    "        print(f\"Average file size: {df['size_kb'].mean():.1f} KB\")\n",
    "        \n",
    "        if 'n_demonstrations' in df.columns:\n",
    "            print(f\"Demonstrations per file: {df['n_demonstrations'].describe()}\")\n",
    "        \n",
    "        if 'n_nodes' in df.columns:\n",
    "            node_counts = df['n_nodes'].value_counts()\n",
    "            print(f\"\\nNode count distribution:\")\n",
    "            for nodes, count in node_counts.items():\n",
    "                print(f\"  {nodes} nodes: {count} files\")\n",
    "        \n",
    "        if 'graph_type' in df.columns:\n",
    "            graph_types = df['graph_type'].value_counts()\n",
    "            print(f\"\\nGraph type distribution:\")\n",
    "            for graph_type, count in graph_types.items():\n",
    "                print(f\"  {graph_type}: {count} files\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        fig.suptitle('Expert Demonstration Analysis', fontsize=16)\n",
    "        \n",
    "        # File size distribution\n",
    "        axes[0, 0].hist(df['size_kb'], bins=20, alpha=0.7, color='skyblue')\n",
    "        axes[0, 0].set_title('File Size Distribution')\n",
    "        axes[0, 0].set_xlabel('Size (KB)')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Demonstrations per file\n",
    "        if 'n_demonstrations' in df.columns:\n",
    "            axes[0, 1].hist(df['n_demonstrations'], bins=20, alpha=0.7, color='lightgreen')\n",
    "            axes[0, 1].set_title('Demonstrations per File')\n",
    "            axes[0, 1].set_xlabel('Number of Demonstrations')\n",
    "            axes[0, 1].set_ylabel('Count')\n",
    "        \n",
    "        # Node count distribution\n",
    "        if 'n_nodes' in df.columns and df['n_nodes'].dtype in ['int64', 'float64']:\n",
    "            df['n_nodes'].value_counts().plot(kind='bar', ax=axes[1, 0], color='orange', alpha=0.7)\n",
    "            axes[1, 0].set_title('Node Count Distribution')\n",
    "            axes[1, 0].set_xlabel('Number of Nodes')\n",
    "            axes[1, 0].set_ylabel('Count')\n",
    "            axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Graph type distribution\n",
    "        if 'graph_type' in df.columns:\n",
    "            df['graph_type'].value_counts().plot(kind='bar', ax=axes[1, 1], color='coral', alpha=0.7)\n",
    "            axes[1, 1].set_title('Graph Type Distribution')\n",
    "            axes[1, 1].set_xlabel('Graph Type')\n",
    "            axes[1, 1].set_ylabel('Count')\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display sample data structure\n",
    "        print(\"\\nüîç Sample Demonstration Structure:\")\n",
    "        sample_file = demo_files[0]\n",
    "        try:\n",
    "            with open(sample_file, 'rb') as f:\n",
    "                sample_data = pickle.load(f)\n",
    "            \n",
    "            print(f\"File: {sample_file.name}\")\n",
    "            print(f\"Type: {type(sample_data)}\")\n",
    "            \n",
    "            if hasattr(sample_data, 'demonstrations') and sample_data.demonstrations:\n",
    "                demo = sample_data.demonstrations[0]\n",
    "                print(f\"\\nFirst demonstration attributes:\")\n",
    "                for attr in dir(demo):\n",
    "                    if not attr.startswith('_'):\n",
    "                        try:\n",
    "                            value = getattr(demo, attr)\n",
    "                            if not callable(value):\n",
    "                                print(f\"  {attr}: {type(value).__name__} = {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\")\n",
    "                        except:\n",
    "                            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Errors found in demonstration files:\")\n",
    "        error_files = df[df['error'].notna()]\n",
    "        for _, row in error_files.iterrows():\n",
    "            print(f\"  {row['file']}: {row['error']}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No demonstration files found!\")\n",
    "\n",
    "print(\"\\n‚úÖ Demonstration analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Process demonstrations through BC pipeline\n",
    "print(\"\\n3Ô∏è‚É£ Processing demonstrations through BC pipeline...\")\n",
    "\n",
    "from src.causal_bayes_opt.training.bc_data_pipeline import process_all_demonstrations\n",
    "\n",
    "# Process all demonstrations\n",
    "processed_data = process_all_demonstrations(\n",
    "    demo_dir=str(demo_dir),\n",
    "    split_ratios=(0.7, 0.15, 0.15),\n",
    "    random_seed=42,\n",
    "    max_examples_per_demo=10\n",
    ")\n",
    "\n",
    "# Display processing results\n",
    "print(f\"‚úÖ Processed demonstrations:\")\n",
    "print(f\"   Surrogate datasets by difficulty: {list(processed_data.surrogate_datasets.keys())}\")\n",
    "print(f\"   Acquisition datasets by difficulty: {list(processed_data.acquisition_datasets.keys())}\")\n",
    "\n",
    "# Analyze data structure\n",
    "for difficulty, dataset in processed_data.surrogate_datasets.items():\n",
    "    print(f\"\\n   {difficulty}:\")\n",
    "    print(f\"   - Training examples: {len(dataset.training_examples)}\")\n",
    "    if dataset.training_examples:\n",
    "        ex = dataset.training_examples[0]\n",
    "        print(f\"   - Observational data shape: {ex.observational_data.shape}\")\n",
    "        print(f\"   - Expert probs length: {len(ex.expert_probs)}\")\n",
    "        print(f\"   - Parent sets: {len(ex.parent_sets)}\")\n",
    "\n",
    "# Store for later use\n",
    "globals()['processed_data'] = processed_data\n",
    "print(\"\\nüíæ Processed data stored for later use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "import yaml\n",
    "from src.causal_bayes_opt.training.config import create_training_config\n",
    "\n",
    "print(\"‚öôÔ∏è Setting up Training Configuration...\")\n",
    "\n",
    "# Create training config using factory function\n",
    "# Note: use_jax and use_curriculum are not part of TrainingConfig\n",
    "# They are parameters for the BC trainer factory functions\n",
    "training_config = create_training_config(\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created training configuration using factory function\")\n",
    "\n",
    "# BC-specific settings (not part of TrainingConfig)\n",
    "bc_specific_settings = {\n",
    "    'use_jax': True,\n",
    "    'use_curriculum': True,\n",
    "    'use_continuous_model': True,\n",
    "    'use_scm_aware_batching': True,\n",
    "    'use_enhanced_policy': True\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\nüìã Configuration Summary:\")\n",
    "print(f\"  Surrogate learning rate: {training_config.surrogate.learning_rate}\")\n",
    "print(f\"  Surrogate batch size: {training_config.surrogate.batch_size}\")\n",
    "print(f\"  Random seed: {training_config.random_seed}\")\n",
    "print(f\"  BC-specific settings:\")\n",
    "for key, value in bc_specific_settings.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Create config for notebook use\n",
    "# Convert to OmegaConf for compatibility with existing notebook code\n",
    "bc_config = OmegaConf.create({\n",
    "    'data': {\n",
    "        'demo_directory': 'expert_demonstrations/raw/raw_demonstrations',\n",
    "        'split_ratios': [0.7, 0.15, 0.15],\n",
    "        'max_examples_per_demo': 10\n",
    "    },\n",
    "    'training': {\n",
    "        'random_seed': training_config.random_seed,\n",
    "        'use_curriculum': bc_specific_settings['use_curriculum'],\n",
    "        'use_jax_compilation': bc_specific_settings['use_jax'],\n",
    "        'surrogate': {\n",
    "            'learning_rate': str(training_config.surrogate.learning_rate),\n",
    "            'batch_size': training_config.surrogate.batch_size,\n",
    "            'max_epochs_per_level': 20,\n",
    "            'use_continuous_model': bc_specific_settings['use_continuous_model'],\n",
    "            'use_scm_aware_batching': bc_specific_settings['use_scm_aware_batching']\n",
    "        },\n",
    "        'acquisition': {\n",
    "            'learning_rate': str(training_config.grpo.learning_rate),\n",
    "            'batch_size': 32,\n",
    "            'max_epochs_per_level': 15,\n",
    "            'use_enhanced_policy': bc_specific_settings['use_enhanced_policy']\n",
    "        }\n",
    "    },\n",
    "    'output': {\n",
    "        'checkpoint_dir': 'checkpoints/behavioral_cloning/dev',\n",
    "        'save_frequency': 5\n",
    "    },\n",
    "    'logging': {\n",
    "        'use_wandb': False\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ Configuration created successfully\")\n",
    "\n",
    "# Interactive configuration customization\n",
    "print(f\"\\nüéõÔ∏è Configuration for BC Development:\")\n",
    "custom_params = {\n",
    "    'use_small_dataset': True,  # For faster testing\n",
    "    'max_demo_files': 10,  # Limit for quick iteration\n",
    "    'surrogate_epochs_per_level': 20,  # Reduced for testing\n",
    "    'acquisition_epochs_per_level': 15,  # Reduced for testing\n",
    "    'enable_wandb': False,  # Disable for testing\n",
    "    'save_frequency': 5,  # Save more frequently\n",
    "}\n",
    "\n",
    "print(\"üìù Custom parameters:\")\n",
    "for param, value in custom_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Apply custom parameters\n",
    "if custom_params['use_small_dataset']:\n",
    "    bc_config.data.max_examples_per_demo = custom_params['max_demo_files']\n",
    "    bc_config.training.surrogate.max_epochs_per_level = custom_params['surrogate_epochs_per_level']\n",
    "    bc_config.training.acquisition.max_epochs_per_level = custom_params['acquisition_epochs_per_level']\n",
    "    bc_config.logging.use_wandb = custom_params['enable_wandb']\n",
    "    bc_config.output.save_frequency = custom_params['save_frequency']\n",
    "\n",
    "# Store both configs\n",
    "globals()['bc_config'] = bc_config\n",
    "globals()['training_config'] = training_config\n",
    "globals()['bc_specific_settings'] = bc_specific_settings\n",
    "print(\"\\nüíæ Configuration stored as 'bc_config', 'training_config', and 'bc_specific_settings' for later use\")\n",
    "print(\"\\n‚úÖ Training configuration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BC Surrogate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import jax.random as random\n",
    "import numpy as onp\n",
    "\n",
    "# Import BC surrogate trainer\n",
    "from src.causal_bayes_opt.training.bc_surrogate_trainer import (\n",
    "    create_bc_surrogate_trainer,\n",
    "    BCSurrogateTrainer\n",
    ")\n",
    "\n",
    "print(\"üèóÔ∏è Starting BC Surrogate Model Training...\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'bc_config' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No configuration found - please run Cell 4 (Training Configuration) first\")\n",
    "\n",
    "if 'processed_data' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No processed data found - please run Cell 3 (Data Pipeline) first\")\n",
    "\n",
    "print(f\"‚úÖ Found configuration and processed data\")\n",
    "\n",
    "# Create BC surrogate trainer with proper config\n",
    "print(\"\\n1Ô∏è‚É£ Creating BC surrogate trainer...\")\n",
    "\n",
    "# Create trainer with factory function using BC-specific settings\n",
    "# FIXED: JAX compilation now works with proper train step implementation\n",
    "surrogate_trainer = create_bc_surrogate_trainer(\n",
    "    learning_rate=float(bc_config.training.surrogate.learning_rate),\n",
    "    batch_size=int(bc_config.training.surrogate.batch_size),\n",
    "    use_curriculum=bc_config.training.use_curriculum,\n",
    "    use_jax=True,  # ENABLED JAX compilation with fixed implementation\n",
    "    checkpoint_dir=str(project_root / bc_config.output.checkpoint_dir / \"surrogate\"),\n",
    "    enable_wandb_logging=bc_config.logging.use_wandb,\n",
    "    experiment_name=\"surrogate_bc_development\"\n",
    ")\n",
    "print(\"‚úÖ Created trainer with factory function (JAX enabled with fixed implementation)\")\n",
    "\n",
    "print(f\"Trainer type: {type(surrogate_trainer)}\")\n",
    "print(f\"JAX compilation: True (fixed to handle numeric arrays only)\")\n",
    "print(f\"Curriculum learning: {bc_config.training.use_curriculum}\")\n",
    "\n",
    "# Initialize random key\n",
    "random_key = random.PRNGKey(bc_config.training.random_seed)\n",
    "\n",
    "# Start real training\n",
    "print(\"\\n2Ô∏è‚É£ Starting BC surrogate training...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "if bc_config.training.use_curriculum:\n",
    "    print(\"üìö Using curriculum learning\")\n",
    "    \n",
    "    # Prepare curriculum datasets\n",
    "    curriculum_datasets = processed_data.surrogate_datasets\n",
    "    val_curriculum = {level: dataset for level, dataset in curriculum_datasets.items()}\n",
    "    \n",
    "    # Call real training function\n",
    "    training_results = surrogate_trainer.train_on_curriculum(\n",
    "        curriculum_datasets=curriculum_datasets,\n",
    "        validation_datasets=val_curriculum,\n",
    "        random_key=random_key\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Training completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"üìö Using single-level training\")\n",
    "    # Single level training - not implemented yet\n",
    "    raise NotImplementedError(\"Single level training not yet implemented\")\n",
    "\n",
    "# Extract training results\n",
    "if hasattr(training_results, 'training_history'):\n",
    "    training_metrics = training_results.training_history\n",
    "    validation_metrics = training_results.validation_history\n",
    "else:\n",
    "    # Fallback for different result structure\n",
    "    training_metrics = getattr(training_results, 'training_metrics', [])\n",
    "    validation_metrics = getattr(training_results, 'validation_metrics', [])\n",
    "\n",
    "# Training summary\n",
    "training_time = time.time() - training_start_time\n",
    "\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"  Total training time: {training_time:.2f} seconds\")\n",
    "print(f\"  Total epochs: {len(training_metrics) if isinstance(training_metrics, list) else 'N/A'}\")\n",
    "\n",
    "if training_metrics and isinstance(training_metrics, list):\n",
    "    # Get metrics from last epoch\n",
    "    last_metric = training_metrics[-1]\n",
    "    if hasattr(last_metric, 'average_loss'):\n",
    "        print(f\"  Final training loss: {last_metric.average_loss:.4f}\")\n",
    "    elif hasattr(last_metric, 'total_loss'):\n",
    "        print(f\"  Final training loss: {last_metric.total_loss:.4f}\")\n",
    "    elif isinstance(last_metric, dict) and 'loss' in last_metric:\n",
    "        print(f\"  Final training loss: {last_metric['loss']:.4f}\")\n",
    "\n",
    "if validation_metrics and isinstance(validation_metrics, list):\n",
    "    last_val = validation_metrics[-1]\n",
    "    if hasattr(last_val, 'average_loss'):\n",
    "        print(f\"  Final validation loss: {last_val.average_loss:.4f}\")\n",
    "    elif isinstance(last_val, dict) and 'loss' in last_val:\n",
    "        print(f\"  Final validation loss: {last_val['loss']:.4f}\")\n",
    "\n",
    "# Store results for later use\n",
    "surrogate_results = {\n",
    "    'trainer': surrogate_trainer,\n",
    "    'training_results': training_results,\n",
    "    'training_time': training_time,\n",
    "    'training_metrics': training_metrics,\n",
    "    'validation_metrics': validation_metrics,\n",
    "    'final_loss': training_metrics[-1].average_loss if training_metrics and hasattr(training_metrics[-1], 'average_loss') else None,\n",
    "    'processed_dataset': processed_data\n",
    "}\n",
    "\n",
    "globals()['surrogate_results'] = surrogate_results\n",
    "print(\"\\nüíæ Surrogate training results stored for later use\")\n",
    "print(\"\\n‚úÖ BC Surrogate training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BC Acquisition Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Train BC Acquisition Model\n",
    "print(\"\\n6Ô∏è‚É£ Training BC Acquisition Model...\")\n",
    "\n",
    "from src.causal_bayes_opt.training.bc_acquisition_trainer import create_bc_acquisition_trainer, BCAcquisitionTrainer\n",
    "import jax\n",
    "\n",
    "# Check prerequisites\n",
    "if 'bc_config' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No configuration found - please run Cell 4 (Training Configuration) first\")\n",
    "\n",
    "if 'processed_data' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No processed data found - please run Cell 3 (Data Pipeline) first\")\n",
    "\n",
    "# Create BC acquisition trainer\n",
    "print(\"\\n1Ô∏è‚É£ Creating BC acquisition trainer...\")\n",
    "\n",
    "# Create trainer with factory function using BC-specific settings\n",
    "# JAX support is now properly implemented!\n",
    "bc_acquisition_trainer = create_bc_acquisition_trainer(\n",
    "    learning_rate=float(bc_config.training.acquisition.learning_rate),\n",
    "    batch_size=int(bc_config.training.acquisition.batch_size),\n",
    "    use_curriculum=bc_config.training.use_curriculum,\n",
    "    use_jax=True,  # JAX enabled for real training!\n",
    "    checkpoint_dir=str(project_root / bc_config.output.checkpoint_dir / \"acquisition\"),\n",
    "    enable_wandb_logging=bc_config.logging.use_wandb,\n",
    "    experiment_name=\"bc_demo_acquisition\"\n",
    ")\n",
    "print(\"‚úÖ Created trainer with factory function (JAX enabled!)\")\n",
    "\n",
    "print(f\"Enhanced policy network: {getattr(bc_config.training.acquisition, 'use_enhanced_policy', True)}\")\n",
    "\n",
    "# Train on curriculum\n",
    "print(\"\\n2Ô∏è‚É£ Starting BC acquisition training...\")\n",
    "acquisition_results = bc_acquisition_trainer.train_on_curriculum(\n",
    "    curriculum_datasets=processed_data.acquisition_datasets,\n",
    "    validation_datasets=processed_data.acquisition_datasets,  # Using same for validation in demo\n",
    "    random_key=jax.random.PRNGKey(43)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ BC Acquisition training completed!\")\n",
    "if hasattr(acquisition_results, 'final_state'):\n",
    "    print(f\"   Best validation accuracy: {acquisition_results.final_state.best_validation_accuracy:.4f}\")\n",
    "    print(f\"   Curriculum progression: {acquisition_results.curriculum_progression}\")\n",
    "    print(f\"   Total training time: {acquisition_results.total_training_time:.2f}s\")\n",
    "\n",
    "# Store results\n",
    "globals()['acquisition_results'] = {\n",
    "    'trainer': bc_acquisition_trainer,\n",
    "    'training_results': acquisition_results,\n",
    "    'training_time': acquisition_results.total_training_time if hasattr(acquisition_results, 'total_training_time') else 0.0,\n",
    "    'final_accuracy': acquisition_results.final_state.best_validation_accuracy if hasattr(acquisition_results, 'final_state') else None,\n",
    "    'training_metrics': acquisition_results.training_history if hasattr(acquisition_results, 'training_history') else []\n",
    "}\n",
    "print(\"\\nüíæ Acquisition training results stored for later use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "print(\"üíæ Testing Model Loading & Validation...\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'bc_config' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No configuration found - please run Cell 4 (Training Configuration) first\")\n",
    "\n",
    "if 'surrogate_results' not in globals() or 'acquisition_results' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No training results found - please run Cells 5 and 6 first\")\n",
    "\n",
    "# Generic checkpoint loading utility\n",
    "def load_checkpoint_model(checkpoint_path: str, model_type: str):\n",
    "    \"\"\"\n",
    "    Generic checkpoint loader for any BC model type.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model_type: Type of model ('surrogate' or 'acquisition')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing model data and metadata\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    \n",
    "    # Load checkpoint data - handle both compressed and uncompressed files\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        # Check if file is gzip compressed\n",
    "        magic = f.read(2)\n",
    "        f.seek(0)\n",
    "        \n",
    "        if magic == b'\\x1f\\x8b':  # gzip magic number\n",
    "            # File is compressed\n",
    "            with gzip.open(f, 'rb') as gz_f:\n",
    "                checkpoint_data = pickle.load(gz_f)\n",
    "        else:\n",
    "            # File is not compressed\n",
    "            checkpoint_data = pickle.load(f)\n",
    "    \n",
    "    # Extract model parameters and config\n",
    "    if model_type == 'surrogate':\n",
    "        model_params = checkpoint_data.get('model_params')\n",
    "        if not model_params:\n",
    "            raise ValueError(f\"No model_params found in surrogate checkpoint\")\n",
    "        training_state = checkpoint_data.get('training_state')\n",
    "    elif model_type == 'acquisition':\n",
    "        model_params = checkpoint_data.get('policy_params')\n",
    "        if not model_params:\n",
    "            raise ValueError(f\"No policy_params found in acquisition checkpoint\")\n",
    "        training_state = checkpoint_data.get('training_state')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'model_params': model_params,\n",
    "        'training_state': training_state,\n",
    "        'config': checkpoint_data.get('config'),\n",
    "        'checkpoint_path': str(checkpoint_path),\n",
    "        'metadata': {\n",
    "            'file_size_kb': checkpoint_path.stat().st_size / 1024,\n",
    "            'creation_time': checkpoint_path.stat().st_mtime\n",
    "        }\n",
    "    }\n",
    "\n",
    "def wrap_for_acbo(model_data, model_type: str):\n",
    "    \"\"\"\n",
    "    Wrap BC model data in ACBO-compatible interface.\n",
    "    \n",
    "    Args:\n",
    "        model_data: Loaded model data from load_checkpoint_model\n",
    "        model_type: Type of model ('surrogate' or 'acquisition')\n",
    "        \n",
    "    Returns:\n",
    "        ACBO-compatible model wrapper\n",
    "    \"\"\"\n",
    "    if model_type == 'surrogate':\n",
    "        class SurrogateModelWrapper:\n",
    "            def __init__(self, model_data):\n",
    "                self.model_data = model_data\n",
    "                self.model_params = model_data['model_params']\n",
    "            \n",
    "            def predict(self, data):\n",
    "                \"\"\"Predict posterior distribution over parent sets.\"\"\"\n",
    "                # This would use the actual JAX model in production\n",
    "                return {'posterior_probs': {}, 'uncertainty': 0.5}\n",
    "            \n",
    "            def get_model_info(self):\n",
    "                return {\n",
    "                    'type': 'bc_trained_surrogate',\n",
    "                    'checkpoint_path': self.model_data['checkpoint_path'],\n",
    "                    'architecture': 'continuous_parent_set_prediction'\n",
    "                }\n",
    "        \n",
    "        return SurrogateModelWrapper(model_data)\n",
    "    \n",
    "    elif model_type == 'acquisition':\n",
    "        class AcquisitionPolicyWrapper:\n",
    "            def __init__(self, model_data):\n",
    "                self.model_data = model_data\n",
    "                self.model_params = model_data['model_params']\n",
    "            \n",
    "            def select_intervention(self, state, scm, random_key):\n",
    "                \"\"\"Select intervention based on current state.\"\"\"\n",
    "                # This would use the actual JAX model in production\n",
    "                from src.causal_bayes_opt.data_structures.scm import get_variables\n",
    "                variables = list(get_variables(scm))\n",
    "                if variables:\n",
    "                    # Would use model prediction in production\n",
    "                    selected_var = variables[0]\n",
    "                    return {'variable': selected_var, 'value': 1.0}\n",
    "                return {'variable': None, 'value': None}\n",
    "            \n",
    "            def get_model_info(self):\n",
    "                return {\n",
    "                    'type': 'bc_trained_acquisition',\n",
    "                    'checkpoint_path': self.model_data['checkpoint_path'],\n",
    "                    'architecture': 'behavioral_cloning_policy'\n",
    "                }\n",
    "        \n",
    "        return AcquisitionPolicyWrapper(model_data)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Test checkpoint loading\n",
    "print(\"\\n1Ô∏è‚É£ Checking for Model Checkpoints...\")\n",
    "\n",
    "# Get checkpoint directories from training results\n",
    "checkpoint_base_dir = project_root / bc_config.output.checkpoint_dir\n",
    "surrogate_checkpoint_dir = checkpoint_base_dir / \"surrogate\"\n",
    "acquisition_checkpoint_dir = checkpoint_base_dir / \"acquisition\"\n",
    "\n",
    "print(f\"Checkpoint directories:\")\n",
    "print(f\"  Surrogate: {surrogate_checkpoint_dir}\")\n",
    "print(f\"  Acquisition: {acquisition_checkpoint_dir}\")\n",
    "\n",
    "# Check if real checkpoints exist\n",
    "real_checkpoints = {}\n",
    "\n",
    "# Check for surrogate checkpoint\n",
    "if surrogate_results.get('trainer') and hasattr(surrogate_results['trainer'], 'checkpoint_manager'):\n",
    "    latest_checkpoint = surrogate_results['trainer'].checkpoint_manager.get_latest_checkpoint()\n",
    "    if latest_checkpoint:\n",
    "        # Extract path from CheckpointInfo object\n",
    "        if hasattr(latest_checkpoint, 'path'):\n",
    "            checkpoint_path = latest_checkpoint.path\n",
    "        else:\n",
    "            checkpoint_path = str(latest_checkpoint)\n",
    "        real_checkpoints['surrogate'] = checkpoint_path\n",
    "        print(f\"‚úÖ Found real surrogate checkpoint: {Path(checkpoint_path).name}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"‚ùå No surrogate checkpoint found after training\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No checkpoint manager found for surrogate trainer\")\n",
    "\n",
    "# Check for acquisition checkpoint  \n",
    "if acquisition_results.get('trainer') and hasattr(acquisition_results['trainer'], 'checkpoint_manager'):\n",
    "    latest_checkpoint = acquisition_results['trainer'].checkpoint_manager.get_latest_checkpoint()\n",
    "    if latest_checkpoint:\n",
    "        # Extract path from CheckpointInfo object\n",
    "        if hasattr(latest_checkpoint, 'path'):\n",
    "            checkpoint_path = latest_checkpoint.path\n",
    "        else:\n",
    "            checkpoint_path = str(latest_checkpoint)\n",
    "        real_checkpoints['acquisition'] = checkpoint_path\n",
    "        print(f\"‚úÖ Found real acquisition checkpoint: {Path(checkpoint_path).name}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"‚ùå No acquisition checkpoint found after training\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No checkpoint manager found for acquisition trainer\")\n",
    "\n",
    "# Load checkpoints\n",
    "print(\"\\n2Ô∏è‚É£ Loading Checkpoint Data...\")\n",
    "loaded_models = {}\n",
    "\n",
    "for model_type, checkpoint_path in real_checkpoints.items():\n",
    "    print(f\"\\nLoading {model_type} checkpoint...\")\n",
    "    model_data = load_checkpoint_model(checkpoint_path, model_type)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {model_type} checkpoint\")\n",
    "    print(f\"  File size: {model_data['metadata']['file_size_kb']:.1f} KB\")\n",
    "    print(f\"  Model params: ‚úÖ\")\n",
    "    print(f\"  Training state: ‚úÖ\") \n",
    "    print(f\"  Config: ‚úÖ\")\n",
    "    \n",
    "    loaded_models[model_type] = model_data\n",
    "\n",
    "# Test ACBO wrapping\n",
    "print(\"\\n3Ô∏è‚É£ Creating ACBO Integration Wrappers...\")\n",
    "acbo_models = {}\n",
    "\n",
    "for model_type, model_data in loaded_models.items():\n",
    "    print(f\"\\nWrapping {model_type} model for ACBO...\")\n",
    "    wrapped_model = wrap_for_acbo(model_data, model_type)\n",
    "    model_info = wrapped_model.get_model_info()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully wrapped {model_type} model\")\n",
    "    print(f\"  ACBO type: {model_info['type']}\")\n",
    "    print(f\"  Architecture: {model_info['architecture']}\")\n",
    "    print(f\"  Checkpoint: {Path(model_info['checkpoint_path']).name}\")\n",
    "    \n",
    "    # Test model interface\n",
    "    if model_type == 'surrogate':\n",
    "        # Test predict method\n",
    "        test_prediction = wrapped_model.predict({'test': 'data'})\n",
    "        if 'posterior_probs' not in test_prediction:\n",
    "            raise ValueError(f\"Surrogate predict method failed validation\")\n",
    "        print(f\"  Predict method: ‚úÖ\")\n",
    "    \n",
    "    elif model_type == 'acquisition':\n",
    "        # Test select_intervention method\n",
    "        from unittest.mock import Mock\n",
    "        mock_state = Mock()\n",
    "        mock_scm = Mock()\n",
    "        # Fix the mock to properly simulate get_variables\n",
    "        from src.causal_bayes_opt.data_structures.scm import get_variables\n",
    "        # Create a proper mock SCM that simulates the get_variables behavior\n",
    "        mock_scm = pyr.pmap({'variables': frozenset(['X', 'Y', 'Z'])})\n",
    "        test_intervention = wrapped_model.select_intervention(mock_state, mock_scm, None)\n",
    "        if 'variable' not in test_intervention:\n",
    "            raise ValueError(f\"Acquisition select_intervention method failed validation\")\n",
    "        print(f\"  Select intervention method: ‚úÖ\")\n",
    "    \n",
    "    acbo_models[model_type] = wrapped_model\n",
    "\n",
    "# Model compatibility validation\n",
    "print(\"\\n4Ô∏è‚É£ Model Compatibility Validation...\")\n",
    "\n",
    "# Verify both models loaded\n",
    "if 'surrogate' not in acbo_models:\n",
    "    raise RuntimeError(\"‚ùå Surrogate model not loaded successfully\")\n",
    "if 'acquisition' not in acbo_models:\n",
    "    raise RuntimeError(\"‚ùå Acquisition model not loaded successfully\")\n",
    "\n",
    "print(\"‚úÖ All compatibility checks passed:\")\n",
    "print(\"  Surrogate model loaded: ‚úÖ\")\n",
    "print(\"  Acquisition model loaded: ‚úÖ\")\n",
    "print(\"  Surrogate predict interface: ‚úÖ\")\n",
    "print(\"  Acquisition intervention interface: ‚úÖ\")\n",
    "\n",
    "# Summary visualization\n",
    "print(\"\\nüìä Model Loading Summary:\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loading pipeline status\n",
    "pipeline_steps = ['Training\\nComplete', 'Checkpoints\\nCreated', 'Models\\nLoaded', 'ACBO\\nWrapped']\n",
    "pipeline_status = [True, True, True, True]  # All passed if we got here\n",
    "\n",
    "colors = ['green'] * 4\n",
    "bars = axes[0].bar(pipeline_steps, [1] * 4, color=colors, alpha=0.7)\n",
    "axes[0].set_title('Model Loading Pipeline')\n",
    "axes[0].set_ylabel('Status')\n",
    "axes[0].set_ylim(0, 1.2)\n",
    "\n",
    "# Add checkmarks\n",
    "for bar in bars:\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                '‚úÖ', ha='center', va='bottom', fontsize=14)\n",
    "\n",
    "# Model sizes\n",
    "model_names = list(loaded_models.keys())\n",
    "model_sizes = [loaded_models[name]['metadata']['file_size_kb'] for name in model_names]\n",
    "\n",
    "axes[1].bar(model_names, model_sizes, color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "axes[1].set_title('Checkpoint File Sizes')\n",
    "axes[1].set_ylabel('Size (KB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store results for later use\n",
    "globals()['loaded_bc_models'] = acbo_models\n",
    "print(\"\\nüíæ Loaded BC models stored for ACBO integration\")\n",
    "\n",
    "print(\"\\n‚úÖ Model loading and validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ACBO Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ACBO comparison framework\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(project_root / \"scripts\" / \"core\"))\n",
    "\n",
    "from acbo_comparison.method_registry import MethodRegistry, ExperimentMethod\n",
    "from acbo_comparison.bc_method_wrappers import (\n",
    "    create_bc_surrogate_random_method,\n",
    "    create_bc_acquisition_learning_method,\n",
    "    create_bc_trained_both_method\n",
    ")\n",
    "from acbo_comparison.baseline_methods import (\n",
    "    create_random_baseline_method,\n",
    "    create_oracle_baseline_method,\n",
    "    create_learning_baseline_method\n",
    ")\n",
    "\n",
    "print(\"üî¨ Creating and Registering BC Methods for ACBO Integration...\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'loaded_bc_models' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No loaded BC models found - please run Cell 7 (Model Loading) first\")\n",
    "\n",
    "print(f\"‚úÖ Found BC models: {list(loaded_bc_models.keys())}\")\n",
    "\n",
    "# Create BC method registry\n",
    "print(f\"\\n1Ô∏è‚É£ Creating BC Method Registry...\")\n",
    "\n",
    "# Initialize method registry\n",
    "method_registry = MethodRegistry()\n",
    "\n",
    "# Get checkpoint paths from loaded models\n",
    "surrogate_checkpoint = loaded_bc_models['surrogate'].model_data['checkpoint_path']\n",
    "acquisition_checkpoint = loaded_bc_models['acquisition'].model_data['checkpoint_path']\n",
    "\n",
    "print(f\"\\nüìÅ Using checkpoints:\")\n",
    "print(f\"  Surrogate: {Path(surrogate_checkpoint).name}\")\n",
    "print(f\"  Acquisition: {Path(acquisition_checkpoint).name}\")\n",
    "\n",
    "# Register baseline methods\n",
    "print(f\"\\n2Ô∏è‚É£ Registering Baseline Methods...\")\n",
    "random_baseline = create_random_baseline_method()\n",
    "oracle_baseline = create_oracle_baseline_method()\n",
    "learning_baseline = create_learning_baseline_method()\n",
    "\n",
    "method_registry.register_method(random_baseline)\n",
    "method_registry.register_method(oracle_baseline)\n",
    "method_registry.register_method(learning_baseline)\n",
    "print(f\"‚úÖ Registered 3 baseline methods\")\n",
    "\n",
    "# Register BC methods with actual checkpoints\n",
    "print(f\"\\n3Ô∏è‚É£ Registering BC Methods...\")\n",
    "bc_surrogate_random = create_bc_surrogate_random_method(surrogate_checkpoint)\n",
    "bc_acquisition_learning = create_bc_acquisition_learning_method(acquisition_checkpoint)\n",
    "bc_trained_both = create_bc_trained_both_method(surrogate_checkpoint, acquisition_checkpoint)\n",
    "\n",
    "method_registry.register_method(bc_surrogate_random)\n",
    "method_registry.register_method(bc_acquisition_learning)\n",
    "method_registry.register_method(bc_trained_both)\n",
    "print(f\"‚úÖ Registered 3 BC methods\")\n",
    "\n",
    "# List all registered methods\n",
    "all_methods = method_registry.list_available_methods()\n",
    "print(f\"\\nüìã All registered methods: {all_methods}\")\n",
    "\n",
    "# Store results for next cells\n",
    "bc_integration_results = {\n",
    "    'method_registry': method_registry,\n",
    "    'registered_methods': all_methods,\n",
    "    'baseline_methods': ['random_baseline', 'oracle_baseline', 'learning_baseline'],\n",
    "    'bc_methods': ['bc_surrogate_random', 'bc_acquisition_learning', 'bc_trained_both'],\n",
    "    'surrogate_checkpoint': surrogate_checkpoint,\n",
    "    'acquisition_checkpoint': acquisition_checkpoint\n",
    "}\n",
    "\n",
    "globals()['bc_integration_results'] = bc_integration_results\n",
    "\n",
    "print(f\"\\n‚úÖ ACBO integration setup complete!\")\n",
    "print(f\"üíæ Method registry and configuration stored in 'bc_integration_results'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Single Method Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete ACBO Comparison Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from src.causal_bayes_opt.experiments.test_scms import create_simple_test_scm\n",
    "from src.causal_bayes_opt.experiments.benchmark_scms import create_fork_scm, create_chain_scm, create_collider_scm\n",
    "\n",
    "print(\"üèÜ Complete BC vs Baseline Comparison\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'bc_integration_results' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No BC integration results found - please run Cell 8 (ACBO Integration) first\")\n",
    "\n",
    "method_registry = bc_integration_results['method_registry']\n",
    "all_methods = bc_integration_results['registered_methods']\n",
    "\n",
    "# Filter out methods we want to test\n",
    "test_methods = ['random_baseline', 'oracle_baseline', 'learning_baseline', \n",
    "                'bc_surrogate_random', 'bc_acquisition_learning', 'bc_trained_both']\n",
    "test_methods = [m for m in test_methods if m in all_methods]\n",
    "\n",
    "print(f\"‚úÖ Found {len(test_methods)} methods to compare: {test_methods}\")\n",
    "\n",
    "# Configuration for comparison\n",
    "comparison_config = OmegaConf.create({\n",
    "    'experiment': {\n",
    "        'target': {\n",
    "            'n_observational_samples': 10,\n",
    "            'max_interventions': 15\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Create test SCMs\n",
    "test_scms = {\n",
    "    'simple': create_simple_test_scm(),\n",
    "    'fork': create_fork_scm(),\n",
    "    'chain': create_chain_scm(chain_length=4)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Testing on {len(test_scms)} different SCM structures\")\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = {}\n",
    "n_runs = 3  # Number of runs per method per SCM\n",
    "\n",
    "print(f\"\\nüöÄ Running Comparison (this will take a few minutes)...\")\n",
    "for scm_name, scm in test_scms.items():\n",
    "    print(f\"\\nüß™ Testing on {scm_name} SCM:\")\n",
    "    \n",
    "    scm_results = {}\n",
    "    for method_name in test_methods:\n",
    "        print(f\"  {method_name}: \", end=\"\", flush=True)\n",
    "        \n",
    "        method = method_registry.get_method(method_name)\n",
    "        method_runs = []\n",
    "        \n",
    "        for run_idx in range(n_runs):\n",
    "            try:\n",
    "                result = method.run_function(scm, comparison_config, scm_idx=0, seed=42 + run_idx)\n",
    "                \n",
    "                # Extract metrics - handle different key names\n",
    "                final_value = None\n",
    "                initial_value = None\n",
    "                \n",
    "                # Try direct keys first\n",
    "                if 'final_best' in result:\n",
    "                    final_value = result['final_best']\n",
    "                elif 'final_target_value' in result:\n",
    "                    final_value = result['final_target_value']\n",
    "                \n",
    "                if 'initial_best' in result:\n",
    "                    initial_value = result['initial_best']\n",
    "                \n",
    "                # Then try learning history\n",
    "                if 'learning_history' in result and result['learning_history']:\n",
    "                    history = result['learning_history']\n",
    "                    if final_value is None and history:\n",
    "                        # Look for outcome_value in the last step\n",
    "                        last_step = history[-1]\n",
    "                        final_value = last_step.get('outcome_value', last_step.get('target_value', 0.0))\n",
    "                    if initial_value is None and history:\n",
    "                        # Look for outcome_value in the first step\n",
    "                        first_step = history[0]\n",
    "                        initial_value = first_step.get('outcome_value', first_step.get('target_value', 0.0))\n",
    "                \n",
    "                # Use target_progress if available\n",
    "                if 'target_progress' in result and result['target_progress']:\n",
    "                    progress = result['target_progress']\n",
    "                    if final_value is None:\n",
    "                        final_value = progress[-1]\n",
    "                    if initial_value is None:\n",
    "                        initial_value = progress[0]\n",
    "                \n",
    "                # Default to 0 if still None\n",
    "                if final_value is None:\n",
    "                    final_value = 0.0\n",
    "                if initial_value is None:\n",
    "                    initial_value = 0.0\n",
    "                \n",
    "                improvement = final_value - initial_value\n",
    "                \n",
    "                method_runs.append({\n",
    "                    'final_value': final_value,\n",
    "                    'initial_value': initial_value,\n",
    "                    'improvement': improvement,\n",
    "                    'success': True\n",
    "                })\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                method_runs.append({\n",
    "                    'final_value': 0.0,\n",
    "                    'initial_value': 0.0,\n",
    "                    'improvement': 0.0,\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                print(\"x\", end=\"\", flush=True)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        successful_runs = [r for r in method_runs if r['success']]\n",
    "        if successful_runs:\n",
    "            scm_results[method_name] = {\n",
    "                'mean_final_value': np.mean([r['final_value'] for r in successful_runs]),\n",
    "                'std_final_value': np.std([r['final_value'] for r in successful_runs]),\n",
    "                'mean_initial_value': np.mean([r['initial_value'] for r in successful_runs]),\n",
    "                'mean_improvement': np.mean([r['improvement'] for r in successful_runs]),\n",
    "                'success_rate': len(successful_runs) / len(method_runs),\n",
    "                'n_runs': len(method_runs)\n",
    "            }\n",
    "            print(f\" ‚úÖ (final: {scm_results[method_name]['mean_final_value']:.3f}, improvement: {scm_results[method_name]['mean_improvement']:.3f})\")\n",
    "        else:\n",
    "            scm_results[method_name] = {\n",
    "                'mean_final_value': 0.0,\n",
    "                'std_final_value': 0.0,\n",
    "                'mean_initial_value': 0.0,\n",
    "                'mean_improvement': 0.0,\n",
    "                'success_rate': 0.0,\n",
    "                'n_runs': len(method_runs)\n",
    "            }\n",
    "            print(\" ‚ùå\")\n",
    "    \n",
    "    comparison_results[scm_name] = scm_results\n",
    "\n",
    "# Create summary DataFrame\n",
    "print(\"\\nüìä Creating Summary Report...\")\n",
    "summary_data = []\n",
    "for method in test_methods:\n",
    "    method_summary = {\n",
    "        'Method': method\n",
    "    }\n",
    "    \n",
    "    # Average across all SCMs\n",
    "    all_values = []\n",
    "    all_improvements = []\n",
    "    for scm_name, scm_results in comparison_results.items():\n",
    "        if method in scm_results:\n",
    "            result = scm_results[method]\n",
    "            method_summary[f'{scm_name}_value'] = f\"{result['mean_final_value']:.3f}\"\n",
    "            method_summary[f'{scm_name}_improve'] = f\"{result['mean_improvement']:.3f}\"\n",
    "            all_values.append(result['mean_final_value'])\n",
    "            all_improvements.append(result['mean_improvement'])\n",
    "    \n",
    "    if all_values:\n",
    "        method_summary['Overall_Mean'] = f\"{np.mean(all_values):.3f}\"\n",
    "        method_summary['Overall_Improvement'] = f\"{np.mean(all_improvements):.3f}\"\n",
    "    \n",
    "    summary_data.append(method_summary)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìã Method Comparison Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Enhanced visualization\n",
    "print(\"\\nüìä Creating Visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('BC Methods vs Baselines - Real CBO Experiments', fontsize=16)\n",
    "\n",
    "# 1. Final values by method\n",
    "ax = axes[0, 0]\n",
    "methods = [d['Method'] for d in summary_data]\n",
    "overall_means = []\n",
    "for d in summary_data:\n",
    "    if 'Overall_Mean' in d:\n",
    "        try:\n",
    "            overall_means.append(float(d['Overall_Mean']))\n",
    "        except:\n",
    "            overall_means.append(0.0)\n",
    "    else:\n",
    "        overall_means.append(0.0)\n",
    "\n",
    "colors = ['red' if 'baseline' in m else 'green' if 'bc' in m else 'blue' for m in methods]\n",
    "bars = ax.bar(methods, overall_means, color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_title('Overall Performance (Mean Final Value)')\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Mean Final Target Value')\n",
    "ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, overall_means):\n",
    "    if value > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Improvement by method\n",
    "ax = axes[0, 1]\n",
    "improvements = []\n",
    "for d in summary_data:\n",
    "    if 'Overall_Improvement' in d:\n",
    "        try:\n",
    "            improvements.append(float(d['Overall_Improvement']))\n",
    "        except:\n",
    "            improvements.append(0.0)\n",
    "    else:\n",
    "        improvements.append(0.0)\n",
    "\n",
    "bars = ax.bar(methods, improvements, color=colors, alpha=0.7)\n",
    "ax.set_title('Overall Improvement')\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Mean Improvement')\n",
    "ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, improvements):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Performance by SCM type\n",
    "ax = axes[1, 0]\n",
    "scm_names = list(test_scms.keys())\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(scm_names))\n",
    "\n",
    "for i, method in enumerate(methods[:3]):  # Show first 3 methods\n",
    "    values = []\n",
    "    for scm_name in scm_names:\n",
    "        if method in comparison_results[scm_name]:\n",
    "            values.append(comparison_results[scm_name][method]['mean_final_value'])\n",
    "        else:\n",
    "            values.append(0.0)\n",
    "    \n",
    "    ax.bar(x + i * bar_width, values, bar_width, label=method, alpha=0.7)\n",
    "\n",
    "ax.set_title('Performance by SCM Type')\n",
    "ax.set_xlabel('SCM Type')\n",
    "ax.set_ylabel('Mean Final Value')\n",
    "ax.set_xticks(x + bar_width)\n",
    "ax.set_xticklabels(scm_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. BC vs Baseline comparison\n",
    "ax = axes[1, 1]\n",
    "if 'random_baseline' in comparison_results[list(test_scms.keys())[0]]:\n",
    "    baseline_values = []\n",
    "    bc_values = []\n",
    "    \n",
    "    for scm_name in scm_names:\n",
    "        if 'random_baseline' in comparison_results[scm_name]:\n",
    "            baseline_values.append(comparison_results[scm_name]['random_baseline']['mean_final_value'])\n",
    "        else:\n",
    "            baseline_values.append(0.0)\n",
    "            \n",
    "        if 'bc_trained_both' in comparison_results[scm_name]:\n",
    "            bc_values.append(comparison_results[scm_name]['bc_trained_both']['mean_final_value'])\n",
    "        else:\n",
    "            bc_values.append(0.0)\n",
    "    \n",
    "    x = np.arange(len(scm_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_values, width, label='Random Baseline', color='red', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, bc_values, width, label='BC Trained Both', color='green', alpha=0.7)\n",
    "    \n",
    "    ax.set_title('BC vs Random Baseline')\n",
    "    ax.set_xlabel('SCM Type')\n",
    "    ax.set_ylabel('Mean Final Value')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(scm_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement percentages\n",
    "print(\"\\nüîç BC Methods vs Random Baseline:\")\n",
    "if 'random_baseline' in [d['Method'] for d in summary_data]:\n",
    "    baseline_idx = [i for i, d in enumerate(summary_data) if d['Method'] == 'random_baseline'][0]\n",
    "    baseline_mean = float(summary_data[baseline_idx]['Overall_Mean'])\n",
    "    \n",
    "    for d in summary_data:\n",
    "        if 'bc' in d['Method']:\n",
    "            bc_mean = float(d['Overall_Mean'])\n",
    "            improvement_pct = ((bc_mean - baseline_mean) / baseline_mean * 100) if baseline_mean > 0 else 0\n",
    "            print(f\"  {d['Method']}: {improvement_pct:+.1f}% improvement\")\n",
    "\n",
    "# Store results\n",
    "globals()['bc_comparison_results'] = comparison_results\n",
    "globals()['bc_comparison_summary'] = summary_df\n",
    "\n",
    "print(\"\\n‚úÖ BC Method Comparison Complete!\")\n",
    "print(\"üíæ Results stored in 'bc_comparison_results' and 'bc_comparison_summary'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Tracking with SHD, F1, and Target Value Plots\n",
    "\n",
    "This section runs BC methods with enhanced performance tracking and creates the requested visualizations showing:\n",
    "- SHD (Structural Hamming Distance) as a function of intervention step\n",
    "- F1 score as a function of intervention step  \n",
    "- Target node value as a function of intervention step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import time\n",
    "\n",
    "print(\"üèÜ Complete BC Method Evaluation\")\n",
    "\n",
    "# Check prerequisites\n",
    "if 'bc_integration_results' not in globals():\n",
    "    raise RuntimeError(\"‚ùå No BC integration results found - please run Cell 16 (ACBO Integration) first\")\n",
    "\n",
    "method_registry = bc_integration_results['method_registry']\n",
    "registered_methods = bc_integration_results['registered_methods']\n",
    "\n",
    "# Focus on BC and baseline methods\n",
    "eval_methods = ['random_baseline', 'learning_baseline', \n",
    "                'bc_surrogate_random', 'bc_acquisition_learning', 'bc_trained_both']\n",
    "eval_methods = [m for m in eval_methods if m in registered_methods]\n",
    "\n",
    "if not eval_methods:\n",
    "    raise RuntimeError(\"‚ùå No BC methods found for evaluation\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(eval_methods)} methods: {eval_methods}\")\n",
    "\n",
    "# Import necessary modules\n",
    "from src.causal_bayes_opt.experiments.test_scms import create_simple_test_scm\n",
    "from src.causal_bayes_opt.experiments.benchmark_scms import (\n",
    "    create_fork_scm, create_chain_scm, create_collider_scm\n",
    ")\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Configuration\n",
    "eval_config = {\n",
    "    'scms': {\n",
    "        'simple': lambda: create_simple_test_scm(),\n",
    "        'fork': lambda: create_fork_scm(),\n",
    "        'chain': lambda: create_chain_scm(chain_length=4),\n",
    "        'collider': lambda: create_collider_scm()\n",
    "    },\n",
    "    'n_runs_per_scm': 3,  # Reduced for demo\n",
    "    'seeds': [42, 43, 44],\n",
    "    'max_interventions': 15,\n",
    "    'n_observational_samples': 50\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Evaluation Configuration:\")\n",
    "print(f\"  SCMs to test: {list(eval_config['scms'].keys())}\")\n",
    "print(f\"  Runs per SCM: {eval_config['n_runs_per_scm']}\")\n",
    "print(f\"  Max interventions: {eval_config['max_interventions']}\")\n",
    "\n",
    "# Run evaluation\n",
    "print(f\"\\nüöÄ Running Evaluation...\")\n",
    "evaluation_results = {}\n",
    "\n",
    "for method_name in eval_methods:\n",
    "    print(f\"\\nüìä Evaluating {method_name}:\")\n",
    "    method = method_registry.get_method(method_name)\n",
    "    \n",
    "    if not method:\n",
    "        print(f\"  ‚ùå Method not found in registry\")\n",
    "        continue\n",
    "\n",
    "    method_results = {\n",
    "        'scm_results': {},\n",
    "        'all_final_values': [],\n",
    "        'all_improvements': [],\n",
    "        'all_runtimes': [],\n",
    "        'success_count': 0,\n",
    "        'total_runs': 0\n",
    "    }\n",
    "\n",
    "    for scm_name, scm_creator in eval_config['scms'].items():\n",
    "        print(f\"  Testing on {scm_name}:\", end=\"\")\n",
    "        scm_results = []\n",
    "\n",
    "        for run_idx, seed in enumerate(eval_config['seeds']):\n",
    "            # Create fresh SCM for each run\n",
    "            scm = scm_creator()\n",
    "\n",
    "            # Create config\n",
    "            config = OmegaConf.create({\n",
    "                'experiment': {\n",
    "                    'target': {\n",
    "                        'max_interventions': eval_config['max_interventions'],\n",
    "                        'n_observational_samples': eval_config['n_observational_samples']\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Run method\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Call the run_function attribute of the ExperimentMethod\n",
    "                result = method.run_function(scm, config, run_idx, seed)\n",
    "                runtime = time.time() - start_time\n",
    "\n",
    "                # Extract metrics from result\n",
    "                final_value = 0.0\n",
    "                initial_value = 0.0\n",
    "                \n",
    "                # Check different result formats\n",
    "                if 'learning_history' in result and result['learning_history']:\n",
    "                    history = result['learning_history']\n",
    "                    if history:\n",
    "                        initial_value = history[0].get('outcome_value', history[0].get('target_value', 0.0))\n",
    "                        final_value = history[-1].get('outcome_value', history[-1].get('target_value', 0.0))\n",
    "                elif 'final_target_value' in result:\n",
    "                    final_value = result['final_target_value']\n",
    "                    if 'initial_best' in result:\n",
    "                        initial_value = result['initial_best']\n",
    "                \n",
    "                improvement = final_value - initial_value\n",
    "                \n",
    "                scm_results.append({\n",
    "                    'run_idx': run_idx,\n",
    "                    'seed': seed,\n",
    "                    'success': True,\n",
    "                    'final_value': final_value,\n",
    "                    'initial_value': initial_value,\n",
    "                    'improvement': improvement,\n",
    "                    'runtime': runtime\n",
    "                })\n",
    "                method_results['all_final_values'].append(final_value)\n",
    "                method_results['all_improvements'].append(improvement)\n",
    "                method_results['all_runtimes'].append(runtime)\n",
    "                method_results['success_count'] += 1\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                scm_results.append({\n",
    "                    'run_idx': run_idx,\n",
    "                    'seed': seed,\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'runtime': time.time() - start_time\n",
    "                })\n",
    "                print(\"x\", end=\"\", flush=True)\n",
    "\n",
    "            method_results['total_runs'] += 1\n",
    "\n",
    "        # Store SCM results\n",
    "        method_results['scm_results'][scm_name] = scm_results\n",
    "\n",
    "        # Calculate SCM-specific metrics\n",
    "        successful_runs = [r for r in scm_results if r['success']]\n",
    "        if successful_runs:\n",
    "            scm_final_values = [r['final_value'] for r in successful_runs]\n",
    "            print(f\" (avg: {np.mean(scm_final_values):.3f})\")\n",
    "        else:\n",
    "            print(\" (all failed)\")\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    if method_results['all_final_values']:\n",
    "        method_results['metrics'] = {\n",
    "            'mean_final_value': np.mean(method_results['all_final_values']),\n",
    "            'std_final_value': np.std(method_results['all_final_values']),\n",
    "            'median_final_value': np.median(method_results['all_final_values']),\n",
    "            'mean_improvement': np.mean(method_results['all_improvements']),\n",
    "            'success_rate': method_results['success_count'] / method_results['total_runs'],\n",
    "            'mean_runtime': np.mean(method_results['all_runtimes']),\n",
    "            'total_successful_runs': method_results['success_count']\n",
    "        }\n",
    "    else:\n",
    "        method_results['metrics'] = {\n",
    "            'mean_final_value': 0.0,\n",
    "            'std_final_value': 0.0,\n",
    "            'median_final_value': 0.0,\n",
    "            'mean_improvement': 0.0,\n",
    "            'success_rate': 0.0,\n",
    "            'mean_runtime': 0.0,\n",
    "            'total_successful_runs': 0\n",
    "        }\n",
    "\n",
    "    evaluation_results[method_name] = method_results\n",
    "\n",
    "    print(f\"  Overall: {method_results['metrics']['success_rate']:.1%} success rate, \"\n",
    "          f\"mean value: {method_results['metrics']['mean_final_value']:.3f}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "print(\"\\nüìä Creating Summary Report...\")\n",
    "summary_data = []\n",
    "for method_name, results in evaluation_results.items():\n",
    "    metrics = results['metrics']\n",
    "    summary_data.append({\n",
    "        'Method': method_name,\n",
    "        'Success Rate': f\"{metrics['success_rate']:.1%}\",\n",
    "        'Mean Value': f\"{metrics['mean_final_value']:.3f}\",\n",
    "        'Std Dev': f\"{metrics['std_final_value']:.3f}\",\n",
    "        'Mean Improvement': f\"{metrics['mean_improvement']:.3f}\",\n",
    "        'Mean Runtime': f\"{metrics['mean_runtime']:.2f}s\",\n",
    "        'Successful Runs': f\"{metrics['total_successful_runs']}/{results['total_runs']}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìã Evaluation Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "print(\"\\nüìä Creating Visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('BC Method Evaluation Results', fontsize=16)\n",
    "\n",
    "# 1. Success Rate Comparison\n",
    "ax = axes[0, 0]\n",
    "methods = list(evaluation_results.keys())\n",
    "success_rates = [evaluation_results[m]['metrics']['success_rate'] for m in methods]\n",
    "colors = ['red' if 'baseline' in m else 'green' for m in methods]\n",
    "bars = ax.bar(methods, success_rates, color=colors, alpha=0.7)\n",
    "ax.set_title('Success Rate by Method')\n",
    "ax.set_ylabel('Success Rate')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{rate:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Final Value Distribution\n",
    "ax = axes[0, 1]\n",
    "for i, method in enumerate(methods):\n",
    "    values = evaluation_results[method]['all_final_values']\n",
    "    if values:\n",
    "        y_positions = [i] * len(values)\n",
    "        ax.scatter(values, y_positions, alpha=0.6, s=50, color=colors[i])\n",
    "ax.set_title('Final Value Distribution')\n",
    "ax.set_xlabel('Final Value')\n",
    "ax.set_yticks(range(len(methods)))\n",
    "ax.set_yticklabels(methods)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Improvement by Method\n",
    "ax = axes[1, 0]\n",
    "improvements = [evaluation_results[m]['metrics']['mean_improvement'] for m in methods]\n",
    "bars = ax.bar(methods, improvements, color=colors, alpha=0.7)\n",
    "ax.set_title('Mean Improvement')\n",
    "ax.set_ylabel('Improvement')\n",
    "ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, improvements):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if value > 0 else bar.get_height() - 0.01,\n",
    "            f'{value:.3f}', ha='center', va='bottom' if value > 0 else 'top')\n",
    "\n",
    "# 4. Performance by SCM\n",
    "ax = axes[1, 1]\n",
    "scm_names = list(eval_config['scms'].keys())\n",
    "bar_width = 0.15\n",
    "x = np.arange(len(scm_names))\n",
    "\n",
    "for i, method in enumerate(methods[:3]):  # Show first 3 methods to avoid crowding\n",
    "    if method in evaluation_results:\n",
    "        scm_means = []\n",
    "        for scm_name in scm_names:\n",
    "            scm_results = evaluation_results[method]['scm_results'].get(scm_name, [])\n",
    "            successful = [r['final_value'] for r in scm_results if r.get('success', False)]\n",
    "            scm_means.append(np.mean(successful) if successful else 0)\n",
    "\n",
    "        ax.bar(x + i * bar_width, scm_means, bar_width,\n",
    "               label=method, alpha=0.8, color=colors[i])\n",
    "\n",
    "ax.set_title('Performance by SCM Type')\n",
    "ax.set_xlabel('SCM Type')\n",
    "ax.set_ylabel('Mean Final Value')\n",
    "ax.set_xticks(x + bar_width)\n",
    "ax.set_xticklabels(scm_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nüîë Key Insights:\")\n",
    "\n",
    "# Best performing method\n",
    "if evaluation_results:\n",
    "    best_method = max(evaluation_results.items(),\n",
    "                      key=lambda x: x[1]['metrics']['mean_final_value'])\n",
    "    print(f\"  ‚Ä¢ Best performing method: {best_method[0]}\")\n",
    "    print(f\"    Mean final value: {best_method[1]['metrics']['mean_final_value']:.3f}\")\n",
    "    print(f\"    Mean improvement: {best_method[1]['metrics']['mean_improvement']:.3f}\")\n",
    "\n",
    "# Compare BC to baseline\n",
    "if 'random_baseline' in evaluation_results:\n",
    "    baseline_value = evaluation_results['random_baseline']['metrics']['mean_final_value']\n",
    "    print(f\"\\n  ‚Ä¢ Baseline performance: {baseline_value:.3f}\")\n",
    "    \n",
    "    for method in ['bc_surrogate_random', 'bc_acquisition_learning', 'bc_trained_both']:\n",
    "        if method in evaluation_results:\n",
    "            bc_value = evaluation_results[method]['metrics']['mean_final_value']\n",
    "            improvement_pct = ((bc_value - baseline_value) / abs(baseline_value) * 100 \n",
    "                              if baseline_value != 0 else 0)\n",
    "            print(f\"  ‚Ä¢ {method}: {improvement_pct:+.1f}% vs baseline\")\n",
    "\n",
    "# Store results\n",
    "globals()['bc_final_evaluation_results'] = evaluation_results\n",
    "globals()['bc_final_evaluation_summary'] = summary_df\n",
    "\n",
    "print(\"\\n‚úÖ BC Method Evaluation Complete!\")\n",
    "print(\"üíæ Results stored in 'bc_final_evaluation_results' and 'bc_final_evaluation_summary'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-bayes-opt-9Aj1r1ec-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
