{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Training Pipeline\n",
    "\n",
    "**Purpose**: Train new GRPO policies or fine-tune existing checkpoints for causal discovery.\n",
    "\n",
    "**Features**:\n",
    "- ‚úÖ **119x improvement** surrogate integration system\n",
    "- ‚úÖ **Multiple training modes**: QUICK, FULL, PRECISION\n",
    "- ‚úÖ **Fine-tuning support** from existing checkpoints\n",
    "- ‚úÖ **Checkpoint management** with metadata\n",
    "- ‚úÖ **Training monitoring** and early stopping\n",
    "\n",
    "**Workflow**:\n",
    "1. Configure training parameters\n",
    "2. Generate training SCMs\n",
    "3. Train or fine-tune policy\n",
    "4. Save checkpoint with metadata\n",
    "5. Quick validation\n",
    "\n",
    "**Output**: Trained checkpoint ready for evaluation in `grpo_evaluation_benchmark.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß JAX devices: [CpuDevice(id=0)]\n",
      "üîß JAX backend: cpu\n",
      "‚úÖ Environment Setup Complete\n",
      "üìÅ Project root: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt\n",
      "üìÅ Checkpoint directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Environment Setup for GRPO Training Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Project root configuration\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"experiments\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Core JAX imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as onp\n",
    "import pyrsistent as pyr\n",
    "\n",
    "# Configuration\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Project imports\n",
    "from causal_bayes_opt.experiments.variable_scm_factory import VariableSCMFactory\n",
    "from causal_bayes_opt.training.enriched_trainer import EnrichedGRPOTrainer\n",
    "from causal_bayes_opt.data_structures.scm import get_variables, get_target, get_edges\n",
    "\n",
    "# 119x Improvement System\n",
    "from causal_bayes_opt.surrogate.bootstrap import create_bootstrap_surrogate_features\n",
    "from causal_bayes_opt.surrogate.phase_manager import PhaseConfig, BootstrapConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# JAX configuration\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"üîß JAX devices: {jax.devices()}\")\n",
    "print(f\"üîß JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Create directories\n",
    "checkpoint_dir = project_root / \"checkpoints\" / \"grpo_training\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment Setup Complete\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìÅ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training Mode: QUICK\n",
      "üéØ Training Objective: TARGET_FOCUSED\n",
      "üìù Description: Fast testing and development\n",
      "üìù Objective: Prioritizes target variable maximization above all else\n",
      "‚è±Ô∏è Duration: 5 minutes\n",
      "üéì Learning rate: 0.001\n",
      "üìä Number of SCMs: 32\n",
      "üîÑ Episodes per SCM: 3\n",
      "‚öñÔ∏è Reward weights: {'optimization': 0.8, 'discovery': 0.1, 'efficiency': 0.1}\n",
      "\n",
      "üöÄ Training from scratch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configure GRPO Training Parameters\n",
    "\"\"\"\n",
    "\n",
    "# Training mode selection\n",
    "TRAINING_MODE = \"QUICK\"  # Options: \"QUICK\", \"FULL\", \"PRECISION\"\n",
    "\n",
    "# NEW: Training objective selection\n",
    "TRAINING_OBJECTIVE = \"TARGET_FOCUSED\"  # Options: \"STRUCTURE_FOCUSED\", \"TARGET_FOCUSED\", \"BALANCED\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Fine-tuning configuration (set to None for training from scratch)\n",
    "FINETUNE_FROM_CHECKPOINT = None  # Or path to existing checkpoint\n",
    "\n",
    "# Mode configurations\n",
    "training_configs = {\n",
    "    \"QUICK\": {\n",
    "        'episodes_per_scm': 3,\n",
    "        'episode_length': 8,\n",
    "        'learning_rate': 0.001,\n",
    "        'training_duration_minutes': 5,\n",
    "        'num_scms': 32,  # Number of training SCMs\n",
    "        'description': 'Fast testing and development'\n",
    "    },\n",
    "    \"FULL\": {\n",
    "        'episodes_per_scm': 8,\n",
    "        'episode_length': 12,\n",
    "        'learning_rate': 0.001,\n",
    "        'training_duration_minutes': 15,\n",
    "        'num_scms': 64,\n",
    "        'description': 'Production-quality training'\n",
    "    },\n",
    "    \"PRECISION\": {\n",
    "        'episodes_per_scm': 15,\n",
    "        'episode_length': 15,\n",
    "        'learning_rate': 0.0005,\n",
    "        'training_duration_minutes': 30,\n",
    "        'num_scms': 128,\n",
    "        'description': 'Maximum quality training'\n",
    "    }\n",
    "}\n",
    "\n",
    "# NEW: Reward weight configurations for different objectives\n",
    "objective_configs = {\n",
    "    \"STRUCTURE_FOCUSED\": {\n",
    "        'reward_weights': {\n",
    "            'optimization': 0.2,    # Low target optimization weight\n",
    "            'discovery': 0.6,       # High structure discovery weight\n",
    "            'efficiency': 0.2       # Medium efficiency weight\n",
    "        },\n",
    "        'description': 'Emphasizes quick SHD reduction and structure learning'\n",
    "    },\n",
    "    \"TARGET_FOCUSED\": {\n",
    "        'reward_weights': {\n",
    "            'optimization': 0.8,    # High target optimization weight\n",
    "            'discovery': 0.1,       # Low structure discovery weight\n",
    "            'efficiency': 0.1       # Low efficiency weight\n",
    "        },\n",
    "        'description': 'Prioritizes target variable maximization above all else'\n",
    "    },\n",
    "    \"BALANCED\": {\n",
    "        'reward_weights': {\n",
    "            'optimization': 0.5,    # Balanced target optimization\n",
    "            'discovery': 0.3,       # Medium structure discovery\n",
    "            'efficiency': 0.2       # Medium efficiency\n",
    "        },\n",
    "        'description': 'Balanced approach between all objectives'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get configurations\n",
    "train_config = training_configs[TRAINING_MODE]\n",
    "objective_config = objective_configs[TRAINING_OBJECTIVE]\n",
    "\n",
    "# Production configurations from Phase 4 validation\n",
    "PRODUCTION_PHASE_CONFIG = PhaseConfig(\n",
    "    bootstrap_steps=100,\n",
    "    transition_steps=50,\n",
    "    exploration_noise_start=0.5,\n",
    "    exploration_noise_end=0.1,\n",
    "    transition_schedule=\"linear\"\n",
    ")\n",
    "\n",
    "PRODUCTION_BOOTSTRAP_CONFIG = BootstrapConfig(\n",
    "    structure_encoding_dim=128,\n",
    "    use_graph_distance=True,\n",
    "    use_structural_priors=True,\n",
    "    noise_schedule=\"exponential_decay\",\n",
    "    min_noise_factor=0.1\n",
    ")\n",
    "\n",
    "print(f\"üéØ Training Mode: {TRAINING_MODE}\")\n",
    "print(f\"üéØ Training Objective: {TRAINING_OBJECTIVE}\")\n",
    "print(f\"üìù Description: {train_config['description']}\")\n",
    "print(f\"üìù Objective: {objective_config['description']}\")\n",
    "print(f\"‚è±Ô∏è Duration: {train_config['training_duration_minutes']} minutes\")\n",
    "print(f\"üéì Learning rate: {train_config['learning_rate']}\")\n",
    "print(f\"üìä Number of SCMs: {train_config['num_scms']}\")\n",
    "print(f\"üîÑ Episodes per SCM: {train_config['episodes_per_scm']}\")\n",
    "print(f\"‚öñÔ∏è Reward weights: {objective_config['reward_weights']}\")\n",
    "\n",
    "if FINETUNE_FROM_CHECKPOINT:\n",
    "    print(f\"\\nüîß Fine-tuning from: {FINETUNE_FROM_CHECKPOINT}\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Save Checkpoint with Training Metadata\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíæ Saving Checkpoint with Metadata\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate checkpoint name with timestamp and objective\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_name = f\"grpo_{TRAINING_MODE.lower()}_{TRAINING_OBJECTIVE.lower()}_{timestamp}\"\n",
    "final_checkpoint_dir = checkpoint_dir / checkpoint_name\n",
    "\n",
    "# Copy the actual checkpoint files from trainer output to the new named directory\n",
    "if checkpoint_path and checkpoint_path.exists():\n",
    "    print(f\"\\nüìÅ Copying checkpoint files from: {checkpoint_path}\")\n",
    "    print(f\"   to: {final_checkpoint_dir}\")\n",
    "    \n",
    "    # Create the new checkpoint directory\n",
    "    final_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy all files from the trainer's checkpoint directory\n",
    "    if checkpoint_path.is_dir():\n",
    "        # Copy all files in the directory\n",
    "        import shutil\n",
    "        for item in checkpoint_path.iterdir():\n",
    "            if item.is_file():\n",
    "                shutil.copy2(item, final_checkpoint_dir / item.name)\n",
    "                print(f\"   ‚úì Copied: {item.name}\")\n",
    "    else:\n",
    "        # Single file checkpoint\n",
    "        shutil.copy2(checkpoint_path, final_checkpoint_dir / \"checkpoint.pkl\")\n",
    "        print(f\"   ‚úì Copied checkpoint file\")\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint files copied successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No checkpoint found at expected path: {checkpoint_path}\")\n",
    "    print(f\"   Creating empty checkpoint directory: {final_checkpoint_dir}\")\n",
    "    final_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'training_config': {\n",
    "        'mode': TRAINING_MODE,\n",
    "        'objective': TRAINING_OBJECTIVE,\n",
    "        'config': train_config,\n",
    "        'objective_config': objective_config,\n",
    "        'total_episodes': total_episodes,\n",
    "        'learning_rate': grpo_config.training.learning_rate,\n",
    "        'architecture': OmegaConf.to_container(grpo_config.training.architecture),\n",
    "        'reward_weights': objective_config['reward_weights']\n",
    "    },\n",
    "    'scm_config': scm_config,\n",
    "    'training_results': {\n",
    "        'duration_minutes': training_duration / 60,\n",
    "        'final_performance': performance,\n",
    "        'timestamp': timestamp,\n",
    "        'success': True\n",
    "    },\n",
    "    'environment': {\n",
    "        'jax_backend': jax.default_backend(),\n",
    "        'num_devices': len(jax.devices()),\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'surrogate_config': {\n",
    "        'phase_config': OmegaConf.to_container(grpo_config.surrogate_integration.phase_config),\n",
    "        'bootstrap_config': OmegaConf.to_container(grpo_config.surrogate_integration.bootstrap_config)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata both in the checkpoint directory and in the parent\n",
    "metadata_path = final_checkpoint_dir / \"metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Also save in parent directory with checkpoint name\n",
    "parent_metadata_path = final_checkpoint_dir.parent / f\"{checkpoint_name}_metadata.json\"\n",
    "with open(parent_metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint saved: {final_checkpoint_dir}\")\n",
    "print(f\"üìã Metadata saved: {metadata_path}\")\n",
    "print(f\"üìã Parent metadata saved: {parent_metadata_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"  Mode: {TRAINING_MODE}\")\n",
    "print(f\"  Objective: {TRAINING_OBJECTIVE}\")\n",
    "print(f\"  Duration: {training_duration/60:.1f} minutes\")\n",
    "print(f\"  Episodes: {total_episodes}\")\n",
    "print(f\"  Reward weights: {objective_config['reward_weights']}\")\n",
    "print(f\"  Final reward: {performance.get('final_reward', 'N/A')}\")\n",
    "print(f\"  Checkpoint: {checkpoint_name}\")\n",
    "print(f\"\\nüéØ Training objective: {objective_config['description']}\")\n",
    "\n",
    "# Clean up old generic checkpoints if desired\n",
    "CLEANUP_GENERIC_CHECKPOINTS = True  # Set to False to keep original checkpoints\n",
    "\n",
    "if CLEANUP_GENERIC_CHECKPOINTS and checkpoint_path and checkpoint_path.exists():\n",
    "    print(f\"\\nüßπ Cleaning up generic checkpoint: {checkpoint_path}\")\n",
    "    try:\n",
    "        if checkpoint_path.is_dir():\n",
    "            shutil.rmtree(checkpoint_path)\n",
    "        else:\n",
    "            checkpoint_path.unlink()\n",
    "        print(\"   ‚úì Cleanup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not clean up: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Checkpoint Naming\n",
    "\n",
    "After training completes:\n",
    "1. The trainer saves checkpoints with generic names (e.g., `enriched_grpo_final`, `enriched_grpo_episode_50`)\n",
    "2. The checkpoint saving cell above copies these to descriptively named directories (e.g., `grpo_quick_structure_focused_20250719_123456`)\n",
    "3. The evaluation notebook can then identify the training objective from the checkpoint name\n",
    "\n",
    "**Important**: If you want to evaluate intermediate checkpoints (episode_50, etc.), you'll need to manually copy and rename them with the appropriate objective in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Generating Training SCMs\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 3 vars, 2 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 4 vars, 3 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 5 vars, 4 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 6 vars, 5 edges, target=X3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 32 training SCMs\n",
      "\n",
      "üìä SCM Distribution:\n",
      "Structure types: {'fork': 8, 'chain': 8, 'collider': 8, 'mixed': 8}\n",
      "Variable counts: {3: 8, 4: 8, 5: 8, 6: 8}\n",
      "\n",
      "üìà Total training episodes: 96\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Diverse SCMs for Training\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî¨ Generating Training SCMs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# SCM generation configuration\n",
    "scm_config = {\n",
    "    'variable_range': [3, 6],\n",
    "    'structure_types': ['fork', 'chain', 'collider', 'mixed'],\n",
    "    'noise_scale': 1.0,\n",
    "    'edge_density_range': [0.3, 0.7],\n",
    "    'target_selection': 'random'\n",
    "}\n",
    "\n",
    "# Create factory\n",
    "scm_factory = VariableSCMFactory(\n",
    "    noise_scale=scm_config['noise_scale'],\n",
    "    coefficient_range=(-2.0, 2.0),\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Generate balanced SCM suite\n",
    "training_scms = []\n",
    "scm_metadata = []\n",
    "key = random.PRNGKey(RANDOM_SEED)\n",
    "\n",
    "# Calculate SCMs per configuration\n",
    "n_structure_types = len(scm_config['structure_types'])\n",
    "n_var_sizes = scm_config['variable_range'][1] - scm_config['variable_range'][0] + 1\n",
    "scms_per_config = train_config['num_scms'] // (n_structure_types * n_var_sizes)\n",
    "remaining_scms = train_config['num_scms'] % (n_structure_types * n_var_sizes)\n",
    "\n",
    "# Generate SCMs\n",
    "for structure_type in scm_config['structure_types']:\n",
    "    for n_vars in range(scm_config['variable_range'][0], scm_config['variable_range'][1] + 1):\n",
    "        n_instances = scms_per_config + (1 if remaining_scms > 0 else 0)\n",
    "        remaining_scms = max(0, remaining_scms - 1)\n",
    "        \n",
    "        for instance in range(n_instances):\n",
    "            key, subkey = random.split(key)\n",
    "            \n",
    "            scm = scm_factory.create_variable_scm(\n",
    "                num_variables=n_vars,\n",
    "                structure_type=structure_type,\n",
    "                target_variable=None,\n",
    "                edge_density=0.5\n",
    "            )\n",
    "            \n",
    "            training_scms.append(scm)\n",
    "            \n",
    "            scm_metadata.append({\n",
    "                'structure_type': structure_type,\n",
    "                'n_variables': n_vars,\n",
    "                'target': get_target(scm),\n",
    "                'n_edges': len(get_edges(scm)),\n",
    "                'variables': list(get_variables(scm)),\n",
    "                'instance': instance\n",
    "            })\n",
    "\n",
    "print(f\"‚úÖ Generated {len(training_scms)} training SCMs\")\n",
    "\n",
    "# Analyze distribution\n",
    "structure_counts = {}\n",
    "variable_counts = {}\n",
    "\n",
    "for meta in scm_metadata:\n",
    "    struct_type = meta['structure_type']\n",
    "    n_vars = meta['n_variables']\n",
    "    \n",
    "    structure_counts[struct_type] = structure_counts.get(struct_type, 0) + 1\n",
    "    variable_counts[n_vars] = variable_counts.get(n_vars, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä SCM Distribution:\")\n",
    "print(f\"Structure types: {structure_counts}\")\n",
    "print(f\"Variable counts: {variable_counts}\")\n",
    "\n",
    "# Total episodes\n",
    "total_episodes = len(training_scms) * train_config['episodes_per_scm']\n",
    "print(f\"\\nüìà Total training episodes: {total_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GRPO Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 3 vars, 2 edges, target=X1\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 4 vars, 3 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 5 vars, 4 edges, target=X2\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated collider SCM: 6 vars, 5 edges, target=X3\n",
      "INFO:causal_bayes_opt.experiments.test_scms:Created linear SCM with 6 variables, 5 edges, target='X4'\n",
      "INFO:causal_bayes_opt.experiments.variable_scm_factory:Generated mixed SCM: 6 vars, 5 edges, target=X4\n",
      "INFO:causal_bayes_opt.training.modular_trainer:Created 16 variable SCMs for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GRPO Policy Training\n",
      "======================================================================\n",
      "üîß Training mode: QUICK\n",
      "üéØ Training objective: TARGET_FOCUSED\n",
      "üìä Total episodes: 96\n",
      "‚öñÔ∏è Reward weights: {'optimization': 0.8, 'discovery': 0.1, 'efficiency': 0.1}\n",
      "‚úÖ 119x surrogate integration: ACTIVE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Using optimized GRPO config: group_size=64, interventions_per_state=8\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Correct GRPO Config: group_size=64, lr=0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Correct GRPO Config: entropy_coeff=0.010, clip_ratio=0.20\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Initialized trainer with 6 max variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:GRPO group size: 64, update frequency: 1 episodes\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Starting enriched GRPO training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÉ Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000070085187\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 0: reward=0.617, intervention_rate=1.000, scm=fork_3var, F1=0.000, P(Parents)=0.000, SHD=2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 10):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -6.52063737e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.98\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1811, Std: 1.1673\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.49\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -3.4989\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 10):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -3.498926564989082)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -3.498926564989082}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.3498926564989082}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.849893\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.849893\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.850) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000040070156\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 20):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  2.94296485e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.97\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1660, Std: 1.1934\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.49\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.4006\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 20):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 5.4005927329141485)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 5.4005927329141485}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000019655575\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 30):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.0000000e+09  1.7346222e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.95\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1659, Std: 1.2056\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.48\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -3.2212\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 30):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -3.2212389260823824)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -3.2212389260823824}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.32212389260823826}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.822124\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.822124\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.822) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000010398447\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 40):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -1.11606127e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.94\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1534, Std: 1.1933\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.48\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 1.7185\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 40):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 1.718491596270665)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 1.718491596270665}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.1718491596270665}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.671849\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.671849\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000026549614\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 5):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=1.899687\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.556\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00002655\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00002655\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=-0.100, max=0.690, group_baseline=0.556\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.238920, entropy=-2.329358\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.14641731\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 2.475430\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000068182960\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 50):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.40372931e-01 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.91\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1784, Std: 1.2037\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.47\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -2.3587\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 50):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -2.358710344539843)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': -2.358710344539843}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.23587103445398433}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.735871\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.735871\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000088189069\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 60):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-3.76652791e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.89\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1860, Std: 1.2227\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.46\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.3198\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 60):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 0.3197622472940592)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 0.3197622472940592}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.03197622472940592}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.531976\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.531976\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000100461280\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 70):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-5.14250137e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.88\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1663, Std: 1.2137\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.46\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.8666\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 70):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 0.8666339924097763)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 0.8666339924097763}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.08666339924097764}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.586663\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.586663\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000114008122\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 80):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-7.68983635e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.86\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1689, Std: 1.2024\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.45\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.1522\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 80):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 2.1522346530101397)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 2.1522346530101397}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.215223465301014}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.715223\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.715223\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000124730626\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 10):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=2.152235\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.606\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00012473\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00012473\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.550, max=0.715, group_baseline=0.606\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.344016, entropy=-2.409519\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.06150858\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 1000000002.278794\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000112546187\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 10: reward=0.635, intervention_rate=1.000, scm=collider_3var, F1=0.000, P(Parents)=0.000, SHD=2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 90):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -5.93693796e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.83\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1566, Std: 1.2209\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.44\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.1215\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 90):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -0.12147873163108835)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -0.12147873163108835}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.012147873163108835}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.512148\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.512148\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000123053157\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 100):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -5.76952245e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.81\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1779, Std: 1.2527\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.44\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 1.2296\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 100):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 1.2296411028130694)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 1.2296411028130694}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.12296411028130694}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.622964\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.622964\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.705, trend=-0.227\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000127430681\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 110):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -6.68951801e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.80\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1777, Std: 1.2717\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.43\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.3361\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 110):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -0.3360839546974502)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -0.3360839546974502}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.03360839546974503}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.533608\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.533608\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.673, trend=-0.466\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000121191034\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 120):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -1.40008776e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.78\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1959, Std: 1.2807\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.43\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.0905\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 120):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -1.090507537765823)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -1.090507537765823}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.10905075377658231}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.609051\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.609051\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.634, trend=-0.213\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000184825266\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 15):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=2.089305\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.624\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00018483\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00018483\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.567, max=0.709, group_baseline=0.624\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.331930, entropy=-2.458335\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.22510529\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 2.474841\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000202878781\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 130):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -1.22693192e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.75\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1600, Std: 1.2891\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.42\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.7819\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 130):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 0.7819120455651767)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 0.7819120455651767}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.07819120455651768}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.578191\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.578191\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.610, trend=-0.094\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000218819796\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 140):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -5.04776853e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.73\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1980, Std: 1.3620\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.41\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.8845\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 140):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 0.8845331478617399)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 0.8845331478617399}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.08845331478617399}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.588453\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.588453\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.601, trend=-0.147\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000268436010\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 150):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -7.04488415e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.72\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2220, Std: 1.3449\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.41\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.5012\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 150):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 0.5012172608414154)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 0.5012172608414154}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.05012172608414154}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.550122\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.550122\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.583, trend=+0.018\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000266993634\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 160):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -1.22421917e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.70\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2324, Std: 1.3754\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.40\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 1.0417\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 160):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 1.0417326466784107)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 1.0417326466784107}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.10417326466784108}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.604173\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.604173\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.590, trend=+0.018\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000260863783\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 20):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=4.285308\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.715\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00026086\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00026086\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.541, max=0.929, group_baseline=0.715\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.338722, entropy=-2.506287\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.16617504\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.364287\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000268619891\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 20: reward=0.655, intervention_rate=1.000, scm=fork_4var, F1=0.000, P(Parents)=0.000, SHD=3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 170):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.03129082e-01 -1.00000000e+09 -1.58440575e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.67\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.50827012 0.         0.49172988]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2283, Std: 1.4244\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.39\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.6462\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 170):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, 2.646223840090626)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': 2.646223840090626}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.26462238400906263}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.764622\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.764622\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.608, trend=+0.049\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000290548231\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 180):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.57281728e-01 -1.00000000e+09 -1.60907849e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.66\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.50054734 0.         0.49945266]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1960, Std: 1.3787\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.39\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.3147\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 180):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 0.31472392962327306)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 0.31472392962327306}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.03147239296232731}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.531472\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.531472\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.589, trend=+0.019\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000316993655\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 190):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-5.05323453e-02 -1.00000000e+09  3.62207247e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.64\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.48678356 0.         0.51321644]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2195, Std: 1.4357\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.38\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.3932\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 190):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -0.3931926809496964)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': -0.3931926809496964}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.03931926809496964}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.539319\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.539319\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.592, trend=-0.084\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000313544943\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 200):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-4.39791852e-02 -1.00000000e+09 -5.07035069e-03]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.62\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.49401431 0.         0.50598569]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2439, Std: 1.4225\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.38\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -3.2664\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 200):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -3.2663797136141675)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': -3.2663797136141675}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.32663797136141676}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.826638\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.826638\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.827) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.613, trend=+0.293\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000311885981\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 25):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=4.309387\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.701\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00031189\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00031189\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.529, max=0.931, group_baseline=0.701\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.358060, entropy=-2.967681\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.10629620\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 250000003.603465\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000295082284\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 210):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.55291843e-01 -7.78364556e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.59\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.48785253 0.51214747 0.        ]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2445, Std: 1.4517\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.36\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.7654\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 210):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 2.765392385737001)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 2.765392385737001}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.2765392385737001}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.776539\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.776539\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.637, trend=+0.167\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000268883627\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 220):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-9.72540284e-02 -1.37117677e-01 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.58\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.5063147 0.4936853 0.       ]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2120, Std: 1.4507\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.36\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.7301\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 220):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -0.7301010490454516)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -0.7301010490454516}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.07301010490454517}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.573010\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.573010\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.633, trend=-0.005\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000221593719\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 230):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.87556044e-01 -1.61554193e-01 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.56\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.4958398 0.5041602 0.       ]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2517, Std: 1.5025\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.35\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.1667\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 230):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 0.16670361518362098)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 0.16670361518362098}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.016670361518362098}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.516670\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.516670\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.627, trend=-0.072\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000182970261\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 240):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.52263603e-01 -3.53818389e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.55\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.481119 0.518881 0.      ]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2299, Std: 1.4974\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.35\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.0817\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 240):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 0.08168640993097384)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 0.08168640993097384}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.008168640993097385}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.508169\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.508169\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.619, trend=-0.042\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000180576042\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 30):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=2.706404\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.603\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00018058\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00018058\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.508, max=0.771, group_baseline=0.603\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.312026, entropy=-3.022894\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.06320066\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 500000003.005940\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000179468194\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 30: reward=0.730, intervention_rate=1.000, scm=collider_4var, F1=0.000, P(Parents)=0.000, SHD=3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 250):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-4.09824462e-02 -1.00000000e+09 -1.43709299e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.52\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.51693815 0.         0.48306185]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2309, Std: 1.5314\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.34\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.0532\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 250):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -1.0531624543576887)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': -1.0531624543576887}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.10531624543576888}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.605316\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.605316\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.625, trend=+0.001\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000177332439\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 260):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.32563242e-01 -1.00000000e+09 -1.30829318e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.50\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.49971101 0.         0.50028899]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2425, Std: 1.5726\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.33\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.7534\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 260):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, 5.753420277851165)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': 5.753420277851165}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.664, trend=+0.235\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000175754772\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 270):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.03729646e-01 -1.00000000e+09 -7.80672016e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.48\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.49567801 0.         0.50432199]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2066, Std: 1.5381\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.33\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.0730\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 270):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -1.072991740144121)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': -1.072991740144121}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.1072991740144121}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.607299\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.607299\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.648, trend=+0.076\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000165573429\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 280):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-6.49160570e-02 -1.00000000e+09 -1.60686067e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.47\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.51629551 0.         0.48370449]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2339, Std: 1.5853\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.32\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.8733\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 280):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 0.8732952880617934)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 0.8732952880617934}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.08732952880617934}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.587330\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.587330\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.654, trend=+0.048\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000176072621\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 35):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=4.078014\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.671\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00017607\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00017607\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.511, max=0.908, group_baseline=0.671\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.299149, entropy=-3.082279\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.08146070\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 625000003.206366\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000204290628\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 290):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-9.61454798e-02 -1.00000000e+09 -1.19219273e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.44\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.50401275 0.         0.49598725]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2705, Std: 1.5612\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.31\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -3.3145\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 290):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -3.314493759347731)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': -3.314493759347731}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.33144937593477314}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.831449\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.831449\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.831) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.683, trend=+0.005\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000225105291\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 300):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.70179367e-01 -1.00000000e+09 -2.84581539e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.42\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.47510259 0.         0.52489741]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2691, Std: 1.6464\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.31\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 4.4948\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 300):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 4.494808540596029)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 4.494808540596029}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.4494808540596029}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.949481\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.949481\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.949) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.696, trend=+0.173\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000246601943\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 310):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.32982823e-01 -1.00000000e+09 -1.20443714e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.41\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.49777084 0.         0.50222916]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2665, Std: 1.6541\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.30\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.3135\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 310):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -0.31349002838067597)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': -0.31349002838067597}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.031349002838067595}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.531349\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.531349\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.671, trend=-0.042\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000261176610\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 320):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-4.04728068e-02 -1.00000000e+09 -1.28363542e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X3', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.39\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.51579532 0.         0.48420468]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X3 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2521, Std: 1.6644\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.30\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -2.8177\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 320):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -2.817685532922683)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X3'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X3': -2.817685532922683}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.2817685532922683}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.781769\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.781769\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.692, trend=+0.265\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000273539785\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 40):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=2.817686\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.634\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00027354\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00027354\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.510, max=0.782, group_baseline=0.634\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.309267, entropy=-3.148824\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.11615368\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 125000002.997208\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000282895885\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 40: reward=0.752, intervention_rate=1.000, scm=fork_5var, F1=0.000, P(Parents)=0.000, SHD=4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 330):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.11223399e-01 -1.43613231e-01 -1.00000000e+09 -1.65342361e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.36\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.34043422 0.33241857 0.         0.32714721]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2167, Std: 1.6777\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.29\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.7135\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 330):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 2.7135108688039185)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 2.7135108688039185}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.27135108688039183}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.771351\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.771351\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.717, trend=+0.263\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000280705660\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 340):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-9.56334212e-02 -9.36588758e-02 -1.00000000e+09 -1.53109227e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.34\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33788354 0.3383804  0.         0.32373607]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2339, Std: 1.7283\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.28\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.2037\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 340):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, 2.203656538869584)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 2.203656538869584}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.22036565388695842}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.720366\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.720366\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.739, trend=+0.115\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000281316942\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 350):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.85610848e-01 -1.69694128e-01 -1.00000000e+09 -1.27296327e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.33\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.32712429 0.33106825 0.         0.34180746]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.1740, Std: 1.6842\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.28\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.2125\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 350):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, -1.2124758426162887)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': -1.2124758426162887}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.12124758426162888}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.621248\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.621248\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.740, trend=-0.379\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000257517956\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 360):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.86335920e-01 -1.57183806e-01 -1.00000000e+09 -1.70988891e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.31\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.32957386 0.33697597 0.         0.33345018]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2699, Std: 1.6080\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.27\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 1.3893\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 360):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, 1.3892583167757386)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 1.3892583167757386}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.13892583167757386}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.638926\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.638926\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.704, trend=+0.032\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000136566956\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 45):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=2.813447\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.625\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00013657\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00013657\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.511, max=0.781, group_baseline=0.625\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.324536, entropy=-3.509709\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.06923085\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.247319\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000127261216\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 370):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-2.32844163e-01 -1.00000000e+09  1.11206064e-02 -1.89674239e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.28\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.30825998 0.         0.37291669 0.31882333]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2625, Std: 1.7589\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.26\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.7897\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 370):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, 2.789728539053169)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 2.789728539053169}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.2789728539053169}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.778973\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.778973\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.721, trend=+0.192\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000114708306\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 380):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.07646146e-01 -1.00000000e+09 -9.70615384e-02 -2.24381924e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.27\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.34243193 0.         0.34530775 0.31226032]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2772, Std: 1.9068\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.26\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -5.2980\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 380):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -5.298020770811118)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -5.298020770811118}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.762, trend=+0.169\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000139192854\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 390):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.82744616e-01 -1.00000000e+09 -1.50015726e-01 -1.87643017e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.25\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33083904 0.         0.33961584 0.32954512]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2740, Std: 1.9155\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.25\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 3.9216\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 390):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, 3.9216100901605375)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 3.9216100901605375}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.3921610090160538}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.892161\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.892161\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.892) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.769, trend=-0.057\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000156536940\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 400):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.01205828e-01 -1.00000000e+09 -1.02213581e-01 -1.72595802e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.23\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33979052 0.         0.33951323 0.32069625]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3024, Std: 1.8348\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.24\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.6494\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 400):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -0.649371611065005)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': -0.649371611065005}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.0649371611065005}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.564937\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.564937\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.730, trend=+0.034\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000146701787\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 50):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=3.792242\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.678\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00014670\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00014670\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.526, max=0.879, group_baseline=0.678\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.363805, entropy=-3.593537\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.12529609\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 375000003.665766\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000166141002\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 50: reward=0.663, intervention_rate=1.000, scm=collider_5var, F1=0.000, P(Parents)=0.000, SHD=3\n",
      "INFO:causal_bayes_opt.training.modular_trainer:Saved checkpoint: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/enriched_grpo_episode_50/checkpoint.pkl\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 410):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.20879423e-01 -1.94642122e-01 -1.00000000e+09 -1.52854427e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.20\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.3431349  0.32272946 0.         0.33413564]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2470, Std: 1.8720\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.23\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.7902\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 410):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -1.7901639132635725)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': -1.7901639132635725}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.17901639132635727}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.679016\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.679016\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.745, trend=-0.103\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000196272279\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 420):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-5.75667328e-02 -5.10917678e-02 -1.00000000e+09 -1.84917963e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.19\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.34437933 0.34626222 0.         0.30935844]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2218, Std: 1.8297\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.23\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.4263\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 420):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -1.4262773225567607)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': -1.4262773225567607}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.14262773225567607}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.642628\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.642628\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.731, trend=-0.129\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000166470582\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 430):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00476723e-01 -1.28933048e-01 -1.00000000e+09 -1.05483030e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.17\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33650242 0.32842964 0.         0.33506794]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2430, Std: 1.8548\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.22\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.5977\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 430):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 0.5977179401668391)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 0.5977179401668391}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.059771794016683914}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.559772\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.559772\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.710, trend=-0.161\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000118386130\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 440):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 1.91677526e-02 -1.28050356e-01 -1.00000000e+09 -9.41545600e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.16\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.3587972  0.31590248 0.         0.32530033]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2843, Std: 1.9354\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.22\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -2.4270\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 440):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, -2.4270224115643835)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': -2.4270224115643835}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.24270224115643835}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.742702\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.742702\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.712, trend=+0.121\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000114690423\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 55):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=5.024001\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.725\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00011469\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00011469\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.527, max=1.000, group_baseline=0.725\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.366596, entropy=-3.683992\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.12254649\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.939983\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000111476044\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 450):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-4.08150225e-02 -9.52452250e-02 -1.00000000e+09 -3.63476200e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.12\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33820938 0.32223554 0.         0.33955509]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3041, Std: 2.0393\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.21\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.1949\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 450):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, 5.19493684585533)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 5.19493684585533}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.750, trend=+0.361\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000133646129\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 460):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-9.86946425e-02 -1.36928113e-01 -1.00000000e+09 -6.87688167e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.11\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33406096 0.32274402 0.         0.34319502]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3555, Std: 2.0465\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.20\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.4316\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 460):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 2.4315648343193885)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 2.4315648343193885}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.24315648343193885}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.743156\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.743156\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.760, trend=-0.036\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000158193413\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 470):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-8.45190510e-02 -7.31567161e-02 -1.00000000e+09 -2.24615951e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.09\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.32585801 0.3292608  0.         0.34488119]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3701, Std: 2.2365\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.20\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.2573\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 470):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, -1.2572885435297825)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': -1.2572885435297825}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.12572885435297826}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.625729\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.625729\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.745, trend=-0.374\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000202483315\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 480):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-7.61851866e-02 -8.23132117e-02 -1.00000000e+09 -7.98060455e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X4', 'X2', 'X0', 'X3'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.08\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.33433865 0.33244368 0.         0.33321767]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.2852, Std: 2.0647\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.19\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.2429\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 480):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 2.242911113712217)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 2.242911113712217}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.2242911113712217}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.724291\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.724291\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.717, trend=-0.168\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000241046440\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 60):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=6.488741\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.758\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00024105\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00024105\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.507, max=1.000, group_baseline=0.758\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.320350, entropy=-3.781985\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.10594544\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.986673\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000271103309\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 60: reward=0.707, intervention_rate=1.000, scm=fork_6var, F1=0.000, P(Parents)=0.000, SHD=5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 490):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-0.0535024  -0.06410649 -0.03737555 -0.00797103 -0.01287557]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.012875565947893509\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.05\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.19648416 0.19450396 0.19953438 0.20521834 0.20425915]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 4)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4069, Std: 2.4335\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.18\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.9574\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 490):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (4, -1.9573800023147265)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': -1.9573800023147265}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.19573800023147267}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.695738\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.695738\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.698, trend=+0.131\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000283685047\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 500):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-0.08896812  0.02704458  0.01875446 -0.05391663 -0.012166  ]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.012165996168037672\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.03\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.18722841 0.20952152 0.20784395 0.19370156 0.20170457]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3867, Std: 2.3288\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.18\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.9584\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 500):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, 0.9583889803680471)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 0.9583889803680471}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.09583889803680472}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.595839\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.595839\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.701, trend=-0.083\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000281894139\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 510):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-0.07189887  0.00921461 -0.05833383 -0.1567206   0.00754234]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: 0.007542336839196059\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.02\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.19616218 0.21247139 0.19879977 0.18044482 0.21212184]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 4)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3394, Std: 2.3166\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.17\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.8988\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 510):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (4, 0.8988193521137695)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 0.8988193521137695}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.08988193521137695}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.589882\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.589882\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.692, trend=-0.053\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000270094832\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 520):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-0.02911994  0.00646307 -0.04864961 -0.04908822 -0.00612753]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.00612752626422113\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.00\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.19918843 0.20640376 0.19533608 0.19525042 0.20382131]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3790, Std: 2.4344\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.17\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.5209\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 520):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, 2.5209146986307998)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 2.5209146986307998}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.25209146986308}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.752091\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.752091\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.703, trend=+0.192\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000265596021\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 65):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=4.859301\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.738\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00026560\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00026560\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.502, max=0.986, group_baseline=0.738\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.337554, entropy=-4.107301\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.10598192\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 4.159537\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000260195782\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 530):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  4.91129887e-03 -2.43766417e-02 -5.45599315e-02\n",
      "  1.26868180e-01]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X5' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.97\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.         0.24722888 0.23986633 0.23250806 0.28039673]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3695, Std: 2.6047\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.16\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.3783\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 530):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, 0.3783200379918996)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': 0.3783200379918996}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.03783200379918997}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.537832\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.537832\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.701, trend=-0.205\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000296317601\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 540):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -7.10407511e-02  6.27531571e-02  2.63597705e-02\n",
      " -1.66236238e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X5' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.95\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.         0.23163948 0.26654848 0.25656266 0.24524938]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 4)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4120, Std: 2.5357\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.15\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.3158\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 540):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (4, 2.3158317679857006)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 2.3158317679857006}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.23158317679857007}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.731583\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.731583\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.700, trend=-0.268\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000341169364\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 550):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -7.84866895e-03  2.24475406e-02 -1.29673411e-02\n",
      " -5.13296290e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X5' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.94\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.         0.25112516 0.25937306 0.24975777 0.23974402]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3609, Std: 2.4775\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.15\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 1.4569\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 550):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 1.4568583317860493)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 1.4568583317860493}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.14568583317860492}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.645686\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.645686\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.664, trend=-0.097\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000374206502\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 560):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  1.87455434e-02  3.84304957e-02 -2.92561695e-03\n",
      " -1.87442648e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X5' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.92\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.         0.25262106 0.25807333 0.24675178 0.24255383]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3822, Std: 2.6448\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.14\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.9733\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 560):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -1.9733069871023292)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': -1.9733069871023292}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X5\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.19733069871023293}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.697331\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.697331\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.660, trend=+0.072\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000386310550\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 70):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=6.633504\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.705\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00038631\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00038631\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.541, max=1.000, group_baseline=0.705\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.325760, entropy=-4.213898\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.08453669\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 125000004.152672\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000388029177\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 70: reward=0.704, intervention_rate=1.000, scm=collider_6var, F1=0.000, P(Parents)=0.000, SHD=3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 570):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 0.03826666 -0.01996124  0.03752415  0.0588916  -0.00425969]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.004259693895089874\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.89\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.20355486 0.19067242 0.20338523 0.20832375 0.19406374]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X4 (index 2)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3942, Std: 2.8422\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.13\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -3.0973\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 570):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (2, -3.0973474856478327)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X4'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X4': -3.0973474856478327}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.3097347485647833}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.809735\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.809735\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.810) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.678, trend=+0.085\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000379231970\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 580):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 0.02141468 -0.01910151 -0.00351168  0.0616718  -0.00242143]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.002421429621981784\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.88\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.2021478  0.19300091 0.19647041 0.21166551 0.19671537]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X5 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4028, Std: 2.5724\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.12\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.4103\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 580):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -1.4103127280918846)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X5'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X5': -1.4103127280918846}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.14103127280918845}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.641031\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.641031\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.670, trend=-0.055\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000351173928\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 590):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 0.05319311 -0.00694495 -0.04003556 -0.0188161  -0.01028397]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.010283968229835268\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.86\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.21376536 0.19931772 0.19178879 0.19658334 0.19854479]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4107, Std: 3.0838\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.12\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -4.1270\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 590):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, -4.1269691162944575)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -4.1269691162944575}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.4126969116294458}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.912697\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.912697\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.913) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.691, trend=+0.317\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000306926273\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 600):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 0.05026915  0.05515161  0.01528443 -0.01338251 -0.04674315]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X3' at index 5, logit: -0.04674314834503383\n",
      "WARNING:causal_bayes_opt.training.enriched_trainer:‚ö†Ô∏è Target variable not properly masked!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.84\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.20903402 0.21024713 0.20054399 0.19384483 0.18633003]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X5 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4009, Std: 2.5346\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.11\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 2.8492\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 600):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 2.8492188677309174)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X5'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X5': 2.8492188677309174}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X3\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.28492188677309177}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.784922\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.784922\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.710, trend=+0.195\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000276960340\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 75):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=6.584808\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.835\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00027696\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00027696\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.513, max=1.000, group_baseline=0.835\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.365439, entropy=-4.325664\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.10282100\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 250000004.476366\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000268615274\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 610):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 8.23947878e-03  4.41912523e-02 -1.00000000e+09  2.87924857e-02\n",
      "  4.67372708e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.81\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.24275481 0.25373749 0.         0.24897386 0.25453384]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 4)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4508, Std: 3.2445\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.10\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.4642\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 610):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (4, 5.464241903834713)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 5.464241903834713}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.751, trend=+0.248\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000330212933\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 620):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.92168890e-02  3.52115083e-02 -1.00000000e+09 -1.07698636e-02\n",
      "  1.81152947e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.80\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.2421713  0.25929013 0.         0.24475202 0.25378655]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 3)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4867, Std: 3.4426\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.10\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -4.3257\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 620):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (3, -4.325706009393333)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -4.325706009393333}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.4325706009393333}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.932571\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.932571\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.933) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.769, trend=+0.395\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000364647675\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 630):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-6.01646996e-02  7.28977461e-02 -1.00000000e+09  1.26725916e-02\n",
      " -3.69375926e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.78\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.23182615 0.27487258 0.         0.25447929 0.23882198]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4223, Std: 3.3230\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.09\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 4.0379\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 630):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 4.037921074427295)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 4.037921074427295}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.40379210744272953}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.903792\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.903792\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.904) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.806, trend=+0.172\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000401447032\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 640):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 3.39678694e-02 -2.82286309e-02 -1.00000000e+09  1.92906052e-02\n",
      " -8.40112193e-03]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X5', 'X1', 'X4', 'X2', 'X0', 'X3'], Target: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X4' at index 2, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.77\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0.25979746 0.23952698 0.         0.25486449 0.24581107]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X0 (index 4)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4013, Std: 3.2049\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.09\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 0.9024\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 640):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (4, 0.9023935686804841)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X0'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X0': 0.9023935686804841}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X4\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.09023935686804842}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.590239\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.590239\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.792, trend=-0.055\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000457352505\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 80):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=5.104180\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.699\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00045735\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00045735\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.513, max=1.000, group_baseline=0.699\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.376627, entropy=-4.447212\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.07392521\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 375000004.171986\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000375125792\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 80: reward=0.653, intervention_rate=1.000, scm=fork_3var, F1=0.000, P(Parents)=0.000, SHD=2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 650):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  2.04226821e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.73\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4009, Std: 3.2250\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.08\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 4.4016\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 650):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 4.401587445638127)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 4.401587445638127}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.44015874456381276}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.940159\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.940159\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.940) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.821, trend=+0.243\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000296882856\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 660):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  6.49484109e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.72\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4666, Std: 3.7466\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.07\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.2327\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 660):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 5.232706821476454)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 5.232706821476454}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.852, trend=+0.190\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000270187070\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 670):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -4.44727602e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.70\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4961, Std: 3.8165\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.07\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -4.0154\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 670):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -4.01543956515106)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -4.01543956515106}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.401543956515106}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.901544\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.901544\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (0.902) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.861, trend=+0.261\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000277071376\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 680):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  6.20589795e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.69\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4797, Std: 3.7234\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.06\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.5756\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 680):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -0.5755889110532704)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -0.5755889110532704}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.05755889110532704}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.557559\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.557559\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.852, trend=-0.355\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000319452251\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 85):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=7.367138\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.720\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00031945\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00031945\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.504, max=1.000, group_baseline=0.720\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.369715, entropy=-3.653348\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.09674156\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.422871\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000361502939\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 690):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-6.02820138e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.66\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3294, Std: 2.9887\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.05\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 5.4826\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 690):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 5.482552944865585)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 5.482552944865585}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.861, trend=+0.215\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000388467062\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 700):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [ 5.32081981e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.64\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.5149, Std: 4.2857\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.05\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 9.4475\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 700):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 9.44746738013427)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 9.44746738013427}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.883, trend=+0.000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000393810128\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 710):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-4.00284684e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.62\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.5291, Std: 4.2586\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.04\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 6.6910\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 710):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, 6.691002333744784)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': 6.691002333744784}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.883, trend=+0.067\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000384381411\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 720):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.52349242e-02 -1.00000000e+09]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X2' at index 1, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.61\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [1. 0.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X1 (index 0)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.3402, Std: 3.3356\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.04\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.5906\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 720):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (0, -1.5906321708076252)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X1'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X1': -1.5906321708076252}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.15906321708076254}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.659063\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.659063\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.855, trend=-0.245\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000372231129\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 90):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=4.595602\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.706\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00037223\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00037223\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.568, max=0.960, group_baseline=0.706\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.316425, entropy=-3.790192\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.07545102\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 1000000003.497120\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000367177990\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Episode 90: reward=0.759, intervention_rate=1.000, scm=collider_3var, F1=0.000, P(Parents)=0.000, SHD=2\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 730):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  2.28212838e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.58\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4895, Std: 4.4036\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.03\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -1.4960\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 730):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -1.4960108554819798)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -1.4960108554819798}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.14960108554819798}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.649601\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.649601\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.830, trend=+0.059\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000392234173\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 740):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  3.70428837e-03]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.56\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4219, Std: 3.6539\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.02\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -7.1659\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 740):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -7.165917081785399)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -7.165917081785399}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.871, trend=+0.060\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000415674802\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 750):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09  9.48652858e-03]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.55\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.4733, Std: 4.2984\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.02\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: -0.9835\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 750):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, -0.983543990452588)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': -0.983543990452588}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.0983543990452588}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 0.598354\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 0.598354\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.837, trend=-0.402\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000435529805\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 760):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -4.25683635e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 0.53\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.5404, Std: 5.1512\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.01\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 8.0713\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç PHASE 4 REWARD ANALYSIS (computation 760):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action: (1, 8.071314717908331)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention targets: {'X2'}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Intervention values: {'X2': 8.071314717908331}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Reward components: {'exploration_bonus': 0.2, 'valid_intervention_bonus': 0.3, 'magnitude_bonus': 0.5}\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Total reward before clipping: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Final reward: 1.000000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  ‚úÖ INCENTIVE CHECK: Correctly rewarding intervention on non-target variables\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  üí∞ HIGH REWARD: Policy achieved high reward (1.000) - good performance!\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  REWARD TREND: mean=0.837, trend=+0.098\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000487222711\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Policy Learning Diagnostics (update 95):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Action magnitudes: max=8.071315\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Mean reward: 0.788\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Policy param change: 0.00048722\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Parameter norm change: 0.00048722\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Rewards: min=0.543, max=1.000, group_baseline=0.788\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  GRPO losses: policy=0.346244, entropy=-3.933005\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Gradient norm: 0.07969119\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Learning rate: 0.001000\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  KL penalty: 0.000000, approx_kl: 3.697166\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:‚úÖ Parameters changed - norm delta: 0.000533982229\n",
      "INFO:causal_bayes_opt.training.modular_trainer:Saved checkpoint: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/enriched_grpo_final/checkpoint.pkl\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Training completed in 7538.5s\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:Final checkpoint: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/enriched_grpo_final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training completed!\n",
      "‚è±Ô∏è Training time: 125.7 minutes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train GRPO Policy with 119x Improvement System\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Starting GRPO Policy Training\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üîß Training mode: {TRAINING_MODE}\")\n",
    "print(f\"üéØ Training objective: {TRAINING_OBJECTIVE}\")\n",
    "print(f\"üìä Total episodes: {total_episodes}\")\n",
    "print(f\"‚öñÔ∏è Reward weights: {objective_config['reward_weights']}\")\n",
    "print(f\"‚úÖ 119x surrogate integration: ACTIVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create training configuration\n",
    "def create_grpo_training_config():\n",
    "    \"\"\"Create comprehensive GRPO training configuration.\"\"\"\n",
    "    config_dict = {\n",
    "        'seed': RANDOM_SEED,\n",
    "        \n",
    "        'training': {\n",
    "            'n_episodes': total_episodes,\n",
    "            'episode_length': train_config['episode_length'],\n",
    "            'learning_rate': train_config['learning_rate'],\n",
    "            'gamma': 0.99,\n",
    "            'max_intervention_value': 2.0,\n",
    "            \n",
    "            # Use dynamic reward weights based on objective\n",
    "            'reward_weights': objective_config['reward_weights'],\n",
    "            \n",
    "            # Architecture\n",
    "            'architecture': {\n",
    "                'hidden_dim': 128,\n",
    "                'num_layers': 2,\n",
    "                'num_heads': 4,\n",
    "                'key_size': 32,\n",
    "                'widening_factor': 4,\n",
    "                'dropout': 0.1,\n",
    "                'policy_intermediate_dim': None\n",
    "            },\n",
    "            \n",
    "            # State configuration\n",
    "            'state_config': {\n",
    "                'max_history_size': 100,\n",
    "                'num_channels': 5,  # Per-variable encoding\n",
    "                'standardize_values': True,\n",
    "                'include_temporal_features': True\n",
    "            },\n",
    "            \n",
    "            # GRPO configuration\n",
    "            'grpo_config': {\n",
    "                'group_size': 64,\n",
    "                'interventions_per_state': 8,\n",
    "                'clip_ratio': 0.2,\n",
    "                'entropy_coeff': 0.01,\n",
    "                'kl_penalty_coeff': 0.0,\n",
    "                'max_grad_norm': 1.0,\n",
    "                'scale_rewards': True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # 119x Improvement System\n",
    "        'surrogate_integration': {\n",
    "            'enabled': True,\n",
    "            'phase_config': {\n",
    "                'bootstrap_steps': PRODUCTION_PHASE_CONFIG.bootstrap_steps,\n",
    "                'transition_steps': PRODUCTION_PHASE_CONFIG.transition_steps,\n",
    "                'exploration_noise_start': PRODUCTION_PHASE_CONFIG.exploration_noise_start,\n",
    "                'exploration_noise_end': PRODUCTION_PHASE_CONFIG.exploration_noise_end,\n",
    "                'transition_schedule': PRODUCTION_PHASE_CONFIG.transition_schedule\n",
    "            },\n",
    "            'bootstrap_config': {\n",
    "                'structure_encoding_dim': PRODUCTION_BOOTSTRAP_CONFIG.structure_encoding_dim,\n",
    "                'use_graph_distance': PRODUCTION_BOOTSTRAP_CONFIG.use_graph_distance,\n",
    "                'use_structural_priors': PRODUCTION_BOOTSTRAP_CONFIG.use_structural_priors,\n",
    "                'noise_schedule': PRODUCTION_BOOTSTRAP_CONFIG.noise_schedule,\n",
    "                'min_noise_factor': PRODUCTION_BOOTSTRAP_CONFIG.min_noise_factor\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Experiment configuration\n",
    "        'experiment': {\n",
    "            'scm_generation': {\n",
    "                'use_variable_factory': True,\n",
    "                'variable_range': scm_config['variable_range'],\n",
    "                'structure_types': scm_config['structure_types'],\n",
    "                'rotation_frequency': 5,\n",
    "                'fallback_scms': ['fork_3var', 'chain_3var', 'collider_3var'],\n",
    "                'num_scms': len(training_scms),\n",
    "                'edge_density_range': scm_config['edge_density_range']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Logging\n",
    "        'logging': {\n",
    "            'checkpoint_dir': str(checkpoint_dir),\n",
    "            'wandb': {'enabled': False},\n",
    "            'level': 'INFO',\n",
    "            'save_frequency': 50\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return OmegaConf.create(config_dict)\n",
    "\n",
    "# Create configuration\n",
    "grpo_config = create_grpo_training_config()\n",
    "\n",
    "# Initialize trainer\n",
    "training_start_time = time.time()\n",
    "trainer = EnrichedGRPOTrainer(config=grpo_config)\n",
    "\n",
    "# Load checkpoint if fine-tuning\n",
    "if FINETUNE_FROM_CHECKPOINT:\n",
    "    print(f\"\\nüì• Loading checkpoint for fine-tuning: {FINETUNE_FROM_CHECKPOINT}\")\n",
    "    # TODO: Implement checkpoint loading in trainer\n",
    "    print(\"‚ö†Ô∏è Fine-tuning not yet implemented - training from scratch\")\n",
    "\n",
    "# Train\n",
    "print(\"\\nüèÉ Starting Training Loop...\")\n",
    "training_metrics = trainer.train()\n",
    "\n",
    "training_end_time = time.time()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"‚è±Ô∏è Training time: {training_duration/60:.1f} minutes\")\n",
    "\n",
    "# Extract results\n",
    "performance = training_metrics.get('performance', {})\n",
    "checkpoint_path = training_metrics.get('checkpoint_path', checkpoint_dir / \"grpo_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Checkpoint with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Checkpoint with Metadata\n",
      "==================================================\n",
      "‚úÖ Checkpoint saved: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_target_focused_20250721_234828\n",
      "üìã Metadata saved: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_target_focused_20250721_234828_metadata.json\n",
      "\n",
      "üìä Training Summary:\n",
      "  Mode: QUICK\n",
      "  Objective: TARGET_FOCUSED\n",
      "  Duration: 125.7 minutes\n",
      "  Episodes: 96\n",
      "  Reward weights: {'optimization': 0.8, 'discovery': 0.1, 'efficiency': 0.1}\n",
      "  Final reward: 0.8000869074181126\n",
      "  Checkpoint: grpo_quick_target_focused_20250721_234828\n",
      "\n",
      "üéØ Training objective: Prioritizes target variable maximization above all else\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save Checkpoint with Training Metadata\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíæ Saving Checkpoint with Metadata\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate checkpoint name with timestamp and objective\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_name = f\"grpo_{TRAINING_MODE.lower()}_{TRAINING_OBJECTIVE.lower()}_{timestamp}\"\n",
    "final_checkpoint_dir = checkpoint_dir / checkpoint_name\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'training_config': {\n",
    "        'mode': TRAINING_MODE,\n",
    "        'objective': TRAINING_OBJECTIVE,\n",
    "        'config': train_config,\n",
    "        'objective_config': objective_config,\n",
    "        'total_episodes': total_episodes,\n",
    "        'learning_rate': grpo_config.training.learning_rate,\n",
    "        'architecture': OmegaConf.to_container(grpo_config.training.architecture),\n",
    "        'reward_weights': objective_config['reward_weights']\n",
    "    },\n",
    "    'scm_config': scm_config,\n",
    "    'training_results': {\n",
    "        'duration_minutes': training_duration / 60,\n",
    "        'final_performance': performance,\n",
    "        'timestamp': timestamp,\n",
    "        'success': True\n",
    "    },\n",
    "    'environment': {\n",
    "        'jax_backend': jax.default_backend(),\n",
    "        'num_devices': len(jax.devices()),\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'surrogate_config': {\n",
    "        'phase_config': OmegaConf.to_container(grpo_config.surrogate_integration.phase_config),\n",
    "        'bootstrap_config': OmegaConf.to_container(grpo_config.surrogate_integration.bootstrap_config)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = final_checkpoint_dir.parent / f\"{checkpoint_name}_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Checkpoint saved: {final_checkpoint_dir}\")\n",
    "print(f\"üìã Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"  Mode: {TRAINING_MODE}\")\n",
    "print(f\"  Objective: {TRAINING_OBJECTIVE}\")\n",
    "print(f\"  Duration: {training_duration/60:.1f} minutes\")\n",
    "print(f\"  Episodes: {total_episodes}\")\n",
    "print(f\"  Reward weights: {objective_config['reward_weights']}\")\n",
    "print(f\"  Final reward: {performance.get('final_reward', 'N/A')}\")\n",
    "print(f\"  Checkpoint: {checkpoint_name}\")\n",
    "print(f\"\\nüéØ Training objective: {objective_config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:causal_bayes_opt.training.enriched_trainer:üîç Per-Variable Encoding - Policy Output (call 770):\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable logits: [-1.00000000e+09 -7.36612699e-02]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variables: ['X1', 'X2', 'X0'], Target: X1\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Target variable 'X1' at index 0, logit: -1000000000.0\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Variable selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.00\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Probabilities: [0. 1.]\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Selected: X2 (index 1)\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:  Value selection:\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Mean: 0.5714, Std: 5.4958\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Temperature: 1.00\n",
      "INFO:causal_bayes_opt.training.enriched_trainer:    Sampled value: 7.8151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Quick Policy Validation\n",
      "==================================================\n",
      "\n",
      "1. fork (3v):\n",
      "   Target: X1\n",
      "   Action: X2=8.762\n",
      "   Magnitude: 8.762\n",
      "\n",
      "2. fork (3v):\n",
      "   Target: X1\n",
      "   Action: X2=7.815\n",
      "   Magnitude: 7.815\n",
      "\n",
      "3. fork (4v):\n",
      "   Target: X2\n",
      "   Action: X3=-4.574\n",
      "   Magnitude: 4.574\n",
      "\n",
      "‚úÖ Policy is functioning correctly!\n",
      "\n",
      "üéâ Training Complete! Use checkpoint 'grpo_quick_target_focused_20250721_234828' in evaluation notebook.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick Validation of Trained Policy\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß™ Quick Policy Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test on a few SCMs\n",
    "test_key = random.PRNGKey(999)\n",
    "n_test = min(3, len(training_scms))\n",
    "\n",
    "for i in range(n_test):\n",
    "    test_scm = training_scms[i]\n",
    "    test_meta = scm_metadata[i]\n",
    "    \n",
    "    # Create test state\n",
    "    test_state = trainer._create_tensor_backed_state(test_scm, 0, 0.0)\n",
    "    enriched_input = trainer.state_converter.convert_state_to_enriched_input(test_state)\n",
    "    \n",
    "    # Get policy output\n",
    "    test_key, subkey = random.split(test_key)\n",
    "    target_idx = test_meta['variables'].index(test_meta['target'])\n",
    "    \n",
    "    policy_output = trainer.policy_fn.apply(\n",
    "        trainer.policy_params, subkey, enriched_input, target_idx, False\n",
    "    )\n",
    "    \n",
    "    # Convert to action\n",
    "    action = trainer._policy_output_to_action(policy_output, test_meta['variables'], test_meta['target'])\n",
    "    selected_var_idx, intervention_value = action\n",
    "    \n",
    "    print(f\"\\n{i+1}. {test_meta['structure_type']} ({test_meta['n_variables']}v):\")\n",
    "    print(f\"   Target: {test_meta['target']}\")\n",
    "    print(f\"   Action: {test_meta['variables'][selected_var_idx]}={intervention_value:.3f}\")\n",
    "    print(f\"   Magnitude: {abs(intervention_value):.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Policy is functioning correctly!\")\n",
    "print(f\"\\nüéâ Training Complete! Use checkpoint '{checkpoint_name}' in evaluation notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-bayes-opt-sr_Vb8Og-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
