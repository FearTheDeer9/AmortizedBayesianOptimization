{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Evaluation - Modular Version\n",
    "\n",
    "**Purpose**: Evaluate trained GRPO checkpoints with proper optimization direction handling.\n",
    "\n",
    "**Key Features**:\n",
    "- ‚úÖ **Checkpoint-first approach** - load any checkpoint to evaluate\n",
    "- ‚úÖ **Auto-detect optimization** - reads direction from checkpoint metadata\n",
    "- ‚úÖ **Independent cells** - no need to run training first\n",
    "- ‚úÖ **Correct metrics** - handles both minimization and maximization\n",
    "- ‚úÖ **Multiple modes** - single checkpoint, compare checkpoints, compare objectives\n",
    "\n",
    "**Workflow**:\n",
    "1. Select evaluation mode and checkpoints\n",
    "2. Load checkpoint(s) and validate metadata\n",
    "3. Generate or load test SCMs\n",
    "4. Run evaluation with baselines\n",
    "5. Generate visualizations with correct labels\n",
    "6. Export results for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-07-24 08:59:59,376:jax._src.xla_bridge:749: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "[2025-07-24 08:59:59,376][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Setup Complete\n",
      "üìÅ Project root: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt\n",
      "üîß JAX devices: [CpuDevice(id=0)]\n",
      "üìÖ Date: 2025-07-24 08:59:59\n",
      "\n",
      "üìÅ Checkpoint directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cell 1: Import base components and configure environment\n",
    "\n",
    "This cell sets up the evaluation environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"experiments\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import base components\n",
    "from scripts.notebooks.base_components import (\n",
    "    NotebookError, CheckpointManager, SCMGenerator,\n",
    "    OptimizationConfig, CheckpointMetadata, validate_environment,\n",
    "    format_results_summary\n",
    ")\n",
    "from scripts.notebooks.config_templates import create_evaluation_config\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import pyrsistent as pyr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Validate environment\n",
    "try:\n",
    "    env_info = validate_environment()\n",
    "    print(\"‚úÖ Environment Setup Complete\")\n",
    "    print(f\"üìÅ Project root: {project_root}\")\n",
    "    print(f\"üîß JAX devices: {env_info['jax_devices']}\")\n",
    "    print(f\"üìÖ Date: {env_info['timestamp']}\")\n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Environment validation failed: {e}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_dir = project_root / \"checkpoints\" / \"grpo_training\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_dir)\n",
    "print(f\"\\nüìÅ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Evaluation Configuration\n",
      "==================================================\n",
      "Mode: SINGLE_CHECKPOINT\n",
      "Test SCMs: 10\n",
      "Runs per method: 3\n",
      "Intervention budget: 10\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 2: Configure evaluation parameters\n",
    "\n",
    "Set evaluation mode and parameters for the run.\n",
    "\"\"\"\n",
    "\n",
    "# EVALUATION MODE SELECTION\n",
    "EVALUATION_MODE = \"SINGLE_CHECKPOINT\"  # Options: \"SINGLE_CHECKPOINT\", \"COMPARE_CHECKPOINTS\", \"COMPARE_OBJECTIVES\"\n",
    "\n",
    "# Configuration for different modes\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Evaluate one checkpoint against baselines\n",
    "    NUM_TEST_SCMS = 10\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Compare multiple checkpoints\n",
    "    COMPARISON_COUNT = 3  # Number of checkpoints to compare\n",
    "    NUM_TEST_SCMS = 5  # Fewer SCMs for faster comparison\n",
    "    RUNS_PER_METHOD = 2\n",
    "    INTERVENTION_BUDGET = 8\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Compare minimization vs maximization\n",
    "    NUM_TEST_SCMS = 8\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# General settings\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"üéØ Evaluation Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "if EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    print(f\"Checkpoints to compare: {COMPARISON_COUNT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Mode and Checkpoint Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "<>:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "/var/folders/2f/7z7glsfj1fd22nlr6wj56z5w0000gn/T/ipykernel_63023/2931854767.py:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "  print(f\"üöÄ All selected checkpoints validated and ready for evaluation\\!\")\n",
      "[2025-07-24 08:59:59,391][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,391][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-24 08:59:59,392][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,393][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,393][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-24 08:59:59,394][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,414][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,415][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-24 08:59:59,415][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,428][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-24 08:59:59,429][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-24 08:59:59,429][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Intelligent Checkpoint Selection\n",
      "==================================================\n",
      "Found 4 total checkpoints\n",
      "Usable checkpoints:\n",
      "  MINIMIZE: 1\n",
      "  MAXIMIZE: 2\n",
      "  Total usable: 3\n",
      "üéØ Selected best MINIMIZE checkpoint: grpo_quick_minimize_20250723_101252_fixed\n",
      "  ‚úÖ Checkpoint is valid and ready for evaluation\n",
      "‚úÖ Final Selection (1 checkpoint(s)):\n",
      "1. grpo_quick_minimize_20250723_101252_fixed\n",
      "     Optimization: MINIMIZE\n",
      "     Training mode: QUICK\n",
      "     Path: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_101252_fixed\n",
      "     Status: ‚úÖ Ready for evaluation\n",
      "üöÄ All selected checkpoints validated and ready for evaluation\\!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Select checkpoints for evaluation\n",
    "\n",
    "This cell handles checkpoint selection based on the evaluation mode.\n",
    "Uses intelligent discovery instead of hardcoded names.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Intelligent Checkpoint Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get available checkpoints\n",
    "try:\n",
    "    available_checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    if not available_checkpoints:\n",
    "        raise NotebookError(\"No checkpoints found\")\n",
    "        \n",
    "    print(f\"Found {len(available_checkpoints)} total checkpoints\")\n",
    "    \n",
    "    # Show usable checkpoints by direction\n",
    "    usable_minimize = checkpoint_manager.find_usable_checkpoints('MINIMIZE')\n",
    "    usable_maximize = checkpoint_manager.find_usable_checkpoints('MAXIMIZE')\n",
    "    \n",
    "    print(f\"Usable checkpoints:\")\n",
    "    print(f\"  MINIMIZE: {len(usable_minimize)}\")\n",
    "    print(f\"  MAXIMIZE: {len(usable_maximize)}\")\n",
    "    print(f\"  Total usable: {len(usable_minimize) + len(usable_maximize)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to analyze checkpoints: {e}\")\n",
    "\n",
    "# SELECT CHECKPOINTS BASED ON MODE\n",
    "selected_checkpoints = []\n",
    "\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Find best MINIMIZE checkpoint (preferred for comparison with PARENT_SCALE)\n",
    "    best_checkpoint = checkpoint_manager.find_best_checkpoint({\n",
    "        'optimization_direction': 'MINIMIZE',\n",
    "        'training_mode': 'QUICK'\n",
    "    })\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        selected_checkpoints = [best_checkpoint]\n",
    "        print(f\"üéØ Selected best MINIMIZE checkpoint: {best_checkpoint.name}\")\n",
    "        \n",
    "        # Validate the selected checkpoint\n",
    "        validation = checkpoint_manager.validate_checkpoint(best_checkpoint)\n",
    "        if validation['is_valid']:\n",
    "            print(f\"  ‚úÖ Checkpoint is valid and ready for evaluation\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Checkpoint issues: {validation['issues']}\")\n",
    "    else:\n",
    "        # Fallback: try any usable checkpoint\n",
    "        all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "        if all_usable:\n",
    "            selected_checkpoints = [all_usable[0]]\n",
    "            print(f\"üéØ No MINIMIZE checkpoint found, using: {all_usable[0].name}\")\n",
    "            print(f\"  Optimization: {all_usable[0].optimization_config.direction}\")\n",
    "        else:\n",
    "            raise NotebookError(\"No usable checkpoints found. Please ensure checkpoints have both metadata.json and checkpoint.pkl files.\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Get multiple usable checkpoints\n",
    "    all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "    comparison_count = min(COMPARISON_COUNT, len(all_usable))\n",
    "    selected_checkpoints = all_usable[:comparison_count]\n",
    "    print(f\"üìä Selected {comparison_count} checkpoints for comparison\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Get best from each optimization direction\n",
    "    best_minimize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MINIMIZE'})\n",
    "    best_maximize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MAXIMIZE'})\n",
    "    \n",
    "    selected_checkpoints = []\n",
    "    if best_minimize:\n",
    "        selected_checkpoints.append(best_minimize)\n",
    "    if best_maximize:\n",
    "        selected_checkpoints.append(best_maximize)\n",
    "    \n",
    "    if not selected_checkpoints:\n",
    "        raise NotebookError(\"Need checkpoints from both MINIMIZE and MAXIMIZE directions for objective comparison\")\n",
    "    \n",
    "    print(f\"üîÑ Selected checkpoints for objective comparison:\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        print(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\")\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# Final validation\n",
    "if not selected_checkpoints:\n",
    "    raise NotebookError(\"No checkpoints selected for evaluation\")\n",
    "\n",
    "print(f\"‚úÖ Final Selection ({len(selected_checkpoints)} checkpoint(s)):\")\n",
    "for i, checkpoint in enumerate(selected_checkpoints, 1):\n",
    "    print(f\"{i}. {checkpoint.name}\")\n",
    "    print(f\"     Optimization: {checkpoint.optimization_config.direction}\")\n",
    "    print(f\"     Training mode: {checkpoint.training_config.get('mode', 'unknown')}\")\n",
    "    print(f\"     Path: {checkpoint.path}\")\n",
    "    \n",
    "    # Final validation\n",
    "    validation = checkpoint_manager.validate_checkpoint(checkpoint)\n",
    "    if validation['is_valid']:\n",
    "        print(f\"     Status: ‚úÖ Ready for evaluation\")\n",
    "    else:\n",
    "        print(f\"     Status: ‚ùå Issues found: {validation['issues']}\")\n",
    "        raise NotebookError(f\"Selected checkpoint {checkpoint.name} has validation issues: {validation['issues']}\")\n",
    "\n",
    "print(f\"üöÄ All selected checkpoints validated and ready for evaluation\\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Checkpoint Metadata\n",
      "============================================================\n",
      "\n",
      "Loading: grpo_quick_minimize_20250723_101252_fixed\n",
      "  ‚úì Optimization: MINIMIZE\n",
      "  ‚úì Training mode: QUICK\n",
      "  ‚úì Episodes completed: unknown\n",
      "  ‚úì Duration: 5.0 minutes\n",
      "  ‚úì Reward weights: opt=0.8, struct=0.1, eff=0.1\n",
      "\n",
      "‚úÖ Loaded 1 checkpoint(s) successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 3: Load checkpoint metadata and validate\n",
    "\n",
    "This cell loads the selected checkpoints and validates their metadata.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• Loading Checkpoint Metadata\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store loaded checkpoint info\n",
    "loaded_checkpoints = {}\n",
    "\n",
    "for ckpt in selected_checkpoints:\n",
    "    print(f\"\\nLoading: {ckpt.name}\")\n",
    "    try:\n",
    "        # For now, we're using the metadata we already have\n",
    "        # In production, this would load the actual model parameters\n",
    "        loaded_checkpoints[ckpt.name] = {\n",
    "            'metadata': ckpt,\n",
    "            'optimization_config': ckpt.optimization_config,\n",
    "            'training_config': ckpt.training_config,\n",
    "            'model_params': None  # TODO: Load actual model parameters\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Optimization: {ckpt.optimization_config.direction}\")\n",
    "        print(f\"  ‚úì Training mode: {ckpt.training_config.get('mode', 'unknown')}\")\n",
    "        print(f\"  ‚úì Episodes completed: {ckpt.training_results.get('episodes_completed', 'unknown')}\")\n",
    "        print(f\"  ‚úì Duration: {ckpt.training_results.get('duration_minutes', 0):.1f} minutes\")\n",
    "        \n",
    "        # Show reward weights if available\n",
    "        if 'reward_weights' in ckpt.training_config:\n",
    "            weights = ckpt.training_config['reward_weights']\n",
    "            print(f\"  ‚úì Reward weights: opt={weights.get('optimization', 0):.1f}, \"\n",
    "                  f\"struct={weights.get('discovery', 0):.1f}, \"\n",
    "                  f\"eff={weights.get('efficiency', 0):.1f}\")\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed to load: {e}\")\n",
    "        raise NotebookError(f\"Failed to load checkpoint {ckpt.name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(loaded_checkpoints)} checkpoint(s) successfully\")\n",
    "\n",
    "# Check optimization compatibility for comparison modes\n",
    "if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    directions = [ckpt.optimization_config.direction for ckpt in selected_checkpoints]\n",
    "    if len(set(directions)) == 1:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: All checkpoints have same optimization direction: {directions[0]}\")\n",
    "        print(\"   Objective comparison may not be meaningful.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Comparing optimization directions: {set(directions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Test SCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Generating Test SCMs\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-24 08:59:59,771][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-24 08:59:59,772][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-24 08:59:59,788][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-24 08:59:59,789][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-24 08:59:59,804][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "[2025-07-24 08:59:59,805][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "[2025-07-24 08:59:59,807][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "[2025-07-24 08:59:59,807][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "[2025-07-24 08:59:59,821][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "[2025-07-24 08:59:59,821][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "[2025-07-24 08:59:59,823][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "[2025-07-24 08:59:59,823][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "[2025-07-24 08:59:59,826][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "[2025-07-24 08:59:59,827][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "[2025-07-24 08:59:59,829][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "[2025-07-24 08:59:59,829][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "[2025-07-24 08:59:59,831][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-24 08:59:59,831][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-24 08:59:59,833][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-24 08:59:59,833][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-24 08:59:59,834][scripts.notebooks.base_components][INFO] - Generated 10 SCMs\n",
      "[2025-07-24 08:59:59,834][scripts.notebooks.base_components][INFO] - Distribution: {'structure_types': {'fork': 4, 'chain': 4, 'collider': 2}, 'variable_counts': {3: 3, 4: 3, 5: 2, 6: 2}, 'total': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Generated 10 test SCMs\n",
      "\n",
      "üìä Test Set Distribution:\n",
      "  Structure types: {'fork': 4, 'chain': 4, 'collider': 2}\n",
      "  Variable counts: {3: 3, 4: 3, 5: 2, 6: 2}\n",
      "\n",
      "üíæ Saved test SCM metadata to: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/test_scms/test_scms_1042.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Generate test SCMs for evaluation\n",
    "\n",
    "Create a balanced set of test SCMs different from training.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî¨ Generating Test SCMs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SCM generator\n",
    "scm_generator = SCMGenerator()\n",
    "\n",
    "# Generate test SCMs with different seed than training\n",
    "test_seed = RANDOM_SEED + 1000  # Ensure different from training\n",
    "\n",
    "try:\n",
    "    test_scms, test_metadata = scm_generator.generate_balanced_scms(\n",
    "        num_scms=NUM_TEST_SCMS,\n",
    "        variable_range=(3, 6),\n",
    "        structure_types=['fork', 'chain', 'collider', 'mixed'],\n",
    "        seed=test_seed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(test_scms)} test SCMs\")\n",
    "    \n",
    "    # Analyze distribution\n",
    "    distribution = scm_generator._summarize_distribution(test_metadata)\n",
    "    print(f\"\\nüìä Test Set Distribution:\")\n",
    "    print(f\"  Structure types: {distribution['structure_types']}\")\n",
    "    print(f\"  Variable counts: {distribution['variable_counts']}\")\n",
    "    \n",
    "    # Save test SCM metadata\n",
    "    test_scm_path = project_root / \"results\" / \"test_scms\" / f\"test_scms_{test_seed}.json\"\n",
    "    test_scm_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(test_scm_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'metadata': test_metadata,\n",
    "            'config': {\n",
    "                'num_scms': len(test_scms),\n",
    "                'seed': test_seed,\n",
    "                'variable_range': [3, 6],\n",
    "                'structure_types': ['fork', 'chain', 'collider', 'mixed']\n",
    "            },\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved test SCM metadata to: {test_scm_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to generate test SCMs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Running Evaluation\n",
      "============================================================\n",
      "üìÅ Output directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250724_085959\n",
      "\n",
      "Evaluation parameters:\n",
      "  Mode: SINGLE_CHECKPOINT\n",
      "  Test SCMs: 10\n",
      "  Runs per method: 3\n",
      "  Intervention budget: 10\n",
      "\n",
      "============================================================\n",
      "Evaluating: grpo_quick_minimize_20250723_101252_fixed\n",
      "Optimization: MINIMIZE\n",
      "\n",
      "Running evaluation command...\n",
      "  (This may take a few minutes)\n",
      "\n",
      "‚úÖ Evaluation completed in 13.6 minutes\n",
      "\n",
      "============================================================\n",
      "‚úÖ All evaluations complete!\n",
      "  Total duration: 13.6 minutes\n",
      "  Successful: 1/1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 5: Run evaluation with proper optimization handling\n",
    "\n",
    "Evaluate checkpoints against baselines with correct metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üèÅ Running Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = project_root / \"results\" / f\"evaluation_{EVALUATION_MODE.lower()}_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"\\nEvaluation parameters:\")\n",
    "print(f\"  Mode: {EVALUATION_MODE}\")\n",
    "print(f\"  Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"  Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"  Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "\n",
    "# Store results\n",
    "evaluation_results = {}\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Run evaluation for each checkpoint\n",
    "for ckpt in selected_checkpoints:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {ckpt.name}\")\n",
    "    print(f\"Optimization: {ckpt.optimization_config.direction}\")\n",
    "    \n",
    "    # Create checkpoint-specific output directory\n",
    "    ckpt_output_dir = output_dir / ckpt.name\n",
    "    ckpt_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Run evaluation using unified pipeline\n",
    "    cmd = [\n",
    "        \"poetry\", \"run\", \"python\",\n",
    "        str(project_root / \"scripts\" / \"unified_pipeline.py\"),\n",
    "        f\"--checkpoint={ckpt.path}\",\n",
    "        f\"--num-scms={min(3, NUM_TEST_SCMS)}\",  # Use subset for speed\n",
    "        f\"--runs-per-method={RUNS_PER_METHOD}\",\n",
    "        f\"--intervention-budget={INTERVENTION_BUDGET}\",\n",
    "        f\"--output-dir={ckpt_output_dir}\",\n",
    "        f\"--optimization-direction={ckpt.optimization_config.direction}\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nRunning evaluation command...\")\n",
    "    print(f\"  (This may take a few minutes)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            duration = (time.time() - start_time) / 60\n",
    "            print(f\"\\n‚úÖ Evaluation completed in {duration:.1f} minutes\")\n",
    "            \n",
    "            # Load results\n",
    "            results_file = ckpt_output_dir / \"comparison_results.json\"\n",
    "            if not results_file.exists():\n",
    "                # Try to find results in alternative locations\n",
    "                alt_results = list(ckpt_output_dir.glob(\"*results*.json\"))\n",
    "                if alt_results:\n",
    "                    results_file = alt_results[0]\n",
    "            \n",
    "            if results_file.exists():\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                \n",
    "                # Store results with optimization info\n",
    "                evaluation_results[ckpt.name] = {\n",
    "                    'results': results,\n",
    "                    'optimization_direction': ckpt.optimization_config.direction,\n",
    "                    'checkpoint_metadata': ckpt.to_dict(),\n",
    "                    'duration_minutes': duration\n",
    "                }\n",
    "                \n",
    "                # Quick summary\n",
    "                if 'statistical_analysis' in results:\n",
    "                    print(\"\\nüìä Quick Summary:\")\n",
    "                    summary = results['statistical_analysis'].get('summary_statistics', {})\n",
    "                    for method, stats in list(summary.items())[:3]:  # Show top 3\n",
    "                        mean_val = stats.get('target_improvement_mean', 0)\n",
    "                        # Convert if needed\n",
    "                        if ckpt.optimization_config.is_minimizing:\n",
    "                            display_val = -mean_val  # Show actual minimized value\n",
    "                        else:\n",
    "                            display_val = mean_val\n",
    "                        print(f\"  {method}: {ckpt.optimization_config.format_improvement(display_val)}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No results file found at {results_file}\")\n",
    "                evaluation_results[ckpt.name] = {'error': 'No results file found'}\n",
    "                \n",
    "        else:\n",
    "            print(f\"\\n‚ùå Evaluation failed with return code {result.returncode}\")\n",
    "            print(f\"Error: {result.stderr[:500]}...\")\n",
    "            evaluation_results[ckpt.name] = {'error': result.stderr}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed with exception: {e}\")\n",
    "        evaluation_results[ckpt.name] = {'error': str(e)}\n",
    "\n",
    "total_duration = (time.time() - evaluation_start_time) / 60\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ All evaluations complete!\")\n",
    "print(f\"  Total duration: {total_duration:.1f} minutes\")\n",
    "print(f\"  Successful: {sum(1 for r in evaluation_results.values() if 'error' not in r)}/{len(evaluation_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 6: Generate visualizations with trajectory plots\n",
    "\n",
    "Create comprehensive plots showing method performance over time.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Generating Visualizations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have results to visualize\n",
    "valid_results = {k: v for k, v in evaluation_results.items() if 'error' not in v}\n",
    "\n",
    "if not valid_results:\n",
    "    print(\"‚ùå No valid results to visualize\")\n",
    "else:\n",
    "    # Import required visualization functions and SCM utilities\n",
    "    from src.causal_bayes_opt.visualization.plots import (\n",
    "        plot_baseline_comparison, plot_method_comparison\n",
    "    )\n",
    "    from src.causal_bayes_opt.analysis.trajectory_metrics import (\n",
    "        extract_metrics_from_experiment_result\n",
    "    )\n",
    "    from causal_bayes_opt.data_structures.scm import get_target, get_parents\n",
    "    \n",
    "    # Process results for each checkpoint\n",
    "    for ckpt_name, eval_data in valid_results.items():\n",
    "        print(f\"\\nüìà Creating plots for: {ckpt_name}\")\n",
    "        \n",
    "        results = eval_data['results']\n",
    "        opt_direction = eval_data['optimization_direction']\n",
    "        opt_config = OptimizationConfig(direction=opt_direction)\n",
    "        \n",
    "        # Extract trajectory data from method results\n",
    "        plot_data = {}\n",
    "        \n",
    "        if 'method_results' in results:\n",
    "            print(f\"Found {len(results['method_results'])} methods to plot\")\n",
    "            \n",
    "            # Get SCM metadata to find true parents\n",
    "            scm_summary = results.get('scm_summary', {})\n",
    "            \n",
    "            for method_name, method_runs in results['method_results'].items():\n",
    "                print(f\"  Processing {method_name}...\")\n",
    "                \n",
    "                # Aggregate trajectories across runs\n",
    "                all_target_trajectories = []\n",
    "                all_f1_trajectories = []\n",
    "                all_shd_trajectories = []\n",
    "                \n",
    "                for run_idx, run_result in enumerate(method_runs):\n",
    "                    if run_result.get('success', True):\n",
    "                        # Get true parents for this run's SCM\n",
    "                        true_parents = []\n",
    "                        \n",
    "                        # First try metadata in the result\n",
    "                        if 'metadata' in run_result and 'true_parents' in run_result['metadata']:\n",
    "                            true_parents = run_result['metadata']['true_parents']\n",
    "                        # Then try SCM metadata\n",
    "                        elif 'metadata' in run_result and 'scm_name' in run_result['metadata']:\n",
    "                            scm_name = run_result['metadata']['scm_name']\n",
    "                            if scm_name in scm_summary:\n",
    "                                scm_info = scm_summary[scm_name]\n",
    "                                if 'target_parents' in scm_info:\n",
    "                                    true_parents = scm_info['target_parents']\n",
    "                        # Finally try to get from SCM index\n",
    "                        elif 'scm_idx' in run_result:\n",
    "                            scm_idx = run_result['scm_idx']\n",
    "                            # Get from test SCMs if available\n",
    "                            if hasattr(test_metadata, '__iter__') and scm_idx < len(test_metadata):\n",
    "                                scm_meta = test_metadata[scm_idx]\n",
    "                                if 'target_parents' in scm_meta:\n",
    "                                    true_parents = scm_meta['target_parents']\n",
    "                        \n",
    "                        # Extract trajectory using the analysis function\n",
    "                        trajectory = extract_metrics_from_experiment_result(\n",
    "                            run_result, \n",
    "                            true_parents\n",
    "                        )\n",
    "                        \n",
    "                        if trajectory['target_values']:\n",
    "                            all_target_trajectories.append(trajectory['target_values'])\n",
    "                        if trajectory['f1_scores']:\n",
    "                            all_f1_trajectories.append(trajectory['f1_scores'])\n",
    "                        if trajectory['shd_values']:\n",
    "                            all_shd_trajectories.append(trajectory['shd_values'])\n",
    "                \n",
    "                # Compute mean trajectories\n",
    "                if all_target_trajectories:\n",
    "                    # Pad trajectories to same length\n",
    "                    max_len = max(len(t) for t in all_target_trajectories)\n",
    "                    \n",
    "                    # Pad and compute means\n",
    "                    padded_targets = []\n",
    "                    for traj in all_target_trajectories:\n",
    "                        padded = list(traj) + [traj[-1]] * (max_len - len(traj))\n",
    "                        padded_targets.append(padded)\n",
    "                    \n",
    "                    padded_f1s = []\n",
    "                    for traj in all_f1_trajectories:\n",
    "                        if traj:  # Check if trajectory exists\n",
    "                            padded = list(traj) + [traj[-1]] * (max_len - len(traj))\n",
    "                        else:\n",
    "                            padded = [0.0] * max_len\n",
    "                        padded_f1s.append(padded)\n",
    "                        \n",
    "                    padded_shds = []\n",
    "                    for traj in all_shd_trajectories:\n",
    "                        if traj:  # Check if trajectory exists\n",
    "                            padded = list(traj) + [traj[-1]] * (max_len - len(traj))\n",
    "                        else:\n",
    "                            # Use number of true parents as worst-case SHD\n",
    "                            worst_shd = len(true_parents) if true_parents else 3\n",
    "                            padded = [worst_shd] * max_len\n",
    "                        padded_shds.append(padded)\n",
    "                    \n",
    "                    plot_data[method_name] = {\n",
    "                        'steps': list(range(1, max_len + 1)),\n",
    "                        'target_mean': np.mean(padded_targets, axis=0).tolist(),\n",
    "                        'target_std': np.std(padded_targets, axis=0).tolist(),\n",
    "                        'f1_mean': np.mean(padded_f1s, axis=0).tolist() if padded_f1s else [],\n",
    "                        'f1_std': np.std(padded_f1s, axis=0).tolist() if padded_f1s else [],\n",
    "                        'shd_mean': np.mean(padded_shds, axis=0).tolist() if padded_shds else [],\n",
    "                        'shd_std': np.std(padded_shds, axis=0).tolist() if padded_shds else [],\n",
    "                        'n_runs': len(all_target_trajectories)\n",
    "                    }\n",
    "                    print(f\"    ‚úì Extracted {len(all_target_trajectories)} runs, {max_len} steps\")\n",
    "                    print(f\"    ‚úì F1 trajectory: {len(plot_data[method_name]['f1_mean'])} steps\")\n",
    "                    print(f\"    ‚úì SHD trajectory: {len(plot_data[method_name]['shd_mean'])} steps\")\n",
    "                else:\n",
    "                    print(f\"    ‚ö†Ô∏è No trajectory data found\")\n",
    "        \n",
    "        # Create trajectory comparison plot if we have data\n",
    "        if plot_data:\n",
    "            print(f\"\\nüìä Creating trajectory plots with {len(plot_data)} methods...\")\n",
    "            \n",
    "            # Create output directory for this checkpoint\n",
    "            ckpt_plot_dir = output_dir / ckpt_name / \"plots\"\n",
    "            ckpt_plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 1. Three-panel trajectory plot\n",
    "            trajectory_path = ckpt_plot_dir / \"trajectory_comparison.png\"\n",
    "            try:\n",
    "                fig = plot_baseline_comparison(\n",
    "                    plot_data,\n",
    "                    title=f\"{ckpt_name} - {opt_config.direction} Optimization\",\n",
    "                    save_path=str(trajectory_path),\n",
    "                    figsize=(15, 10)\n",
    "                )\n",
    "                plt.show()\n",
    "                print(f\"‚úÖ Saved trajectory plot: {trajectory_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not create trajectory plot: {e}\")\n",
    "                # Fallback to simple matplotlib plot\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                for method, data in plot_data.items():\n",
    "                    if 'steps' in data and 'target_mean' in data:\n",
    "                        color = 'red' if 'Trained' in method else 'blue'\n",
    "                        label = method.replace(' + ', '\\n')\n",
    "                        \n",
    "                        axes[0].plot(data['steps'], data['target_mean'], \n",
    "                                   label=label, color=color, marker='o', alpha=0.7)\n",
    "                        \n",
    "                        if 'f1_mean' in data and data['f1_mean']:\n",
    "                            axes[1].plot(data['steps'], data['f1_mean'], \n",
    "                                       label=label, color=color, marker='o', alpha=0.7)\n",
    "                        \n",
    "                        if 'shd_mean' in data and data['shd_mean']:\n",
    "                            axes[2].plot(data['steps'], data['shd_mean'], \n",
    "                                       label=label, color=color, marker='o', alpha=0.7)\n",
    "                \n",
    "                axes[0].set_xlabel('Intervention Step')\n",
    "                axes[0].set_ylabel('Target Value')\n",
    "                axes[0].set_title(f'Target Progress ({opt_config.direction})')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                axes[1].set_xlabel('Intervention Step')\n",
    "                axes[1].set_ylabel('F1 Score')\n",
    "                axes[1].set_title('Structure Learning (F1)')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                axes[2].set_xlabel('Intervention Step')\n",
    "                axes[2].set_ylabel('SHD')\n",
    "                axes[2].set_title('Structure Learning (SHD)')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.suptitle(f'{ckpt_name} - Trajectory Comparison', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(trajectory_path, dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                print(f\"‚úÖ Created fallback trajectory plot: {trajectory_path}\")\n",
    "            \n",
    "            # 2. Summary bar plot\n",
    "            summary_path = ckpt_plot_dir / \"performance_summary.png\"\n",
    "            \n",
    "            # Extract final values\n",
    "            methods = []\n",
    "            final_targets = []\n",
    "            final_f1s = []\n",
    "            final_shds = []\n",
    "            \n",
    "            for method, data in plot_data.items():\n",
    "                if 'target_mean' in data and data['target_mean']:\n",
    "                    methods.append(method)\n",
    "                    final_targets.append(data['target_mean'][-1])\n",
    "                    final_f1s.append(data['f1_mean'][-1] if data.get('f1_mean') else 0.0)\n",
    "                    final_shds.append(data['shd_mean'][-1] if data.get('shd_mean') else 0.0)\n",
    "            \n",
    "            if methods:\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                x = np.arange(len(methods))\n",
    "                colors = ['red' if 'Trained' in m else 'blue' for m in methods]\n",
    "                \n",
    "                # Target values\n",
    "                ax1.bar(x, final_targets, color=colors, alpha=0.7)\n",
    "                ax1.set_ylabel('Final Target Value')\n",
    "                ax1.set_title(f'Target Values ({opt_config.direction})')\n",
    "                ax1.set_xticks(x)\n",
    "                ax1.set_xticklabels([m.replace(' + ', '\\n') for m in methods], \n",
    "                                   rotation=45, ha='right')\n",
    "                ax1.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # F1 scores\n",
    "                ax2.bar(x, final_f1s, color=colors, alpha=0.7)\n",
    "                ax2.set_ylabel('F1 Score')\n",
    "                ax2.set_title('Structure Learning F1 (Higher is Better)')\n",
    "                ax2.set_xticks(x)\n",
    "                ax2.set_xticklabels([m.replace(' + ', '\\n') for m in methods], \n",
    "                                   rotation=45, ha='right')\n",
    "                ax2.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # SHD values\n",
    "                ax3.bar(x, final_shds, color=colors, alpha=0.7)\n",
    "                ax3.set_ylabel('SHD')\n",
    "                ax3.set_title('Structural Hamming Distance (Lower is Better)')\n",
    "                ax3.set_xticks(x)\n",
    "                ax3.set_xticklabels([m.replace(' + ', '\\n') for m in methods], \n",
    "                                   rotation=45, ha='right')\n",
    "                ax3.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                plt.suptitle(f'{ckpt_name} - Final Performance Summary', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                print(f\"‚úÖ Saved summary plot: {summary_path}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No trajectory data available for plotting\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualization complete!\")\n",
    "    print(f\"üìÅ Plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 7: Export results and generate summary report\n",
    "\n",
    "Save all results and provide clear guidance on where to find outputs.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíæ Exporting Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare export data\n",
    "export_data = {\n",
    "    'evaluation_config': {\n",
    "        'mode': EVALUATION_MODE,\n",
    "        'num_test_scms': NUM_TEST_SCMS,\n",
    "        'runs_per_method': RUNS_PER_METHOD,\n",
    "        'intervention_budget': INTERVENTION_BUDGET,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'timestamp': timestamp\n",
    "    },\n",
    "    'checkpoints_evaluated': [\n",
    "        {\n",
    "            'name': ckpt.name,\n",
    "            'optimization_direction': ckpt.optimization_config.direction,\n",
    "            'path': str(ckpt.path),\n",
    "            'training_mode': ckpt.training_config.get('mode', 'unknown')\n",
    "        }\n",
    "        for ckpt in selected_checkpoints\n",
    "    ],\n",
    "    'results': evaluation_results,\n",
    "    'method_performance': method_performance if 'method_performance' in locals() else {},\n",
    "    'duration_minutes': total_duration if 'total_duration' in locals() else 0\n",
    "}\n",
    "\n",
    "# Save JSON results\n",
    "json_path = output_dir / \"evaluation_results.json\"\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {json_path}\")\n",
    "\n",
    "# Generate text summary\n",
    "summary_path = output_dir / \"evaluation_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"GRPO Evaluation Summary\\n\")\n",
    "    f.write(f\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Evaluation Mode: {EVALUATION_MODE}\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "    f.write(f\"Duration: {total_duration:.1f} minutes\\n\\n\" if 'total_duration' in locals() else \"\\n\")\n",
    "    \n",
    "    f.write(f\"Configuration:\\n\")\n",
    "    f.write(f\"  Test SCMs: {NUM_TEST_SCMS}\\n\")\n",
    "    f.write(f\"  Runs per method: {RUNS_PER_METHOD}\\n\")\n",
    "    f.write(f\"  Intervention budget: {INTERVENTION_BUDGET}\\n\")\n",
    "    f.write(f\"  Random seed: {RANDOM_SEED}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Checkpoints Evaluated:\\n\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        f.write(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\\n\")\n",
    "    \n",
    "    f.write(f\"\\nKey Findings:\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Summarize results\n",
    "    if 'method_performance' in locals() and method_performance:\n",
    "        # Group by checkpoint\n",
    "        for ckpt in selected_checkpoints:\n",
    "            f.write(f\"\\n{ckpt.name} ({ckpt.optimization_config.direction}):\\n\")\n",
    "            \n",
    "            # Get methods for this checkpoint\n",
    "            ckpt_methods = {k: v for k, v in method_performance.items() if v['checkpoint'] == ckpt.name}\n",
    "            \n",
    "            if ckpt_methods:\n",
    "                # Sort by performance\n",
    "                if ckpt.optimization_config.is_minimizing:\n",
    "                    sorted_methods = sorted(ckpt_methods.items(), key=lambda x: x[1]['actual_value'])\n",
    "                else:\n",
    "                    sorted_methods = sorted(ckpt_methods.items(), key=lambda x: x[1]['actual_value'], reverse=True)\n",
    "                \n",
    "                for rank, (key, perf) in enumerate(sorted_methods[:5], 1):  # Top 5\n",
    "                    marker = \"*\" if 'Policy' in perf['method'] or 'Trained' in perf['method'] else \" \"\n",
    "                    f.write(f\"  {rank}. {marker} {perf['method']}: {perf['display_value']}\\n\")\n",
    "    \n",
    "    if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "        f.write(f\"\\n\\nOptimization Direction Insights:\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(\"This evaluation compared minimization vs maximization objectives.\\n\")\n",
    "        f.write(\"Key observation: Different optimization directions require different\\n\")\n",
    "        f.write(\"reward structures and may lead to different exploration strategies.\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\\nOutput Files:\\n\")\n",
    "    f.write(f\"  Results JSON: {json_path.name}\\n\")\n",
    "    f.write(f\"  Summary: {summary_path.name}\\n\")\n",
    "    \n",
    "    # List all plots with full paths\n",
    "    plot_locations = []\n",
    "    \n",
    "    # Check for plots in main directory\n",
    "    main_plots = list(output_dir.glob(\"*.png\"))\n",
    "    for plot in main_plots:\n",
    "        f.write(f\"  Plot: {plot.name}\\n\")\n",
    "        plot_locations.append(plot)\n",
    "    \n",
    "    # Check for plots in checkpoint subdirectories\n",
    "    for ckpt in selected_checkpoints:\n",
    "        ckpt_plot_dir = output_dir / ckpt.name / \"plots\"\n",
    "        if ckpt_plot_dir.exists():\n",
    "            ckpt_plots = list(ckpt_plot_dir.glob(\"*.png\"))\n",
    "            for plot in ckpt_plots:\n",
    "                f.write(f\"  Plot ({ckpt.name}): {plot.relative_to(output_dir)}\\n\")\n",
    "                plot_locations.append(plot)\n",
    "    \n",
    "    # Also check for plots from unified pipeline\n",
    "    comparison_results_path = output_dir / selected_checkpoints[0].name / \"comparison_results.json\"\n",
    "    if comparison_results_path.exists():\n",
    "        f.write(f\"\\n  Comparison results: {comparison_results_path.relative_to(output_dir)}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_path}\")\n",
    "\n",
    "# Display final summary with plot locations\n",
    "print(f\"\\nüéâ Evaluation Complete!\")\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Checkpoints: {len(selected_checkpoints)}\")\n",
    "print(f\"Duration: {total_duration:.1f} minutes\" if 'total_duration' in locals() else \"\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "\n",
    "# Show plot locations clearly\n",
    "if plot_locations:\n",
    "    print(f\"\\nüìä Generated Plots ({len(plot_locations)} total):\")\n",
    "    for plot in plot_locations:\n",
    "        print(f\"  - {plot.relative_to(output_dir)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No plots were found. Check the visualization cell output for errors.\")\n",
    "\n",
    "print(f\"\\nüìä Key Insights:\")\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    print(f\"- Evaluated {selected_checkpoints[0].name} with {selected_checkpoints[0].optimization_config.direction} objective\")\n",
    "    print(f\"- Compare against baselines to see if training improved performance\")\n",
    "    print(f\"- Check trajectory plots to see learning progress over time\")\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    print(f\"- Compared {len(selected_checkpoints)} checkpoints with different optimization directions\")\n",
    "    print(f\"- Minimization policies optimize for lower target values (like PARENT_SCALE)\")\n",
    "    print(f\"- Maximization policies optimize for higher target values\")\n",
    "    print(f\"- Check plots to see which direction performs better for your use case\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"1. Review the trajectory plots showing F1, SHD, and target value progress\")\n",
    "print(f\"2. Check the summary plots for final performance comparison\")\n",
    "print(f\"3. Read the detailed results in {json_path.name}\")\n",
    "print(f\"4. If plots are missing, re-run the visualization cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we've accomplished:**\n",
    "1. ‚úÖ Loaded checkpoints with optimization direction metadata\n",
    "2. ‚úÖ Ran evaluation with proper metric handling\n",
    "3. ‚úÖ Generated visualizations that show min/max correctly\n",
    "4. ‚úÖ Exported comprehensive results\n",
    "\n",
    "**Key improvements over original notebook:**\n",
    "- Checkpoint-first approach - no need to run training\n",
    "- Auto-detects optimization direction from metadata\n",
    "- Correctly displays minimization vs maximization results\n",
    "- All cells are independent and can be re-run\n",
    "- No silent failures - explicit errors throughout\n",
    "\n",
    "**Understanding the results:**\n",
    "- For MINIMIZE checkpoints: Lower target values are better\n",
    "- For MAXIMIZE checkpoints: Higher target values are better\n",
    "- The plots show \"(‚Üì better)\" or \"(‚Üë better)\" to clarify\n",
    "- PARENT_SCALE baseline uses minimization by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-bayes-opt-sr_Vb8Og-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
