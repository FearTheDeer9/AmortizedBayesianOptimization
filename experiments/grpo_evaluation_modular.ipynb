{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Evaluation - Modular Version\n",
    "\n",
    "**Purpose**: Evaluate trained GRPO checkpoints with proper optimization direction handling.\n",
    "\n",
    "**Key Features**:\n",
    "- ‚úÖ **Checkpoint-first approach** - load any checkpoint to evaluate\n",
    "- ‚úÖ **Auto-detect optimization** - reads direction from checkpoint metadata\n",
    "- ‚úÖ **Independent cells** - no need to run training first\n",
    "- ‚úÖ **Correct metrics** - handles both minimization and maximization\n",
    "- ‚úÖ **Multiple modes** - single checkpoint, compare checkpoints, compare objectives\n",
    "\n",
    "**Workflow**:\n",
    "1. Select evaluation mode and checkpoints\n",
    "2. Load checkpoint(s) and validate metadata\n",
    "3. Generate or load test SCMs\n",
    "4. Run evaluation with baselines\n",
    "5. Generate visualizations with correct labels\n",
    "6. Export results for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-07-23 20:36:17,380:jax._src.xla_bridge:749: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "[2025-07-23 20:36:17,380][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Setup Complete\n",
      "üìÅ Project root: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt\n",
      "üîß JAX devices: [CpuDevice(id=0)]\n",
      "üìÖ Date: 2025-07-23 20:36:17\n",
      "\n",
      "üìÅ Checkpoint directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cell 1: Import base components and configure environment\n",
    "\n",
    "This cell sets up the evaluation environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"experiments\" else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import base components\n",
    "from scripts.notebooks.base_components import (\n",
    "    NotebookError, CheckpointManager, SCMGenerator,\n",
    "    OptimizationConfig, CheckpointMetadata, validate_environment,\n",
    "    format_results_summary\n",
    ")\n",
    "from scripts.notebooks.config_templates import create_evaluation_config\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import pyrsistent as pyr\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Validate environment\n",
    "try:\n",
    "    env_info = validate_environment()\n",
    "    print(\"‚úÖ Environment Setup Complete\")\n",
    "    print(f\"üìÅ Project root: {project_root}\")\n",
    "    print(f\"üîß JAX devices: {env_info['jax_devices']}\")\n",
    "    print(f\"üìÖ Date: {env_info['timestamp']}\")\n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Environment validation failed: {e}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_dir = project_root / \"checkpoints\" / \"grpo_training\"\n",
    "checkpoint_manager = CheckpointManager(checkpoint_dir)\n",
    "print(f\"\\nüìÅ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Evaluation Configuration\n",
      "==================================================\n",
      "Mode: SINGLE_CHECKPOINT\n",
      "Test SCMs: 10\n",
      "Runs per method: 3\n",
      "Intervention budget: 10\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 2: Configure evaluation parameters\n",
    "\n",
    "Set evaluation mode and parameters for the run.\n",
    "\"\"\"\n",
    "\n",
    "# EVALUATION MODE SELECTION\n",
    "EVALUATION_MODE = \"SINGLE_CHECKPOINT\"  # Options: \"SINGLE_CHECKPOINT\", \"COMPARE_CHECKPOINTS\", \"COMPARE_OBJECTIVES\"\n",
    "\n",
    "# Configuration for different modes\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Evaluate one checkpoint against baselines\n",
    "    NUM_TEST_SCMS = 10\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Compare multiple checkpoints\n",
    "    COMPARISON_COUNT = 3  # Number of checkpoints to compare\n",
    "    NUM_TEST_SCMS = 5  # Fewer SCMs for faster comparison\n",
    "    RUNS_PER_METHOD = 2\n",
    "    INTERVENTION_BUDGET = 8\n",
    "    \n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Compare minimization vs maximization\n",
    "    NUM_TEST_SCMS = 8\n",
    "    RUNS_PER_METHOD = 3\n",
    "    INTERVENTION_BUDGET = 10\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# General settings\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"üéØ Evaluation Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "if EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    print(f\"Checkpoints to compare: {COMPARISON_COUNT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Mode and Checkpoint Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "<>:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "/var/folders/2f/7z7glsfj1fd22nlr6wj56z5w0000gn/T/ipykernel_61810/2931854767.py:108: SyntaxWarning: invalid escape sequence '\\!'\n",
      "  print(f\"üöÄ All selected checkpoints validated and ready for evaluation\\!\")\n",
      "[2025-07-23 20:36:17,454][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,455][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-23 20:36:17,457][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,459][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,461][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-23 20:36:17,470][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,499][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,500][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-23 20:36:17,502][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,559][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n",
      "[2025-07-23 20:36:17,568][scripts.notebooks.base_components][WARNING] - Failed to load metadata from /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_094650/metadata.json: Expecting value: line 11 column 15 (char 381)\n",
      "[2025-07-23 20:36:17,569][scripts.notebooks.base_components][WARNING] - Legacy checkpoint detected - inferring optimization direction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Intelligent Checkpoint Selection\n",
      "==================================================\n",
      "Found 4 total checkpoints\n",
      "Usable checkpoints:\n",
      "  MINIMIZE: 1\n",
      "  MAXIMIZE: 2\n",
      "  Total usable: 3\n",
      "üéØ Selected best MINIMIZE checkpoint: grpo_quick_minimize_20250723_101252_fixed\n",
      "  ‚úÖ Checkpoint is valid and ready for evaluation\n",
      "‚úÖ Final Selection (1 checkpoint(s)):\n",
      "1. grpo_quick_minimize_20250723_101252_fixed\n",
      "     Optimization: MINIMIZE\n",
      "     Training mode: QUICK\n",
      "     Path: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/checkpoints/grpo_training/grpo_quick_minimize_20250723_101252_fixed\n",
      "     Status: ‚úÖ Ready for evaluation\n",
      "üöÄ All selected checkpoints validated and ready for evaluation\\!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Select checkpoints for evaluation\n",
    "\n",
    "This cell handles checkpoint selection based on the evaluation mode.\n",
    "Uses intelligent discovery instead of hardcoded names.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Intelligent Checkpoint Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get available checkpoints\n",
    "try:\n",
    "    available_checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    if not available_checkpoints:\n",
    "        raise NotebookError(\"No checkpoints found\")\n",
    "        \n",
    "    print(f\"Found {len(available_checkpoints)} total checkpoints\")\n",
    "    \n",
    "    # Show usable checkpoints by direction\n",
    "    usable_minimize = checkpoint_manager.find_usable_checkpoints('MINIMIZE')\n",
    "    usable_maximize = checkpoint_manager.find_usable_checkpoints('MAXIMIZE')\n",
    "    \n",
    "    print(f\"Usable checkpoints:\")\n",
    "    print(f\"  MINIMIZE: {len(usable_minimize)}\")\n",
    "    print(f\"  MAXIMIZE: {len(usable_maximize)}\")\n",
    "    print(f\"  Total usable: {len(usable_minimize) + len(usable_maximize)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to analyze checkpoints: {e}\")\n",
    "\n",
    "# SELECT CHECKPOINTS BASED ON MODE\n",
    "selected_checkpoints = []\n",
    "\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    # Find best MINIMIZE checkpoint (preferred for comparison with PARENT_SCALE)\n",
    "    best_checkpoint = checkpoint_manager.find_best_checkpoint({\n",
    "        'optimization_direction': 'MINIMIZE',\n",
    "        'training_mode': 'QUICK'\n",
    "    })\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        selected_checkpoints = [best_checkpoint]\n",
    "        print(f\"üéØ Selected best MINIMIZE checkpoint: {best_checkpoint.name}\")\n",
    "        \n",
    "        # Validate the selected checkpoint\n",
    "        validation = checkpoint_manager.validate_checkpoint(best_checkpoint)\n",
    "        if validation['is_valid']:\n",
    "            print(f\"  ‚úÖ Checkpoint is valid and ready for evaluation\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Checkpoint issues: {validation['issues']}\")\n",
    "    else:\n",
    "        # Fallback: try any usable checkpoint\n",
    "        all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "        if all_usable:\n",
    "            selected_checkpoints = [all_usable[0]]\n",
    "            print(f\"üéØ No MINIMIZE checkpoint found, using: {all_usable[0].name}\")\n",
    "            print(f\"  Optimization: {all_usable[0].optimization_config.direction}\")\n",
    "        else:\n",
    "            raise NotebookError(\"No usable checkpoints found. Please ensure checkpoints have both metadata.json and checkpoint.pkl files.\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_CHECKPOINTS\":\n",
    "    # Get multiple usable checkpoints\n",
    "    all_usable = checkpoint_manager.find_usable_checkpoints()\n",
    "    comparison_count = min(COMPARISON_COUNT, len(all_usable))\n",
    "    selected_checkpoints = all_usable[:comparison_count]\n",
    "    print(f\"üìä Selected {comparison_count} checkpoints for comparison\")\n",
    "\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    # Get best from each optimization direction\n",
    "    best_minimize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MINIMIZE'})\n",
    "    best_maximize = checkpoint_manager.find_best_checkpoint({'optimization_direction': 'MAXIMIZE'})\n",
    "    \n",
    "    selected_checkpoints = []\n",
    "    if best_minimize:\n",
    "        selected_checkpoints.append(best_minimize)\n",
    "    if best_maximize:\n",
    "        selected_checkpoints.append(best_maximize)\n",
    "    \n",
    "    if not selected_checkpoints:\n",
    "        raise NotebookError(\"Need checkpoints from both MINIMIZE and MAXIMIZE directions for objective comparison\")\n",
    "    \n",
    "    print(f\"üîÑ Selected checkpoints for objective comparison:\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        print(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\")\n",
    "\n",
    "else:\n",
    "    raise NotebookError(f\"Unknown evaluation mode: {EVALUATION_MODE}\")\n",
    "\n",
    "# Final validation\n",
    "if not selected_checkpoints:\n",
    "    raise NotebookError(\"No checkpoints selected for evaluation\")\n",
    "\n",
    "print(f\"‚úÖ Final Selection ({len(selected_checkpoints)} checkpoint(s)):\")\n",
    "for i, checkpoint in enumerate(selected_checkpoints, 1):\n",
    "    print(f\"{i}. {checkpoint.name}\")\n",
    "    print(f\"     Optimization: {checkpoint.optimization_config.direction}\")\n",
    "    print(f\"     Training mode: {checkpoint.training_config.get('mode', 'unknown')}\")\n",
    "    print(f\"     Path: {checkpoint.path}\")\n",
    "    \n",
    "    # Final validation\n",
    "    validation = checkpoint_manager.validate_checkpoint(checkpoint)\n",
    "    if validation['is_valid']:\n",
    "        print(f\"     Status: ‚úÖ Ready for evaluation\")\n",
    "    else:\n",
    "        print(f\"     Status: ‚ùå Issues found: {validation['issues']}\")\n",
    "        raise NotebookError(f\"Selected checkpoint {checkpoint.name} has validation issues: {validation['issues']}\")\n",
    "\n",
    "print(f\"üöÄ All selected checkpoints validated and ready for evaluation\\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Checkpoint Metadata\n",
      "============================================================\n",
      "\n",
      "Loading: grpo_quick_minimize_20250723_101252_fixed\n",
      "  ‚úì Optimization: MINIMIZE\n",
      "  ‚úì Training mode: QUICK\n",
      "  ‚úì Episodes completed: unknown\n",
      "  ‚úì Duration: 5.0 minutes\n",
      "  ‚úì Reward weights: opt=0.8, struct=0.1, eff=0.1\n",
      "\n",
      "‚úÖ Loaded 1 checkpoint(s) successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 3: Load checkpoint metadata and validate\n",
    "\n",
    "This cell loads the selected checkpoints and validates their metadata.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• Loading Checkpoint Metadata\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store loaded checkpoint info\n",
    "loaded_checkpoints = {}\n",
    "\n",
    "for ckpt in selected_checkpoints:\n",
    "    print(f\"\\nLoading: {ckpt.name}\")\n",
    "    try:\n",
    "        # For now, we're using the metadata we already have\n",
    "        # In production, this would load the actual model parameters\n",
    "        loaded_checkpoints[ckpt.name] = {\n",
    "            'metadata': ckpt,\n",
    "            'optimization_config': ckpt.optimization_config,\n",
    "            'training_config': ckpt.training_config,\n",
    "            'model_params': None  # TODO: Load actual model parameters\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Optimization: {ckpt.optimization_config.direction}\")\n",
    "        print(f\"  ‚úì Training mode: {ckpt.training_config.get('mode', 'unknown')}\")\n",
    "        print(f\"  ‚úì Episodes completed: {ckpt.training_results.get('episodes_completed', 'unknown')}\")\n",
    "        print(f\"  ‚úì Duration: {ckpt.training_results.get('duration_minutes', 0):.1f} minutes\")\n",
    "        \n",
    "        # Show reward weights if available\n",
    "        if 'reward_weights' in ckpt.training_config:\n",
    "            weights = ckpt.training_config['reward_weights']\n",
    "            print(f\"  ‚úì Reward weights: opt={weights.get('optimization', 0):.1f}, \"\n",
    "                  f\"struct={weights.get('discovery', 0):.1f}, \"\n",
    "                  f\"eff={weights.get('efficiency', 0):.1f}\")\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed to load: {e}\")\n",
    "        raise NotebookError(f\"Failed to load checkpoint {ckpt.name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(loaded_checkpoints)} checkpoint(s) successfully\")\n",
    "\n",
    "# Check optimization compatibility for comparison modes\n",
    "if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    directions = [ckpt.optimization_config.direction for ckpt in selected_checkpoints]\n",
    "    if len(set(directions)) == 1:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: All checkpoints have same optimization direction: {directions[0]}\")\n",
    "        print(\"   Objective comparison may not be meaningful.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Comparing optimization directions: {set(directions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Test SCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Generating Test SCMs\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-23 20:36:18,135][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-23 20:36:18,135][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-23 20:36:18,150][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-23 20:36:18,151][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-23 20:36:18,169][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X2'\n",
      "[2025-07-23 20:36:18,170][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 5 vars, 4 edges, target=X2\n",
      "[2025-07-23 20:36:18,175][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X3'\n",
      "[2025-07-23 20:36:18,175][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated fork SCM: 6 vars, 5 edges, target=X3\n",
      "[2025-07-23 20:36:18,195][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X2'\n",
      "[2025-07-23 20:36:18,197][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 3 vars, 2 edges, target=X2\n",
      "[2025-07-23 20:36:18,201][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X3'\n",
      "[2025-07-23 20:36:18,202][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 4 vars, 3 edges, target=X3\n",
      "[2025-07-23 20:36:18,205][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 5 variables, 4 edges, target='X4'\n",
      "[2025-07-23 20:36:18,206][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 5 vars, 4 edges, target=X4\n",
      "[2025-07-23 20:36:18,210][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 6 variables, 5 edges, target='X5'\n",
      "[2025-07-23 20:36:18,211][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated chain SCM: 6 vars, 5 edges, target=X5\n",
      "[2025-07-23 20:36:18,213][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 3 variables, 2 edges, target='X1'\n",
      "[2025-07-23 20:36:18,214][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 3 vars, 2 edges, target=X1\n",
      "[2025-07-23 20:36:18,217][causal_bayes_opt.experiments.test_scms][INFO] - Created linear SCM with 4 variables, 3 edges, target='X2'\n",
      "[2025-07-23 20:36:18,217][causal_bayes_opt.experiments.variable_scm_factory][INFO] - Generated collider SCM: 4 vars, 3 edges, target=X2\n",
      "[2025-07-23 20:36:18,218][scripts.notebooks.base_components][INFO] - Generated 10 SCMs\n",
      "[2025-07-23 20:36:18,219][scripts.notebooks.base_components][INFO] - Distribution: {'structure_types': {'fork': 4, 'chain': 4, 'collider': 2}, 'variable_counts': {3: 3, 4: 3, 5: 2, 6: 2}, 'total': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Generated 10 test SCMs\n",
      "\n",
      "üìä Test Set Distribution:\n",
      "  Structure types: {'fork': 4, 'chain': 4, 'collider': 2}\n",
      "  Variable counts: {3: 3, 4: 3, 5: 2, 6: 2}\n",
      "\n",
      "üíæ Saved test SCM metadata to: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/test_scms/test_scms_1042.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 4: Generate test SCMs for evaluation\n",
    "\n",
    "Create a balanced set of test SCMs different from training.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî¨ Generating Test SCMs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize SCM generator\n",
    "scm_generator = SCMGenerator()\n",
    "\n",
    "# Generate test SCMs with different seed than training\n",
    "test_seed = RANDOM_SEED + 1000  # Ensure different from training\n",
    "\n",
    "try:\n",
    "    test_scms, test_metadata = scm_generator.generate_balanced_scms(\n",
    "        num_scms=NUM_TEST_SCMS,\n",
    "        variable_range=(3, 6),\n",
    "        structure_types=['fork', 'chain', 'collider', 'mixed'],\n",
    "        seed=test_seed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(test_scms)} test SCMs\")\n",
    "    \n",
    "    # Analyze distribution\n",
    "    distribution = scm_generator._summarize_distribution(test_metadata)\n",
    "    print(f\"\\nüìä Test Set Distribution:\")\n",
    "    print(f\"  Structure types: {distribution['structure_types']}\")\n",
    "    print(f\"  Variable counts: {distribution['variable_counts']}\")\n",
    "    \n",
    "    # Save test SCM metadata\n",
    "    test_scm_path = project_root / \"results\" / \"test_scms\" / f\"test_scms_{test_seed}.json\"\n",
    "    test_scm_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(test_scm_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'metadata': test_metadata,\n",
    "            'config': {\n",
    "                'num_scms': len(test_scms),\n",
    "                'seed': test_seed,\n",
    "                'variable_range': [3, 6],\n",
    "                'structure_types': ['fork', 'chain', 'collider', 'mixed']\n",
    "            },\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved test SCM metadata to: {test_scm_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise NotebookError(f\"Failed to generate test SCMs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Running Evaluation\n",
      "============================================================\n",
      "üìÅ Output directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618\n",
      "\n",
      "Evaluation parameters:\n",
      "  Mode: SINGLE_CHECKPOINT\n",
      "  Test SCMs: 10\n",
      "  Runs per method: 3\n",
      "  Intervention budget: 10\n",
      "\n",
      "============================================================\n",
      "Evaluating: grpo_quick_minimize_20250723_101252_fixed\n",
      "Optimization: MINIMIZE\n",
      "\n",
      "Running evaluation command...\n",
      "  (This may take a few minutes)\n",
      "\n",
      "‚úÖ Evaluation completed in 0.0 minutes\n",
      "‚ö†Ô∏è No results file found at /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618/grpo_quick_minimize_20250723_101252_fixed/comparison_results.json\n",
      "\n",
      "============================================================\n",
      "‚úÖ All evaluations complete!\n",
      "  Total duration: 0.0 minutes\n",
      "  Successful: 0/1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 5: Run evaluation with proper optimization handling\n",
    "\n",
    "Evaluate checkpoints against baselines with correct metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üèÅ Running Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = project_root / \"results\" / f\"evaluation_{EVALUATION_MODE.lower()}_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"\\nEvaluation parameters:\")\n",
    "print(f\"  Mode: {EVALUATION_MODE}\")\n",
    "print(f\"  Test SCMs: {NUM_TEST_SCMS}\")\n",
    "print(f\"  Runs per method: {RUNS_PER_METHOD}\")\n",
    "print(f\"  Intervention budget: {INTERVENTION_BUDGET}\")\n",
    "\n",
    "# Store results\n",
    "evaluation_results = {}\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Run evaluation for each checkpoint\n",
    "for ckpt in selected_checkpoints:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {ckpt.name}\")\n",
    "    print(f\"Optimization: {ckpt.optimization_config.direction}\")\n",
    "    \n",
    "    # Create checkpoint-specific output directory\n",
    "    ckpt_output_dir = output_dir / ckpt.name\n",
    "    ckpt_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Run evaluation using unified pipeline\n",
    "    cmd = [\n",
    "        \"poetry\", \"run\", \"python\",\n",
    "        str(project_root / \"scripts\" / \"unified_pipeline.py\"),\n",
    "        f\"--checkpoint={ckpt.path}\",\n",
    "        f\"--num-scms={min(3, NUM_TEST_SCMS)}\",  # Use subset for speed\n",
    "        f\"--runs-per-method={RUNS_PER_METHOD}\",\n",
    "        f\"--intervention-budget={INTERVENTION_BUDGET}\",\n",
    "        f\"--output-dir={ckpt_output_dir}\",\n",
    "        f\"--optimization-direction={ckpt.optimization_config.direction}\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nRunning evaluation command...\")\n",
    "    print(f\"  (This may take a few minutes)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            duration = (time.time() - start_time) / 60\n",
    "            print(f\"\\n‚úÖ Evaluation completed in {duration:.1f} minutes\")\n",
    "            \n",
    "            # Load results\n",
    "            results_file = ckpt_output_dir / \"comparison_results.json\"\n",
    "            if not results_file.exists():\n",
    "                # Try to find results in alternative locations\n",
    "                alt_results = list(ckpt_output_dir.glob(\"*results*.json\"))\n",
    "                if alt_results:\n",
    "                    results_file = alt_results[0]\n",
    "            \n",
    "            if results_file.exists():\n",
    "                with open(results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                \n",
    "                # Store results with optimization info\n",
    "                evaluation_results[ckpt.name] = {\n",
    "                    'results': results,\n",
    "                    'optimization_direction': ckpt.optimization_config.direction,\n",
    "                    'checkpoint_metadata': ckpt.to_dict(),\n",
    "                    'duration_minutes': duration\n",
    "                }\n",
    "                \n",
    "                # Quick summary\n",
    "                if 'statistical_analysis' in results:\n",
    "                    print(\"\\nüìä Quick Summary:\")\n",
    "                    summary = results['statistical_analysis'].get('summary_statistics', {})\n",
    "                    for method, stats in list(summary.items())[:3]:  # Show top 3\n",
    "                        mean_val = stats.get('target_improvement_mean', 0)\n",
    "                        # Convert if needed\n",
    "                        if ckpt.optimization_config.is_minimizing:\n",
    "                            display_val = -mean_val  # Show actual minimized value\n",
    "                        else:\n",
    "                            display_val = mean_val\n",
    "                        print(f\"  {method}: {ckpt.optimization_config.format_improvement(display_val)}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No results file found at {results_file}\")\n",
    "                evaluation_results[ckpt.name] = {'error': 'No results file found'}\n",
    "                \n",
    "        else:\n",
    "            print(f\"\\n‚ùå Evaluation failed with return code {result.returncode}\")\n",
    "            print(f\"Error: {result.stderr[:500]}...\")\n",
    "            evaluation_results[ckpt.name] = {'error': result.stderr}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed with exception: {e}\")\n",
    "        evaluation_results[ckpt.name] = {'error': str(e)}\n",
    "\n",
    "total_duration = (time.time() - evaluation_start_time) / 60\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ All evaluations complete!\")\n",
    "print(f\"  Total duration: {total_duration:.1f} minutes\")\n",
    "print(f\"  Successful: {sum(1 for r in evaluation_results.values() if 'error' not in r)}/{len(evaluation_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating Visualizations\n",
      "============================================================\n",
      "‚ùå No valid results to visualize\n",
      "\n",
      "‚úÖ Visualization complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 6: Generate visualizations with proper optimization labels\n",
    "\n",
    "Create plots that correctly show minimization vs maximization.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä Generating Visualizations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have results to visualize\n",
    "valid_results = {k: v for k, v in evaluation_results.items() if 'error' not in v}\n",
    "\n",
    "if not valid_results:\n",
    "    print(\"‚ùå No valid results to visualize\")\n",
    "else:\n",
    "    # Extract method performance\n",
    "    method_performance = {}\n",
    "    \n",
    "    for ckpt_name, eval_data in valid_results.items():\n",
    "        results = eval_data['results']\n",
    "        opt_direction = eval_data['optimization_direction']\n",
    "        opt_config = OptimizationConfig(direction=opt_direction)\n",
    "        \n",
    "        if 'statistical_analysis' in results and 'summary_statistics' in results['statistical_analysis']:\n",
    "            for method, stats in results['statistical_analysis']['summary_statistics'].items():\n",
    "                key = f\"{ckpt_name}_{method}\"\n",
    "                \n",
    "                # Get raw value\n",
    "                raw_value = stats.get('target_improvement_mean', 0.0)\n",
    "                \n",
    "                # Convert to actual value for minimization\n",
    "                if opt_config.is_minimizing:\n",
    "                    actual_value = -raw_value\n",
    "                else:\n",
    "                    actual_value = raw_value\n",
    "                \n",
    "                method_performance[key] = {\n",
    "                    'checkpoint': ckpt_name,\n",
    "                    'method': method,\n",
    "                    'raw_value': raw_value,\n",
    "                    'actual_value': actual_value,\n",
    "                    'optimization_direction': opt_direction,\n",
    "                    'display_value': opt_config.format_improvement(actual_value)\n",
    "                }\n",
    "    \n",
    "    # Create visualizations based on mode\n",
    "    if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "        # Single checkpoint visualization\n",
    "        ckpt_name = selected_checkpoints[0].name\n",
    "        opt_config = selected_checkpoints[0].optimization_config\n",
    "        \n",
    "        # Get methods for this checkpoint\n",
    "        checkpoint_methods = {k: v for k, v in method_performance.items() if v['checkpoint'] == ckpt_name}\n",
    "        \n",
    "        if checkpoint_methods:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            \n",
    "            # Method performance bars\n",
    "            methods = [v['method'] for v in checkpoint_methods.values()]\n",
    "            values = [v['actual_value'] for v in checkpoint_methods.values()]\n",
    "            \n",
    "            # Color based on whether it's the trained policy\n",
    "            colors = ['red' if 'Policy' in m or 'Trained' in m else 'lightblue' for m in methods]\n",
    "            \n",
    "            bars = ax1.bar(range(len(methods)), values, color=colors, alpha=0.7, edgecolor='black')\n",
    "            ax1.set_xticks(range(len(methods)))\n",
    "            ax1.set_xticklabels([m.replace(' + ', '\\n') for m in methods], rotation=45, ha='right')\n",
    "            \n",
    "            # Set appropriate y-label based on optimization direction\n",
    "            if opt_config.is_minimizing:\n",
    "                ax1.set_ylabel('Target Value (Lower is Better)')\n",
    "                ax1.invert_yaxis()  # Invert so lower values appear higher\n",
    "            else:\n",
    "                ax1.set_ylabel('Target Value (Higher is Better)')\n",
    "            \n",
    "            ax1.set_title(f'Method Performance - {ckpt_name}')\n",
    "            ax1.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, val, method_data in zip(bars, values, checkpoint_methods.values()):\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        method_data['display_value'], ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Rankings (best to worst)\n",
    "            if opt_config.is_minimizing:\n",
    "                sorted_methods = sorted(zip(methods, values), key=lambda x: x[1])  # Lower is better\n",
    "            else:\n",
    "                sorted_methods = sorted(zip(methods, values), key=lambda x: x[1], reverse=True)  # Higher is better\n",
    "            \n",
    "            y_pos = range(len(sorted_methods))\n",
    "            \n",
    "            for i, (method, score) in enumerate(sorted_methods):\n",
    "                color = 'red' if 'Policy' in method or 'Trained' in method else 'lightblue'\n",
    "                ax2.barh(i, abs(score), color=color, alpha=0.7, edgecolor='black')\n",
    "                display_text = opt_config.format_improvement(score)\n",
    "                ax2.text(abs(score) + 0.02, i, display_text, va='center', fontsize=8)\n",
    "            \n",
    "            ax2.set_yticks(y_pos)\n",
    "            ax2.set_yticklabels([m[0] for m in sorted_methods])\n",
    "            ax2.set_xlabel('Target Value (Absolute)')\n",
    "            ax2.set_title(f'Method Rankings ({\"Best to Worst\" if opt_config.is_minimizing else \"Best to Worst\"})')\n",
    "            ax2.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            plt.suptitle(f'Evaluation Results - {ckpt_name} ({opt_config.direction})', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot_path = output_dir / \"single_checkpoint_results.png\"\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüíæ Saved plot: {plot_path}\")\n",
    "            \n",
    "    elif EVALUATION_MODE in [\"COMPARE_CHECKPOINTS\", \"COMPARE_OBJECTIVES\"]:\n",
    "        # Multi-checkpoint comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Group by checkpoint\n",
    "        checkpoints_data = {}\n",
    "        for key, perf in method_performance.items():\n",
    "            ckpt = perf['checkpoint']\n",
    "            method = perf['method']\n",
    "            if ckpt not in checkpoints_data:\n",
    "                checkpoints_data[ckpt] = {}\n",
    "            checkpoints_data[ckpt][method] = perf\n",
    "        \n",
    "        # 1. Side-by-side comparison\n",
    "        ax1 = axes[0]\n",
    "        all_methods = set()\n",
    "        for methods in checkpoints_data.values():\n",
    "            all_methods.update(methods.keys())\n",
    "        all_methods = sorted(list(all_methods))\n",
    "        \n",
    "        x = np.arange(len(all_methods))\n",
    "        width = 0.8 / len(checkpoints_data)\n",
    "        \n",
    "        for i, (ckpt_name, methods_data) in enumerate(checkpoints_data.items()):\n",
    "            values = [methods_data.get(m, {}).get('actual_value', 0) for m in all_methods]\n",
    "            offset = (i - len(checkpoints_data)/2 + 0.5) * width\n",
    "            \n",
    "            # Get checkpoint info\n",
    "            ckpt_info = next((c for c in selected_checkpoints if c.name == ckpt_name), None)\n",
    "            if ckpt_info:\n",
    "                label = f\"{ckpt_info.optimization_config.direction}\"\n",
    "            else:\n",
    "                label = ckpt_name\n",
    "            \n",
    "            bars = ax1.bar(x + offset, values, width, label=label, alpha=0.7)\n",
    "        \n",
    "        ax1.set_xlabel('Method')\n",
    "        ax1.set_ylabel('Target Value')\n",
    "        ax1.set_title('Method Performance Across Checkpoints')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels([m.replace(' + ', '\\n') for m in all_methods], rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 2. Optimization direction comparison\n",
    "        if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "            ax2 = axes[1]\n",
    "            \n",
    "            # Group by optimization direction\n",
    "            direction_performance = {'MINIMIZE': [], 'MAXIMIZE': []}\n",
    "            \n",
    "            for ckpt_name, methods_data in checkpoints_data.items():\n",
    "                # Get best trained policy performance\n",
    "                trained_methods = [m for m in methods_data if 'Policy' in m or 'Trained' in m]\n",
    "                if trained_methods:\n",
    "                    best_method = trained_methods[0]\n",
    "                    perf = methods_data[best_method]\n",
    "                    direction = perf['optimization_direction']\n",
    "                    if direction in direction_performance:\n",
    "                        direction_performance[direction].append({\n",
    "                            'checkpoint': ckpt_name,\n",
    "                            'value': perf['actual_value'],\n",
    "                            'display': perf['display_value']\n",
    "                        })\n",
    "            \n",
    "            # Plot comparison\n",
    "            if direction_performance['MINIMIZE'] and direction_performance['MAXIMIZE']:\n",
    "                labels = ['Minimization', 'Maximization']\n",
    "                min_val = np.mean([p['value'] for p in direction_performance['MINIMIZE']])\n",
    "                max_val = np.mean([p['value'] for p in direction_performance['MAXIMIZE']])\n",
    "                \n",
    "                bars = ax2.bar(labels, [min_val, max_val], color=['blue', 'red'], alpha=0.7)\n",
    "                \n",
    "                # Add value labels\n",
    "                ax2.text(0, min_val + 0.01, f\"{min_val:.3f}\\n(Lower better)\", ha='center')\n",
    "                ax2.text(1, max_val + 0.01, f\"{max_val:.3f}\\n(Higher better)\", ha='center')\n",
    "                \n",
    "                ax2.set_ylabel('Average Target Value')\n",
    "                ax2.set_title('Optimization Direction Comparison')\n",
    "                ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 3. Summary statistics\n",
    "        ax_summary = axes[3]\n",
    "        ax_summary.axis('off')\n",
    "        \n",
    "        summary_text = f\"Evaluation Summary\\n{'='*30}\\n\\n\"\n",
    "        summary_text += f\"Mode: {EVALUATION_MODE}\\n\"\n",
    "        summary_text += f\"Checkpoints evaluated: {len(selected_checkpoints)}\\n\"\n",
    "        summary_text += f\"Test SCMs: {NUM_TEST_SCMS}\\n\"\n",
    "        summary_text += f\"Runs per method: {RUNS_PER_METHOD}\\n\\n\"\n",
    "        \n",
    "        for ckpt in selected_checkpoints:\n",
    "            if ckpt.name in checkpoints_data:\n",
    "                summary_text += f\"\\n{ckpt.name}:\\n\"\n",
    "                summary_text += f\"  Optimization: {ckpt.optimization_config.direction}\\n\"\n",
    "                \n",
    "                # Get best trained method\n",
    "                methods = checkpoints_data[ckpt.name]\n",
    "                trained = [m for m in methods.values() if 'Policy' in m['method'] or 'Trained' in m['method']]\n",
    "                if trained:\n",
    "                    best = trained[0]\n",
    "                    summary_text += f\"  Best policy: {best['display_value']}\\n\"\n",
    "        \n",
    "        ax_summary.text(0.05, 0.95, summary_text, transform=ax_summary.transAxes, fontsize=10,\n",
    "                       verticalalignment='top', family='monospace',\n",
    "                       bbox=dict(boxstyle='round,pad=1', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        plt.suptitle('Multi-Checkpoint Comparison Results', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = output_dir / \"checkpoint_comparison_results.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüíæ Saved plot: {plot_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting Results\n",
      "============================================================\n",
      "‚úÖ Results saved to: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618/evaluation_results.json\n",
      "‚úÖ Summary saved to: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618/evaluation_summary.txt\n",
      "\n",
      "üéâ Evaluation Complete!\n",
      "Mode: SINGLE_CHECKPOINT\n",
      "Checkpoints: 1\n",
      "Duration: 0.0 minutes\n",
      "\n",
      "Output directory: /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618\n",
      "\n",
      "üìä Key Insights:\n",
      "- Evaluated grpo_quick_minimize_20250723_101252_fixed with MINIMIZE objective\n",
      "- Compare against baselines to see if training improved performance\n",
      "\n",
      "üí° Next steps:\n",
      "1. Review the plots in /Users/harellidar/Documents/Imperial/Individual_Project/causal_bayes_opt/results/evaluation_single_checkpoint_20250723_203618\n",
      "2. Check the detailed results in evaluation_results.json\n",
      "3. Read the summary report for key findings\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell 7: Export results and generate summary report\n",
    "\n",
    "Save all results for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíæ Exporting Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare export data\n",
    "export_data = {\n",
    "    'evaluation_config': {\n",
    "        'mode': EVALUATION_MODE,\n",
    "        'num_test_scms': NUM_TEST_SCMS,\n",
    "        'runs_per_method': RUNS_PER_METHOD,\n",
    "        'intervention_budget': INTERVENTION_BUDGET,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'timestamp': timestamp\n",
    "    },\n",
    "    'checkpoints_evaluated': [\n",
    "        {\n",
    "            'name': ckpt.name,\n",
    "            'optimization_direction': ckpt.optimization_config.direction,\n",
    "            'path': str(ckpt.path),\n",
    "            'training_mode': ckpt.training_config.get('mode', 'unknown')\n",
    "        }\n",
    "        for ckpt in selected_checkpoints\n",
    "    ],\n",
    "    'results': evaluation_results,\n",
    "    'method_performance': method_performance if 'method_performance' in locals() else {},\n",
    "    'duration_minutes': total_duration if 'total_duration' in locals() else 0\n",
    "}\n",
    "\n",
    "# Save JSON results\n",
    "json_path = output_dir / \"evaluation_results.json\"\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {json_path}\")\n",
    "\n",
    "# Generate text summary\n",
    "summary_path = output_dir / \"evaluation_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"GRPO Evaluation Summary\\n\")\n",
    "    f.write(f\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Evaluation Mode: {EVALUATION_MODE}\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "    f.write(f\"Duration: {total_duration:.1f} minutes\\n\\n\" if 'total_duration' in locals() else \"\\n\")\n",
    "    \n",
    "    f.write(f\"Configuration:\\n\")\n",
    "    f.write(f\"  Test SCMs: {NUM_TEST_SCMS}\\n\")\n",
    "    f.write(f\"  Runs per method: {RUNS_PER_METHOD}\\n\")\n",
    "    f.write(f\"  Intervention budget: {INTERVENTION_BUDGET}\\n\")\n",
    "    f.write(f\"  Random seed: {RANDOM_SEED}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Checkpoints Evaluated:\\n\")\n",
    "    for ckpt in selected_checkpoints:\n",
    "        f.write(f\"  - {ckpt.name} ({ckpt.optimization_config.direction})\\n\")\n",
    "    \n",
    "    f.write(f\"\\nKey Findings:\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Summarize results\n",
    "    if 'method_performance' in locals() and method_performance:\n",
    "        # Group by checkpoint\n",
    "        for ckpt in selected_checkpoints:\n",
    "            f.write(f\"\\n{ckpt.name} ({ckpt.optimization_config.direction}):\\n\")\n",
    "            \n",
    "            # Get methods for this checkpoint\n",
    "            ckpt_methods = {k: v for k, v in method_performance.items() if v['checkpoint'] == ckpt.name}\n",
    "            \n",
    "            if ckpt_methods:\n",
    "                # Sort by performance\n",
    "                if ckpt.optimization_config.is_minimizing:\n",
    "                    sorted_methods = sorted(ckpt_methods.items(), key=lambda x: x[1]['actual_value'])\n",
    "                else:\n",
    "                    sorted_methods = sorted(ckpt_methods.items(), key=lambda x: x[1]['actual_value'], reverse=True)\n",
    "                \n",
    "                for rank, (key, perf) in enumerate(sorted_methods[:5], 1):  # Top 5\n",
    "                    marker = \"*\" if 'Policy' in perf['method'] or 'Trained' in perf['method'] else \" \"\n",
    "                    f.write(f\"  {rank}. {marker} {perf['method']}: {perf['display_value']}\\n\")\n",
    "    \n",
    "    if EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "        f.write(f\"\\n\\nOptimization Direction Insights:\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(\"This evaluation compared minimization vs maximization objectives.\\n\")\n",
    "        f.write(\"Key observation: Different optimization directions require different\\n\")\n",
    "        f.write(\"reward structures and may lead to different exploration strategies.\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\\nOutput Files:\\n\")\n",
    "    f.write(f\"  Results JSON: {json_path.name}\\n\")\n",
    "    f.write(f\"  Summary: {summary_path.name}\\n\")\n",
    "    \n",
    "    # List any plots\n",
    "    plots = list(output_dir.glob(\"*.png\"))\n",
    "    for plot in plots:\n",
    "        f.write(f\"  Plot: {plot.name}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_path}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\nüéâ Evaluation Complete!\")\n",
    "print(f\"Mode: {EVALUATION_MODE}\")\n",
    "print(f\"Checkpoints: {len(selected_checkpoints)}\")\n",
    "print(f\"Duration: {total_duration:.1f} minutes\" if 'total_duration' in locals() else \"\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "\n",
    "print(f\"\\nüìä Key Insights:\")\n",
    "if EVALUATION_MODE == \"SINGLE_CHECKPOINT\":\n",
    "    print(f\"- Evaluated {selected_checkpoints[0].name} with {selected_checkpoints[0].optimization_config.direction} objective\")\n",
    "    print(f\"- Compare against baselines to see if training improved performance\")\n",
    "elif EVALUATION_MODE == \"COMPARE_OBJECTIVES\":\n",
    "    print(f\"- Compared {len(selected_checkpoints)} checkpoints with different optimization directions\")\n",
    "    print(f\"- Minimization policies optimize for lower target values (like PARENT_SCALE)\")\n",
    "    print(f\"- Maximization policies optimize for higher target values\")\n",
    "    print(f\"- Check plots to see which direction performs better for your use case\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"1. Review the plots in {output_dir}\")\n",
    "print(f\"2. Check the detailed results in {json_path.name}\")\n",
    "print(f\"3. Read the summary report for key findings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we've accomplished:**\n",
    "1. ‚úÖ Loaded checkpoints with optimization direction metadata\n",
    "2. ‚úÖ Ran evaluation with proper metric handling\n",
    "3. ‚úÖ Generated visualizations that show min/max correctly\n",
    "4. ‚úÖ Exported comprehensive results\n",
    "\n",
    "**Key improvements over original notebook:**\n",
    "- Checkpoint-first approach - no need to run training\n",
    "- Auto-detects optimization direction from metadata\n",
    "- Correctly displays minimization vs maximization results\n",
    "- All cells are independent and can be re-run\n",
    "- No silent failures - explicit errors throughout\n",
    "\n",
    "**Understanding the results:**\n",
    "- For MINIMIZE checkpoints: Lower target values are better\n",
    "- For MAXIMIZE checkpoints: Higher target values are better\n",
    "- The plots show \"(‚Üì better)\" or \"(‚Üë better)\" to clarify\n",
    "- PARENT_SCALE baseline uses minimization by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-bayes-opt-sr_Vb8Og-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
