#!/usr/bin/env python3
"""
Test Script for TensorBackedAcquisitionState Integration

This script validates that our tensor-backed state migration is working correctly
by running a focused training session and comparing with the old implementation.
"""

import sys
import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add project root to path
project_root = Path.cwd().parent if Path.cwd().name == "experiments" else Path.cwd()
sys.path.insert(0, str(project_root))

# Core imports
import jax
import jax.numpy as jnp
import jax.random as random
import numpy as onp
import pyrsistent as pyr
import yaml
from omegaconf import DictConfig, OmegaConf

# Project imports
from causal_bayes_opt.experiments.variable_scm_factory import VariableSCMFactory
from causal_bayes_opt.training.enriched_trainer import EnrichedGRPOTrainer
from causal_bayes_opt.data_structures.scm import get_variables, get_target
from causal_bayes_opt.jax_native.state import TensorBackedAcquisitionState, create_tensor_backed_state_from_scm

# Configure logging
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

def test_tensor_backend_integration():
    """Test the TensorBackedAcquisitionState integration with GRPO trainer."""
    
    print("üß™ TensorBackedAcquisitionState Integration Test")
    print("=" * 60)
    
    # Configure JAX
    jax.config.update("jax_enable_x64", True)
    
    # Create test SCMs (all with same number of variables for GRPO testing)
    scm_factory = VariableSCMFactory(noise_scale=0.1, seed=42)
    test_scms = []
    
    # Use only 3-variable SCMs for consistent tensor shapes
    for structure_type in ['fork', 'chain', 'fork', 'chain']:  # Create 4 SCMs
        scm = scm_factory.create_variable_scm(
            num_variables=3,
            structure_type=structure_type,
            target_variable=None,
            edge_density=0.5
        )
        test_scms.append(scm)
    
    print(f"‚úÖ Created {len(test_scms)} test SCMs")
    
    # Test 1: TensorBackedAcquisitionState creation from SCM
    print("\nüî¨ Test 1: TensorBackedAcquisitionState Factory")
    
    for i, scm in enumerate(test_scms[:2]):  # Test first 2 SCMs
        variables = list(get_variables(scm))
        target = get_target(scm)
        
        try:
            # Create tensor-backed state
            tensor_state = create_tensor_backed_state_from_scm(
                scm=scm,
                step=i * 5,
                best_value=float(i) * 0.3,
                uncertainty_bits=1.0 + i * 0.2
            )
            
            print(f"  SCM {i}: ‚úÖ Created tensor state")
            print(f"    Variables: {tensor_state.variable_names}")
            print(f"    Target: {tensor_state.current_target}")
            print(f"    Config: {tensor_state.config.n_vars} vars, {tensor_state.config.feature_dim} features")
            
            # Test AcquisitionState interface compatibility
            posterior = tensor_state.posterior
            buffer = tensor_state.buffer
            step = tensor_state.step
            best_value = tensor_state.best_value
            
            print(f"    Interface ‚úÖ: posterior={type(posterior).__name__}, buffer={type(buffer).__name__}")
            
        except Exception as e:
            print(f"  SCM {i}: ‚ùå Failed - {e}")
            raise
    
    # Test 2: GRPO Integration
    print("\nüî¨ Test 2: GRPO Integration with TensorBackedAcquisitionState")
    
    from causal_bayes_opt.acquisition.grpo import _extract_policy_input_from_tensor_state, GRPOConfig
    
    try:
        # Create test batch of tensor states
        batch_states = [
            create_tensor_backed_state_from_scm(scm, step=i, best_value=float(i)*0.4)
            for i, scm in enumerate(test_scms[:4])
        ]
        
        # Test policy input extraction (core GRPO functionality)
        policy_inputs = [_extract_policy_input_from_tensor_state(state) for state in batch_states]
        policy_input_batch = jnp.stack(policy_inputs)
        
        print(f"  ‚úÖ Policy input extraction successful")
        print(f"    Batch shape: {policy_input_batch.shape}")
        print(f"    Individual input shape: {policy_inputs[0].shape}")
        
        # Test GRPO config compatibility
        grpo_config = GRPOConfig(group_size=4, learning_rate=0.001)
        print(f"  ‚úÖ GRPO config: group_size={grpo_config.group_size}")
        
        # Test mock GRPO loss computation structure
        from causal_bayes_opt.interventions.handlers import create_perfect_intervention
        
        mock_batch = {
            'states': batch_states,
            'actions': [
                create_perfect_intervention(targets={'X0'}, values={'X0': float(i)})
                for i in range(4)
            ],
            'rewards': jnp.array([0.2, 0.6, 0.4, 0.8]),
            'old_log_probs': jnp.array([-1.0, -0.8, -1.2, -0.6])
        }
        
        print(f"  ‚úÖ Mock GRPO batch created successfully")
        print(f"    States: {len(mock_batch['states'])} TensorBackedAcquisitionState objects")
        print(f"    Actions: {len(mock_batch['actions'])} interventions")
        
    except Exception as e:
        print(f"  ‚ùå GRPO integration failed - {e}")
        raise
    
    # Test 3: Training Integration
    print("\nüî¨ Test 3: Training Integration Test")
    
    # Create lightweight training config for testing
    test_config = OmegaConf.create({
        'seed': 42,
        'training': {
            'n_episodes': 20,  # Short test
            'episode_length': 5,
            'learning_rate': 0.001,
            'gamma': 0.99,
            'max_intervention_value': 2.0,
            'reward_weights': {
                'optimization': 0.5,
                'discovery': 0.3,
                'efficiency': 0.2
            },
            'architecture': {
                'hidden_dim': 64,
                'num_layers': 2,
                'num_heads': 4,
                'key_size': 32,
                'widening_factor': 4,
                'dropout': 0.1,
                'policy_intermediate_dim': None
            },
            'state_config': {
                'max_history_size': 50,
                'num_channels': 10,
                'standardize_values': True,
                'include_temporal_features': True
            },
            'grpo_config': {
                'group_size': 16,
                'interventions_per_state': 4,
                'clip_ratio': 0.2,
                'entropy_coeff': 0.01,
                'kl_penalty_coeff': 0.0,
                'max_grad_norm': 1.0,
                'scale_rewards': True
            }
        },
        'experiment': {
            'scm_generation': {
                'use_variable_factory': True,
                'variable_range': [3, 4],
                'structure_types': ['fork', 'chain'],
                'rotation_frequency': 5
            }
        },
        'logging': {
            'checkpoint_dir': str(project_root / "checkpoints" / "tensor_test"),
            'wandb': {'enabled': False},
            'level': 'INFO'
        }
    })
    
    try:
        print("  üöÄ Initializing trainer with tensor-backed states...")
        
        # Initialize trainer (this tests the full integration)
        trainer = EnrichedGRPOTrainer(config=test_config)
        print("  ‚úÖ Trainer initialization successful")
        
        # Quick training run to verify everything works
        print("  üèÉ Running short training session...")
        start_time = time.time()
        
        metrics = trainer.train()
        
        end_time = time.time()
        training_time = end_time - start_time
        
        print(f"  ‚úÖ Training completed in {training_time:.1f}s")
        
        # Analyze results
        performance = metrics.get('performance', {})
        final_reward = performance.get('final_reward', 0)
        mean_reward = performance.get('mean_reward', 0)
        improvement = performance.get('reward_improvement', 0)
        
        print(f"\nüìä Training Results:")
        print(f"  Final reward: {final_reward:.3f}")
        print(f"  Mean reward: {mean_reward:.3f}")
        print(f"  Improvement: {improvement:.3f}")
        print(f"  Episodes completed: {performance.get('total_episodes', 0)}")
        
        # Test trained policy
        sample_scm = test_scms[0]
        variables = list(get_variables(sample_scm))
        target = get_target(sample_scm)
        
        # Create test state using the new factory
        test_state = trainer._create_tensor_backed_state(sample_scm, 0, 0.0)
        enriched_input = trainer.state_converter.convert_state_to_enriched_input(test_state)
        target_idx = variables.index(target) if target in variables else 0
        
        # Get policy output
        key = random.PRNGKey(42)
        policy_output = trainer.policy_fn.apply(
            trainer.policy_params, key, enriched_input, target_idx, False
        )
        
        # Test action generation
        action = trainer._policy_output_to_action(policy_output, variables, target)
        intervention, reward = trainer._simulate_intervention(sample_scm, action)
        
        max_action = float(jnp.max(jnp.abs(action)))
        n_interventions = len(intervention.get('targets', set()))
        
        print(f"\nüß™ Policy Test:")
        print(f"  Test SCM: {variables} (target: {target})")
        print(f"  Max action magnitude: {max_action:.6f}")
        print(f"  Interventions triggered: {n_interventions}")
        print(f"  Test reward: {reward:.3f}")
        
        # Success criteria
        training_successful = final_reward > 0.1 and training_time < 60
        policy_active = max_action > 0.01 and n_interventions > 0
        no_errors = True  # If we got here, no major errors occurred
        
        print(f"\nüéØ Integration Test Results:")
        print(f"  ‚úÖ TensorBackedAcquisitionState creation: PASS")
        print(f"  ‚úÖ GRPO tensor integration: PASS")
        print(f"  {'‚úÖ' if training_successful else '‚ö†Ô∏è'} Training execution: {'PASS' if training_successful else 'PARTIAL'}")
        print(f"  {'‚úÖ' if policy_active else '‚ö†Ô∏è'} Policy activity: {'PASS' if policy_active else 'PARTIAL'}")
        print(f"  ‚úÖ No major errors: PASS")
        
        overall_success = no_errors and (training_successful or policy_active)
        
        if overall_success:
            print(f"\nüéâ TENSOR BACKEND INTEGRATION: SUCCESS!")
            print(f"‚úÖ All systems working with TensorBackedAcquisitionState")
            print(f"‚úÖ Ready for production use and full evaluation")
        else:
            print(f"\n‚ö†Ô∏è TENSOR BACKEND INTEGRATION: PARTIAL SUCCESS")
            print(f"Core functionality works but may need optimization")
        
        return overall_success
        
    except Exception as e:
        print(f"  ‚ùå Training integration failed - {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_tensor_backend_integration()
    
    if success:
        print(f"\nüöÄ NEXT STEPS:")
        print(f"1. Run full evaluation with training_to_evaluation_pipeline.ipynb")
        print(f"2. Compare performance against baseline methods")
        print(f"3. Deploy in production with confidence")
        
        exit_code = 0
    else:
        print(f"\nüîß NEXT STEPS:")
        print(f"1. Review error logs and fix remaining issues")
        print(f"2. Run focused debugging on failing components")
        print(f"3. Retest before full evaluation")
        
        exit_code = 1
    
    sys.exit(exit_code)