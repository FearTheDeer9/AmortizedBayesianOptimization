{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Evaluation Framework Demo\n",
    "\n",
    "This notebook demonstrates how to use the new unified evaluation framework to compare different causal Bayesian optimization methods.\n",
    "\n",
    "## Key Features:\n",
    "- Single interface for all evaluation methods (GRPO, BC, baselines)\n",
    "- Standardized result format for easy comparison\n",
    "- Built-in visualization and analysis tools\n",
    "- Parallel execution support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import unified evaluation components\n",
    "from src.causal_bayes_opt.evaluation import (\n",
    "    setup_evaluation_runner,\n",
    "    run_evaluation_comparison,\n",
    "    results_to_dataframe,\n",
    "    plot_learning_curves,\n",
    "    create_summary_report\n",
    ")\n",
    "\n",
    "# Import SCM creators\n",
    "from examples.demo_scms import (\n",
    "    create_easy_scm_base,\n",
    "    create_medium_scm,\n",
    "    create_hard_scm\n",
    ")\n",
    "\n",
    "print(\"Unified evaluation framework loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Test SCMs\n",
    "\n",
    "We'll use three SCMs of varying difficulty for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test SCMs\n",
    "test_scms = [\n",
    "    create_easy_scm_base(),\n",
    "    create_medium_scm(),\n",
    "    create_hard_scm()\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_scms)} test SCMs\")\n",
    "\n",
    "# Print SCM info\n",
    "from src.causal_bayes_opt.data_structures.scm import get_target, get_variables, get_parents\n",
    "\n",
    "for i, scm in enumerate(test_scms):\n",
    "    target = get_target(scm)\n",
    "    variables = list(get_variables(scm))\n",
    "    parents = list(get_parents(scm, target))\n",
    "    print(f\"\\nSCM {i} ({'easy' if i==0 else 'medium' if i==1 else 'hard'}):\")\n",
    "    print(f\"  Variables: {len(variables)}\")\n",
    "    print(f\"  Target: {target}\")\n",
    "    print(f\"  True parents: {parents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Evaluation\n",
    "\n",
    "Set up the evaluation configuration that will be used for all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "eval_config = {\n",
    "    'experiment': {\n",
    "        'target': {\n",
    "            'max_interventions': 15,\n",
    "            'n_observational_samples': 100,\n",
    "            'intervention_value_range': (-2.0, 2.0),\n",
    "            'learning_rate': 1e-3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Number of random seeds per SCM\n",
    "n_seeds = 5\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Max interventions: {eval_config['experiment']['target']['max_interventions']}\")\n",
    "print(f\"  Observational samples: {eval_config['experiment']['target']['n_observational_samples']}\")\n",
    "print(f\"  Seeds per SCM: {n_seeds}\")\n",
    "print(f\"  Total runs: {len(test_scms) * n_seeds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Baseline Methods Comparison\n",
    "\n",
    "First, let's compare the three baseline methods:\n",
    "- Random: Uniform random intervention selection\n",
    "- Learning: Online structure learning with simple policy\n",
    "- Oracle: Perfect knowledge of causal structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation runner with baseline methods\n",
    "baseline_runner = setup_evaluation_runner(\n",
    "    methods=['random', 'learning', 'oracle'],\n",
    "    parallel=True  # Enable parallel execution\n",
    ")\n",
    "\n",
    "print(\"Running baseline comparison...\")\n",
    "\n",
    "# Run evaluation\n",
    "baseline_results = run_evaluation_comparison(\n",
    "    runner=baseline_runner,\n",
    "    test_scms=test_scms,\n",
    "    config=eval_config,\n",
    "    n_seeds=n_seeds\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Baseline Results\n",
    "\n",
    "Let's look at the performance of each baseline method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "baseline_df = results_to_dataframe(baseline_results)\n",
    "\n",
    "print(\"Baseline Method Performance:\")\n",
    "print(baseline_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Create bar plot of mean improvements\n",
    "plt.figure(figsize=(10, 6))\n",
    "methods = baseline_df['method'].values\n",
    "improvements = baseline_df['mean_improvement'].values\n",
    "errors = baseline_df['std_improvement'].values\n",
    "\n",
    "bars = plt.bar(methods, improvements, yerr=errors, capsize=10)\n",
    "plt.ylabel('Mean Target Improvement')\n",
    "plt.title('Baseline Method Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Color bars based on performance\n",
    "colors = ['red' if x > 0 else 'green' for x in improvements]\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Learning Curves\n",
    "\n",
    "Visualize how each method learns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for each SCM\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "scm_names = ['Easy SCM', 'Medium SCM', 'Hard SCM']\n",
    "\n",
    "for scm_idx, (ax, scm_name) in enumerate(zip(axes, scm_names)):\n",
    "    # Plot on current axes\n",
    "    plt.sca(ax)\n",
    "    plot_learning_curves(\n",
    "        baseline_results, \n",
    "        scm_idx=scm_idx,\n",
    "        metric='outcome_value',\n",
    "        title=f'{scm_name} - Target Value'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 score evolution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for scm_idx, (ax, scm_name) in enumerate(zip(axes, scm_names)):\n",
    "    plt.sca(ax)\n",
    "    plot_learning_curves(\n",
    "        baseline_results,\n",
    "        scm_idx=scm_idx, \n",
    "        metric='f1_score',\n",
    "        title=f'{scm_name} - F1 Score'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Summary Report\n",
    "\n",
    "Create a comprehensive text report of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary report\n",
    "report = create_summary_report(baseline_results)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with GRPO/BC Methods (if checkpoints available)\n",
    "\n",
    "This section demonstrates how to include GRPO and BC methods in the comparison.\n",
    "You'll need to provide paths to trained checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Setup runner with all methods including GRPO/BC\n",
    "# NOTE: Update these paths to your actual checkpoint locations\n",
    "\n",
    "# checkpoint_paths = {\n",
    "#     'grpo': Path('path/to/grpo/checkpoint'),\n",
    "#     'bc_surrogate': Path('path/to/bc/surrogate/checkpoint'),\n",
    "#     'bc_acquisition': Path('path/to/bc/acquisition/checkpoint')\n",
    "# }\n",
    "\n",
    "# full_runner = setup_evaluation_runner(\n",
    "#     methods=['random', 'learning', 'oracle', 'grpo', 'bc_surrogate', 'bc_acquisition', 'bc_both'],\n",
    "#     checkpoint_paths=checkpoint_paths,\n",
    "#     parallel=True\n",
    "# )\n",
    "\n",
    "# full_results = run_evaluation_comparison(\n",
    "#     runner=full_runner,\n",
    "#     test_scms=test_scms,\n",
    "#     config=eval_config,\n",
    "#     n_seeds=n_seeds,\n",
    "#     output_dir=Path('evaluation_results')  # Save results\n",
    "# )\n",
    "\n",
    "print(\"To compare with GRPO/BC methods, uncomment the code above and provide checkpoint paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis\n",
    "\n",
    "Perform statistical tests to determine if differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access statistical test results\n",
    "if baseline_results.statistical_tests:\n",
    "    print(\"Statistical Test Results:\")\n",
    "    for test_name, test_result in baseline_results.statistical_tests.items():\n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(f\"  Statistic: {test_result.get('statistic', 'N/A'):.3f}\")\n",
    "        print(f\"  P-value: {test_result.get('p_value', 'N/A'):.4f}\")\n",
    "        print(f\"  Significant (p<0.05): {'Yes' if test_result.get('p_value', 1.0) < 0.05 else 'No'}\")\n",
    "else:\n",
    "    print(\"No statistical tests were performed (need at least 2 methods).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Results\n",
    "\n",
    "Demonstrate how to save results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = Path('evaluation_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_path = output_dir / f'baseline_results_{timestamp}.pkl'\n",
    "\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(baseline_results, f)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Example: Load and visualize saved results\n",
    "# from src.causal_bayes_opt.evaluation import load_and_visualize_results\n",
    "# loaded_results = load_and_visualize_results(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the unified evaluation framework for causal Bayesian optimization:\n",
    "\n",
    "1. **Simple Setup**: Use `setup_evaluation_runner()` to configure methods\n",
    "2. **Consistent Interface**: All methods (GRPO, BC, baselines) use the same API\n",
    "3. **Standardized Results**: Results are in a common format for easy comparison\n",
    "4. **Built-in Analysis**: Includes visualization and statistical testing tools\n",
    "5. **Extensible**: Easy to add new evaluation methods by implementing `BaseEvaluator`\n",
    "\n",
    "The framework makes it straightforward to:\n",
    "- Compare different methods fairly\n",
    "- Analyze performance across multiple metrics\n",
    "- Generate publication-ready plots and tables\n",
    "- Save and share results\n",
    "\n",
    "For production use with GRPO/BC methods, simply provide the checkpoint paths when setting up the runner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}