# Component Registry

This document provides a comprehensive overview of the components available in the `causal_meta` package, their functionality, interfaces, and recommended usage patterns. The goal is to reduce code duplication and improve collaboration by ensuring that all team members understand the existing tools before implementing new ones.

## IMPORTANT: READ BEFORE WRITING NEW CODE

**This registry is the authoritative source for understanding the components available in the codebase.** Before implementing new functionality:

1. **ALWAYS check this registry first** to see if a component already exists that meets your needs or can be extended
2. **Follow the interfaces and patterns** documented here for consistent implementation
3. **Update this registry** when adding new components or making significant changes to existing ones
4. **Reference this document** when designing integration between components

Following these guidelines will significantly reduce code duplication, improve integration, and make the codebase more maintainable.

## Core Structure

The `causal_meta` package is organized into the following main modules:

1. **graph**: Core graph representations and generation utilities
2. **environments**: Structural Causal Models (SCMs) and intervention mechanisms
3. **meta_learning**: Amortized causal discovery and meta-learning components
4. **discovery**: Causal discovery algorithms
5. **optimization**: Optimization algorithms for causal systems
6. **inference**: Inference algorithms for causal models
7. **utils**: Utility functions and visualization tools

## Graph Module (`causal_meta.graph`)

### CausalGraph (`causal_meta.graph.causal_graph.CausalGraph`)

**Purpose**: Represents a causal graph structure (Directed Acyclic Graph - DAG) with nodes representing variables and edges representing causal relationships.

**Key Methods**:
- `add_node(node_id, **attrs)`: Add a node to the graph
- `add_edge(source, target, **attrs)`: Add a directed edge from source to target
- `remove_node(node_id)`: Remove a node and all its connected edges
- `remove_edge(source, target)`: Remove an edge from the graph
- `get_nodes()`: Get all nodes in the graph
- `get_edges()`: Get all edges in the graph
- `get_parents(node)`: Get parents of a node
- `get_children(node)`: Get children of a node
- `get_adjacency_matrix()`: Get the adjacency matrix representation of the graph
- `is_acyclic()`: Check if the graph is acyclic
- `to_networkx()`: Convert to a NetworkX graph
- `from_networkx(nx_graph)`: Create a CausalGraph from a NetworkX graph
- `copy()`: Create a deep copy of the graph

**Usage Example**:
```python
from causal_meta.graph.causal_graph import CausalGraph

# Create a new causal graph
graph = CausalGraph()
graph.add_node("X")
graph.add_node("Y")
graph.add_node("Z")
graph.add_edge("X", "Y")
graph.add_edge("Y", "Z")

# Check properties
parents_of_y = graph.get_parents("Y")  # ["X"]
is_dag = graph.is_acyclic()  # True

# Get adjacency matrix
adj_matrix = graph.get_adjacency_matrix()
```

### DirectedGraph (`causal_meta.graph.directed_graph.DirectedGraph`)

**Purpose**: Base class for directed graphs, providing common functionality for graph manipulation. The `CausalGraph` class extends this with causality-specific methods.

**Usage Note**: In most cases, use `CausalGraph` directly rather than `DirectedGraph`.

### TaskFamily (`causal_meta.graph.task_family.TaskFamily`)

**Purpose**: Represents a family of related causal tasks, typically generated by varying a base graph.

**Key Methods**:
- `__init__(base_graph, variations, metadata)`: Constructor
- `get_base_graph()`: Get the base graph of the family
- `get_variations()`: Get the list of variation graphs
- `save(filepath)`: Save the task family to a file
- `load(filepath)`: Load a task family from a file

**Usage Example**:
```python
from causal_meta.graph.task_family import TaskFamily
from causal_meta.graph.generators.factory import GraphFactory
from causal_meta.graph.generators.task_families import generate_task_family

# Create a base graph
base_graph = GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3)

# Generate a task family
variations = generate_task_family(
    base_graph, 
    num_tasks=10, 
    variation_type="edge_weight",
    variation_strength=0.2
)

# Create a TaskFamily object
task_family = TaskFamily(
    base_graph=base_graph,
    variations=variations,
    metadata={"family_type": "edge_weight_variation", "num_tasks": 10}
)

# Save the task family
task_family.save("path/to/task_family.pkl")
```

### Graph Generators (`causal_meta.graph.generators`)

#### GraphFactory (`causal_meta.graph.generators.factory.GraphFactory`)

**Purpose**: Creates different types of causal graph structures.

**Key Methods**:
- `create_random_dag(num_nodes, edge_probability)`: Create a random DAG
- `create_chain_graph(num_nodes)`: Create a chain graph
- `create_tree_graph(num_nodes, branching_factor)`: Create a tree graph
- `create_scale_free_graph(num_nodes, alpha, beta, gamma)`: Create a scale-free graph
- `create_erdos_renyi_graph(num_nodes, edge_probability)`: Create an Erdős–Rényi random graph

**Usage Example**:
```python
from causal_meta.graph.generators.factory import GraphFactory

# Create a random DAG
random_dag = GraphFactory.create_random_dag(num_nodes=10, edge_probability=0.2)

# Create a chain graph
chain_graph = GraphFactory.create_chain_graph(num_nodes=5)
```

#### Task Family Generation (`causal_meta.graph.generators.task_families`)

**Purpose**: Creates families of related causal graphs by introducing variations to a base graph.

**Key Functions**:
- `generate_task_family(base_graph, num_tasks, variation_type, variation_strength)`: Generate a family of related graphs
- `generate_edge_weight_variations(base_graph, num_variations, variation_strength)`: Generate variations by modifying edge weights
- `generate_structure_variations(base_graph, num_variations, variation_strength)`: Generate variations by modifying graph structure

**Usage Example**:
```python
from causal_meta.graph.generators.factory import GraphFactory
from causal_meta.graph.generators.task_families import generate_task_family

# Create a base graph
base_graph = GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3)

# Generate a task family with edge weight variations
edge_weight_variations = generate_task_family(
    base_graph, 
    num_tasks=10, 
    variation_type="edge_weight",
    variation_strength=0.2
)

# Generate a task family with structure variations
structure_variations = generate_task_family(
    base_graph, 
    num_tasks=10, 
    variation_type="structure",
    variation_strength=0.3
)
```

### Visualization (`causal_meta.graph.visualization`)

**Purpose**: Provides visualization utilities for causal graphs and task families.

**Key Functions**:
- `plot_graph(graph, **kwargs)`: Plot a causal graph
- `plot_graphs_comparison(graphs, titles, **kwargs)`: Plot multiple graphs for comparison
- `plot_adjacency_matrix(adj_matrix, **kwargs)`: Plot an adjacency matrix
- `plot_adjacency_matrices_comparison(adj_matrices, titles, **kwargs)`: Plot multiple adjacency matrices
- `plot_intervention_effect(pre_intervention_data, post_intervention_data, intervention_node, **kwargs)`: Visualize intervention effects

**Usage Example**:
```python
from causal_meta.graph.visualization import plot_graph, plot_graphs_comparison
from causal_meta.graph.generators.factory import GraphFactory

# Create a graph
graph = GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3)

# Plot a single graph
plot_graph(graph, title="Random DAG")

# Plot multiple graphs
graphs = [
    GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3),
    GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3)
]
plot_graphs_comparison(graphs, titles=["Graph 1", "Graph 2"])
```

## Environments Module (`causal_meta.environments`)

### StructuralCausalModel (`causal_meta.environments.scm.StructuralCausalModel`)

**Purpose**: Implements a Structural Causal Model (SCM) based on a causal graph, defining structural equations for each node and allowing data generation under observational and interventional scenarios.

**Key Methods**:
- `__init__(graph, structural_equations, noise_distributions)`: Constructor
- `sample_data(n_samples)`: Generate observational data
- `do_intervention(target_node, value)`: Perform a perfect intervention
- `soft_intervention(target_node, func, strength)`: Perform a soft intervention
- `sample_interventional_data(interventions, n_samples)`: Generate interventional data
- `get_causal_graph()`: Get the underlying causal graph
- `get_adjacency_matrix()`: Get the adjacency matrix of the causal graph

**Usage Example**:
```python
from causal_meta.environments.scm import StructuralCausalModel
from causal_meta.graph.generators.factory import GraphFactory

# Create a graph
graph = GraphFactory.create_random_dag(num_nodes=3, edge_probability=0.4)

# Define structural equations
def f_x(noise):
    return noise

def f_y(x, noise):
    return 2 * x + noise

def f_z(y, noise):
    return 0.5 * y + noise

# Create noise distributions
import numpy as np
noise_dists = {
    "X": lambda n: np.random.normal(0, 1, n),
    "Y": lambda n: np.random.normal(0, 0.5, n),
    "Z": lambda n: np.random.normal(0, 0.1, n)
}

# Create the SCM
structural_equations = {
    "X": f_x,
    "Y": f_y,
    "Z": f_z
}

scm = StructuralCausalModel(graph, structural_equations, noise_dists)

# Sample observational data
obs_data = scm.sample_data(n_samples=1000)

# Perform an intervention and sample interventional data
int_scm = scm.do_intervention("X", 2.0)
int_data = int_scm.sample_data(n_samples=1000)
```

### Interventions (`causal_meta.environments.interventions`)

**Purpose**: Defines different types of interventions on SCMs.

**Key Classes**:
- `Intervention`: Abstract base class for interventions
- `PerfectIntervention`: Implements the do-operator (sets node to fixed value, removes parents)
- `SoftIntervention`: Modifies the structural equation
- `ImperfectIntervention`: Modifies node value based on original value, intervention value, and strength

**Usage Example**:
```python
from causal_meta.environments.interventions import PerfectIntervention, SoftIntervention
from causal_meta.environments.scm import StructuralCausalModel

# Create an SCM (see previous example)
# ...

# Perfect intervention
perfect_int = PerfectIntervention(target_node="X", value=2.0)
int_scm = perfect_int.apply(scm)
int_data = int_scm.sample_data(n_samples=1000)

# Soft intervention
def new_mechanism(x, noise):
    return 0.5 * x + noise

soft_int = SoftIntervention(target_node="Y", new_mechanism=new_mechanism)
soft_int_scm = soft_int.apply(scm)
soft_int_data = soft_int_scm.sample_data(n_samples=1000)
```

### Mechanisms (`causal_meta.environments.mechanisms`)

**Purpose**: Provides utilities for creating and managing structural equations for SCMs.

**Key Functions**:
- `create_linear_mechanism(parents, weights)`: Create a linear mechanism
- `create_nonlinear_mechanism(parents, function)`: Create a nonlinear mechanism
- `create_additive_noise_mechanism(parents, function)`: Create a mechanism with additive noise

**Usage Example**:
```python
from causal_meta.environments.mechanisms import create_linear_mechanism

# Create a linear mechanism for a node with two parents
mechanism = create_linear_mechanism(parents=["X", "Y"], weights=[1.5, -0.7])
```

## Meta Learning Module (`causal_meta.meta_learning`)

### Amortized Causal Discovery (`causal_meta.meta_learning.amortized_causal_discovery`)

**Purpose**: Implements neural network-based amortized causal discovery, combining graph structure inference and dynamics modeling.

**Key Classes**:
- `AmortizedCausalDiscovery`: Main class that integrates GraphEncoder and DynamicsDecoder

**Key Methods**:
- `__init__(graph_encoder_params, dynamics_decoder_params)`: Constructor
- `forward(data)`: Forward pass through both components
- `train(train_loader, val_loader, **kwargs)`: Train the model
- `infer_causal_graph(data)`: Infer a causal graph from data
- `predict_intervention_outcomes(data, interventions)`: Predict outcomes of interventions
- `save(path)`: Save the model
- `load(path)`: Load a model from a file

**Usage Example**:
```python
from causal_meta.meta_learning import AmortizedCausalDiscovery
from causal_meta.meta_learning.data_generation import SyntheticDataGenerator

# Create data generator
data_gen = SyntheticDataGenerator(...)

# Create datasets
train_loader, val_loader = data_gen.create_dataloaders(...)

# Initialize model
model = AmortizedCausalDiscovery(
    graph_encoder_params={
        "input_dim": 10,
        "hidden_dim": 64,
        "num_layers": 3
    },
    dynamics_decoder_params={
        "hidden_dim": 64,
        "num_layers": 2
    }
)

# Train the model
model.train(train_loader, val_loader, epochs=100)

# Infer a causal graph
graph = model.infer_causal_graph(test_data)

# Predict intervention outcomes
outcomes = model.predict_intervention_outcomes(test_data, interventions={"X": 2.0})
```

### GraphEncoder (`causal_meta.meta_learning.acd_models.GraphEncoder`)

**Purpose**: Neural network component for inferring causal structure from observational and interventional data.

**Key Methods**:
- `__init__(input_dim, hidden_dim, num_layers, **kwargs)`: Constructor
- `forward(data)`: Compute edge probabilities for a batch of data
- `compute_sparsity_loss(adj_matrices)`: Compute sparsity regularization
- `compute_acyclicity_loss(adj_matrices)`: Compute acyclicity regularization

**Usage Example**:
```python
from causal_meta.meta_learning.acd_models import GraphEncoder
import torch

# Initialize encoder
encoder = GraphEncoder(
    input_dim=10,
    hidden_dim=64,
    num_layers=3
)

# Create dummy data
data = torch.randn(32, 100, 10)  # (batch_size, seq_len, features)

# Forward pass
edge_probs = encoder(data)
```

### DynamicsDecoder (`causal_meta.meta_learning.dynamics_decoder.DynamicsDecoder`)

**Purpose**: Neural network component for predicting intervention outcomes based on inferred graph structure.

**Key Methods**:
- `__init__(hidden_dim, num_layers, **kwargs)`: Constructor
- `forward(data, graph, interventions)`: Predict outcomes under interventions
- `predict_with_uncertainty(data, graph, interventions)`: Predict with uncertainty estimates

**Usage Example**:
```python
from causal_meta.meta_learning.dynamics_decoder import DynamicsDecoder
import torch

# Initialize decoder
decoder = DynamicsDecoder(
    hidden_dim=64,
    num_layers=2
)

# Create dummy data
data = torch.randn(32, 100, 10)  # (batch_size, seq_len, features)
graph = torch.randn(32, 10, 10)  # (batch_size, nodes, nodes)
interventions = {3: 2.0}  # Intervene on node 3, set to 2.0

# Forward pass
predictions = decoder(data, graph, interventions)
```

### Meta-Learning Components (`causal_meta.meta_learning.meta_learning`)

**Purpose**: Implements meta-learning techniques for amortized causal discovery, enabling few-shot adaptation to new tasks.

**Key Classes**:
- `TaskEmbedding`: Converts causal graphs to numerical embeddings
- `MAMLForCausalDiscovery`: Implements MAML for causal discovery

**Key Methods**:
- `TaskEmbedding.embed(graph)`: Convert a graph to a numerical embedding
- `MAMLForCausalDiscovery.meta_train(task_batch)`: Meta-train across a batch of tasks
- `MAMLForCausalDiscovery.adapt(task, n_gradient_steps)`: Adapt to a new task

**Usage Example**:
```python
from causal_meta.meta_learning.meta_learning import TaskEmbedding, MAMLForCausalDiscovery
from causal_meta.graph.generators.factory import GraphFactory

# Create a graph
graph = GraphFactory.create_random_dag(num_nodes=5, edge_probability=0.3)

# Create embedding
embedder = TaskEmbedding(input_dim=5, output_dim=32)
embedding = embedder.embed(graph)

# MAML with a causal discovery model
from causal_meta.meta_learning.amortized_causal_discovery import AmortizedCausalDiscovery

# Create a model
model = AmortizedCausalDiscovery(...)

# Create MAML wrapper
maml = MAMLForCausalDiscovery(model, inner_lr=0.01)

# Meta-train (simplified example)
maml.meta_train(tasks=task_batch)

# Adapt to a new task
adapted_model = maml.adapt(new_task, n_gradient_steps=5)
```

### AmortizedCBO (`causal_meta.meta_learning.amortized_cbo.AmortizedCBO`)

**Purpose**: Implements Causal Bayesian Optimization using amortized neural components.

**Key Methods**:
- `__init__(model, acquisition_function, **kwargs)`: Constructor
- `optimize(initial_data, budget, **kwargs)`: Run the optimization loop
- `select_intervention(data, graph)`: Select the next intervention
- `update_model(data)`: Update the model with new observations

**Usage Example**:
```python
from causal_meta.meta_learning.amortized_cbo import AmortizedCBO, ExpectedImprovement
from causal_meta.meta_learning.amortized_causal_discovery import AmortizedCausalDiscovery

# Create a model
model = AmortizedCausalDiscovery(...)

# Create acquisition function
acq_func = ExpectedImprovement()

# Create CBO optimizer
cbo = AmortizedCBO(model=model, acquisition_function=acq_func)

# Run optimization
results = cbo.optimize(
    initial_data=initial_data,
    budget=10,
    target_node="Y",
    maximize=True
)
```

### Visualization (`causal_meta.meta_learning.visualization`)

**Purpose**: Provides specialized visualization utilities for neural causal discovery and optimization results, complementing the basic graph visualization utilities in `causal_meta.graph.visualization`.

**Key Functions**:
- `plot_graph_inference_results(true_graph, inferred_graph, **kwargs)`: Visualize comparison between ground truth and inferred causal graphs
- `plot_intervention_outcomes(observational_data, intervention_data, predictions, intervention_targets, **kwargs)`: Visualize intervention effects and model predictions
- `plot_optimization_progress(objective_values, best_values, iterations, **kwargs)`: Track optimization progress over iterations
- `plot_performance_comparison(metrics_dict, comparison_key, **kwargs)`: Compare performance metrics across different methods
- `plot_uncertainty(predictions, uncertainty, **kwargs)`: Visualize predictions with uncertainty estimates

**Usage Example**:
```python
from causal_meta.meta_learning.visualization import (
    plot_graph_inference_results,
    plot_intervention_outcomes,
    plot_optimization_progress,
    plot_performance_comparison,
    plot_uncertainty
)

# Visualize graph inference results
plot_graph_inference_results(
    true_graph=ground_truth_graph,
    inferred_graph=model_inferred_graph,
    title="Causal Discovery Performance"
)

# Visualize intervention outcomes
plot_intervention_outcomes(
    observational_data=obs_data,
    intervention_data=int_data,
    predictions=model_predictions,
    intervention_targets={"X1": 0.5},
    title="Intervention Effects and Predictions"
)

# Track optimization progress
plot_optimization_progress(
    objective_values=obj_history,
    best_values=best_obj_history,
    iterations=range(len(obj_history)),
    title="Optimization Convergence"
)

# Compare methods
metrics = {
    "Method1": {"SHD": 2.3, "F1": 0.85, "Runtime": 10.2},
    "Method2": {"SHD": 1.8, "F1": 0.92, "Runtime": 15.7},
    "Method3": {"SHD": 3.1, "F1": 0.78, "Runtime": 8.5}
}
plot_performance_comparison(
    metrics_dict=metrics,
    comparison_key="F1",
    title="Model Performance Comparison"
)

# Visualize uncertainty
plot_uncertainty(
    predictions=model_preds,
    uncertainty=model_uncertainties,
    title="Predictions with Uncertainty"
)
```

The visualization module integrates with the neural models in the meta_learning package, handling tensor outputs and providing specialized visualizations for neural causal discovery and optimization results. It follows consistent styling and interfaces with the existing visualization utilities in `causal_meta.graph.visualization`.

## Data Generation (`causal_meta.meta_learning.data_generation`)

**Purpose**: Generates synthetic data for training and testing neural causal discovery models.

**Key Classes**:
- `SyntheticDataGenerator`: Generates synthetic data from SCMs

**Key Methods**:
- `generate_observational_data(scm, n_samples)`: Generate observational data
- `generate_interventional_data(scm, interventions, n_samples)`: Generate interventional data
- `create_dataloaders(n_tasks, n_samples_per_task)`: Create PyTorch dataloaders

**Usage Example**:
```python
from causal_meta.meta_learning.data_generation import SyntheticDataGenerator
from causal_meta.graph.generators.factory import GraphFactory

# Create data generator
data_gen = SyntheticDataGenerator(
    num_nodes=5,
    graph_type="random",
    edge_probability=0.3
)

# Generate observational data
obs_data = data_gen.generate_observational_data(n_samples=1000)

# Generate interventional data
int_data = data_gen.generate_interventional_data(
    interventions={"X": 2.0},
    n_samples=1000
)

# Create dataloaders
train_loader, val_loader = data_gen.create_dataloaders(
    n_tasks=100,
    n_samples_per_task=1000,
    batch_size=32
)
```

## Best Practices for Leveraging Components

1. **Understand Existing Components**: Before implementing new code, check this registry to see if a component already exists that meets your needs. This is a **mandatory first step** for all new development.

2. **Consistent Interfaces**: When extending or implementing new components, maintain consistent interfaces with existing ones. Follow the method names, parameter orders, and return types defined in similar existing components.

3. **Use the Higher-Level Abstractions**: Prefer using higher-level abstractions (e.g., `AmortizedCausalDiscovery`) rather than lower-level components (e.g., `GraphEncoder` and `DynamicsDecoder`) when possible. Only implement at a lower level when you need specific customization not available in higher-level components.

4. **Documentation and Examples**: Add clear documentation and examples when implementing new components. Use the format in this registry as a template.

5. **Testing**: Write comprehensive tests for all components to ensure they work as expected. Include tests for normal operation, edge cases, and integration with dependent components.

6. **Integration**: When adding new components, ensure they integrate well with existing ones by preserving interface contracts and data formats expected by dependent components.

7. **Register Your Components**: After implementing a new component, you **must** update this registry to document it. This step is not optional and should be treated as part of the implementation process.

## Common Patterns and Anti-Patterns

### Patterns to Follow

1. **Consistent Data Formats**: Use consistent data formats across components (e.g., PyTorch tensors for neural models, pandas DataFrames for data manipulation).

2. **Clear Component Boundaries**: Define clear interfaces between components to facilitate integration.

3. **Error Handling**: Implement robust error handling with informative error messages.

4. **Configuration Management**: Use configuration objects for complex components to make them more flexible.

5. **Serialization**: Implement save/load methods for all stateful components.

### Anti-Patterns to Avoid

1. **Duplicating Functionality**: Do not reimplement functionality that already exists in the registry.

2. **Inconsistent Naming**: Follow naming conventions consistently (e.g., `get_nodes()` vs. `nodes()`).

3. **Hardcoding Parameters**: Avoid hardcoding parameters that should be configurable.

4. **Mixing Abstraction Levels**: Don't mix high-level and low-level operations in the same component.

5. **Ignoring Tensor Shapes**: Always be explicit about tensor shapes and dimensions, especially in neural network components.

## Component Dependency Graph

Here's a simplified dependency graph showing how the key components relate to each other:

```
CausalGraph
    ↑
    |
StructuralCausalModel
    ↑
    |
  TaskFamily ← TaskFamilyGenerator
    ↑
    |
SyntheticDataGenerator
    ↑
    |
  GraphEncoder       DynamicsDecoder
     ↑  \             /  ↑
     |   \           /   |
     |    \         /    |
     |     AmortizedCausalDiscovery
     |               ↑
  TaskEmbedding      |
     |               |
     |   MAMLForCausalDiscovery
     \       /
      \     /
       \   /
    AmortizedCBO
```

This diagram illustrates the hierarchical relationship between components, helping to understand which components depend on others.

## Contribution Guidelines and Ongoing Maintenance

### When Adding New Components

1. **Check Existing Components First**: Always review this registry before implementing new functionality to avoid duplication.

2. **Follow Existing Patterns**: When creating new components, adhere to the interfaces and patterns of similar existing components. For example, if extending or creating a variant of an existing class, maintain the same method signatures and parameter names where possible.

3. **Update This Registry**: After adding a new component to the codebase, update this registry with:
   - A clear description of the component's purpose
   - The key methods and their parameters
   - Usage examples
   - How it integrates with existing components
   - Where it fits in the component dependency graph

4. **Adhere to Architecture**: Structure your code to align with the existing architecture and dependency relationships shown in the component dependency graph.

5. **Implement Required Interfaces**: Ensure new components implement all interfaces expected by dependent components. Check the higher-level components that will use your new component to understand what interfaces they expect.

### When Modifying Existing Components

1. **Interface Stability**: Maintain backward compatibility where possible. If breaking changes are necessary, document them clearly.

2. **Update Documentation**: Update this registry to reflect any changes to interfaces, parameters, or behavior.

3. **Update Usages**: Ensure all code that depends on the modified component is updated as needed.

### Integration Checklist

Before submitting new or modified code, verify that:

- [ ] You've checked this registry for existing components that might fulfill or partially fulfill your needs
- [ ] Your component follows the naming conventions and interface patterns of similar existing components
- [ ] You've added comprehensive tests for the new/modified component
- [ ] You've updated this registry with appropriate documentation
- [ ] You've verified that dependent components still work with your changes
- [ ] You've added proper error handling and input validation
- [ ] Your code handles tensor shapes and data formats consistently with the rest of the codebase

The repository maintainer should review and approve all updates to this registry to ensure accuracy and consistency. This registry serves as both documentation and a design guide for the entire project.

*Last updated: [Current Date]*

## Table of Contents

- [Benchmark Framework](#benchmark-framework)
- [Causal Discovery](#causal-discovery)
- [Causal Models](#causal-models)
- [Intervention Optimization](#intervention-optimization)
- [Graph Neural Networks](#graph-neural-networks)
- [Meta-Learning Components](#meta-learning-components)
- [Utilities](#utilities)

## Benchmark Framework

### Overview

The benchmarking framework provides comprehensive tools for evaluating and comparing causal discovery and causal Bayesian optimization methods. The framework features:

1. Standard benchmarks for causal discovery and intervention optimization
2. Scalability testing to evaluate performance across different graph sizes
3. Memory and runtime profiling capabilities
4. Result visualization and reporting tools
5. Integration with neural network-based approaches

### Base Benchmark (`causal_meta.meta_learning.benchmark.Benchmark`)

**Purpose**: Abstract base class that defines the common interface for all benchmark implementations.

**Key Methods**:
- `setup()`: Set up the benchmark environment, generating datasets and problems
- `run()`: Run the benchmark on all registered models and baselines
- `add_model(name, model)`: Add a model to be evaluated
- `add_baseline(name, baseline)`: Add a baseline method for comparison
- `evaluate_structure_recovery(true_graph, pred_graph)`: Evaluate graph structure recovery performance
- `evaluate_intervention_prediction(scm, model, interventions)`: Evaluate intervention prediction performance
- `time_performance(callable_fn, *args, **kwargs)`: Measure execution time of a function
- `plot_results(metrics, title, save_path)`: Plot benchmark results
- `save_results(results, filename)`: Save benchmark results to a file
- `load_results(filename)`: Load benchmark results from a file

**Usage Example**:
```python
from causal_meta.meta_learning.benchmark import CausalDiscoveryBenchmark
from my_package import MyCausalDiscoveryModel

# Create a benchmark
benchmark = CausalDiscoveryBenchmark(
    name="my_benchmark",
    output_dir="benchmark_results",
    seed=42,
    num_nodes=10,
    num_graphs=20,
    num_samples=1000
)

# Add models to evaluate
my_model = MyCausalDiscoveryModel()
benchmark.add_model("my_model", my_model)

# Set up and run the benchmark
benchmark.setup()
results = benchmark.run()

# Plot and save results
benchmark.plot_results(
    title="My Benchmark Results",
    save_path="benchmark_results/my_plot.png"
)
```

### CausalDiscoveryBenchmark (`causal_meta.meta_learning.benchmark.CausalDiscoveryBenchmark`)

**Purpose**: Benchmarks causal discovery methods on synthetic graph recovery tasks.

**Key Features**:
- Evaluates how well methods recover graph structures from observational data
- Supports both observational and interventional data
- Measures structural accuracy using SHD, precision, recall, and F1 score
- Tracks runtime and memory usage
- Generates comprehensive reports with aggregated statistics

**Additional Methods**:
- `_evaluate_method(method_name, method)`: Evaluate a single causal discovery method
- `_run_structure_learning(method, obs_data, int_data)`: Run structure learning with proper interface handling
- `_aggregate_results()`: Aggregate results across all datasets and methods

**Usage Example**:
```python
from causal_meta.meta_learning.benchmark import CausalDiscoveryBenchmark
from my_package import MyCausalDiscoveryModel

# Create a benchmark for causal discovery evaluation
benchmark = CausalDiscoveryBenchmark(
    name="causal_discovery_er_10",
    output_dir="benchmark_results",
    seed=42,
    num_nodes=10,  # Evaluate on graphs with 10 nodes
    num_graphs=20,  # Generate 20 different test graphs
    num_samples=1000,  # 1000 samples per dataset
    graph_type="random",  # Use random Erdos-Renyi graphs
    edge_prob=0.3  # Control graph sparsity
)

# Add your causal discovery model
model = MyCausalDiscoveryModel()
benchmark.add_model("my_model", model)

# Set up and run the benchmark
benchmark.setup()
results = benchmark.run()
```

### CBOBenchmark (`causal_meta.meta_learning.benchmark.CBOBenchmark`)

**Purpose**: Benchmarks causal Bayesian optimization methods on synthetic intervention optimization tasks.

**Key Features**:
- Evaluates how well methods optimize interventions in causal systems
- Creates synthetic optimization problems with varying graph structures
- Measures optimization performance, sample efficiency, and computational requirements
- Compares against random baseline for relative improvement
- Tracks runtime and memory usage

**Additional Methods**:
- `_evaluate_method(method_name, method)`: Evaluate a single CBO method
- `_run_optimization(method, graph, scm, obs_data, ...)`: Run optimization with proper interface handling
- `_aggregate_results()`: Aggregate results across all optimization problems

**Usage Example**:
```python
from causal_meta.meta_learning.benchmark import CBOBenchmark
from my_package import MyCBOModel

# Create a benchmark for causal Bayesian optimization
benchmark = CBOBenchmark(
    name="cbo_er_10",
    output_dir="benchmark_results",
    seed=42,
    num_nodes=10,  # Evaluate on graphs with 10 nodes
    num_graphs=10,  # Generate 10 different test problems
    num_samples=1000,  # 1000 samples per dataset
    graph_type="random",  # Use random Erdos-Renyi graphs
    intervention_budget=10  # Allow 10 interventions per optimization
)

# Add your CBO model
model = MyCBOModel()
benchmark.add_model("my_model", model)

# Set up and run the benchmark
benchmark.setup()
results = benchmark.run()
```

### ScalabilityBenchmark (`causal_meta.meta_learning.benchmark.ScalabilityBenchmark`)

**Purpose**: Evaluates how well methods scale with increasing graph size, measuring both performance and computational requirements.

**Key Features**:
- Automatically tests on graphs of increasing size
- Measures runtime, memory usage, and accuracy as problem size increases
- Implements curve fitting to identify polynomial or exponential scaling
- Determines computational complexity class of algorithms
- Generates scaling reports with recommendations

**Key Methods**:
- `memory_usage()`: Get current memory usage statistics
- `measure_memory_usage(func, **kwargs)`: Measure the memory usage of a function
- `evaluate_method_scalability(method_name, method)`: Evaluate method scalability
- `analyze_scaling()`: Analyze scaling behavior across all methods
- `plot_scaling_curves(metric, log_scale, save_path)`: Plot scaling curves
- `generate_scaling_report(output_path)`: Generate a comprehensive scaling report

**Usage Example**:
```python
from causal_meta.meta_learning.benchmark import ScalabilityBenchmark
from my_package import MyCausalDiscoveryModel

# Create a scalability benchmark
benchmark = ScalabilityBenchmark(
    name="scalability_benchmark",
    output_dir="benchmark_results",
    seed=42,
    min_nodes=5,  # Start with graphs of size 5
    max_nodes=50,  # Go up to graphs of size 50
    step_size=5,  # Test sizes 5, 10, 15, ..., 50
    num_graphs_per_size=3,  # Generate 3 test problems per size
    measure_mode="both"  # Evaluate both discovery and CBO
)

# Add models to evaluate
model = MyCausalDiscoveryModel()
benchmark.add_model("my_model", model)

# Set up and run the benchmark
benchmark.setup()
results = benchmark.run()

# Generate scaling curves and report
benchmark.plot_scaling_curves(
    metric="runtime",
    log_scale=True,
    save_path="benchmark_results/runtime_scaling.png"
)
report = benchmark.generate_scaling_report(
    output_path="benchmark_results/scaling_report.json"
)
```

### BenchmarkRunner (`causal_meta.meta_learning.benchmark_runner.BenchmarkRunner`)

**Purpose**: Utility class for running multiple benchmarks and aggregating results, making it easy to compare different methods across various tasks.

**Key Methods**:
- `add_benchmark(benchmark)`: Add a benchmark to be run
- `add_model(name, model)`: Add a model to be evaluated in all benchmarks
- `register_model(name, model)`: Alias for add_model (backward compatibility)
- `add_baseline(name, baseline)`: Add a baseline method for comparison
- `run_all()`: Run all registered benchmarks with all registered methods
- `generate_summary_report(output_path)`: Generate a summary report of benchmark results
- `get_best_models()`: Get the best-performing models for each benchmark
- `generate_comparison_plots(output_dir)`: Generate comparison plots of results
- `create_standard_suite(...)`: Class method to create a standard benchmark suite
- `create_scalability_suite(...)`: Create a scalability benchmark suite
- `run_scalability_analysis(...)`: Run scalability analysis on a benchmark

**Usage Example**:
```python
from causal_meta.meta_learning.benchmark_runner import BenchmarkRunner
from my_package import MyCausalDiscoveryModel, MyCBOModel

# Create a benchmark runner
runner = BenchmarkRunner(
    name="my_benchmark_run",
    output_dir="benchmark_results",
    seed=42
)

# Add models to evaluate
discovery_model = MyCausalDiscoveryModel()
cbo_model = MyCBOModel()
runner.add_model("my_discovery_model", discovery_model)
runner.add_model("my_cbo_model", cbo_model)

# Create a standard benchmark suite
benchmark_ids = runner.create_standard_suite(
    graph_sizes=[5, 10, 20],
    num_graphs=5,
    num_samples=500
)

# Run all benchmarks
results = runner.run_all()

# Generate summary report
report_path = runner.generate_summary_report()

# Create a scalability suite
scalability_id = runner.create_scalability_suite(
    min_nodes=5,
    max_nodes=30,
    step_size=5
)[0]

# Run scalability analysis
scaling_results = runner.run_scalability_analysis(scalability_id)
```

## Common Benchmark Integration Patterns

### Integration with Neural Methods

Neural-based causal discovery and optimization methods can be evaluated using the same benchmarking framework. The benchmark classes automatically detect and handle different method interfaces.

For AmortizedCausalDiscovery:
```python
from causal_meta.meta_learning.benchmark import CausalDiscoveryBenchmark
from causal_meta.meta_learning.amortized_causal_discovery import AmortizedCausalDiscovery

# Create a neural causal discovery model
model = AmortizedCausalDiscovery(
    graph_encoder_params={...},
    dynamics_decoder_params={...}
)

# Add the model to a benchmark
benchmark = CausalDiscoveryBenchmark(
    name="neural_cd_benchmark",
    num_nodes=10,
    num_graphs=20
)
benchmark.add_model("neural_model", model)

# Run the benchmark
benchmark.setup()
results = benchmark.run()
```

For AmortizedCBO:
```python
from causal_meta.meta_learning.benchmark import CBOBenchmark
from causal_meta.meta_learning.amortized_cbo import AmortizedCBO

# Create a neural CBO model
model = AmortizedCBO(
    acd_model=acd_model,
    acquisition_function="expected_improvement"
)

# Add the model to a benchmark
benchmark = CBOBenchmark(
    name="neural_cbo_benchmark",
    num_nodes=10,
    num_graphs=10
)
benchmark.add_model("neural_cbo", model)

# Run the benchmark
benchmark.setup()
results = benchmark.run()
```

### Custom Model Interfaces

The benchmarking framework supports various model interfaces:

1. Standard interface:
   ```python
   # For discovery: model.learn_graph(obs_data, int_data)
   # For CBO: model.optimize(scm, graph, obs_data, target_node, ...)
   ```

2. Fit-predict interface:
   ```python
   # For discovery: model.fit(obs_data) followed by model.predict_graph()
   ```

3. Callable interface:
   ```python
   # For discovery: model(obs_data) returns a graph or adjacency matrix
   # For CBO: model(scm, graph, obs_data, ...) returns intervention
   ```

4. Neural interfaces:
   ```python
   # AmortizedCausalDiscovery.infer_graph and predict_intervention_outcomes
   # AmortizedCBO.suggest_intervention and get_best_intervention
   ```

## Benchmarking Framework Components

### `Benchmark` (Abstract Base Class)

**Purpose**: Serves as the foundation for all benchmark types, providing common functionality and interfaces.

**Key Features**:
- Abstract interface for benchmark configuration and execution
- Model registration and management
- Result tracking and serialization
- Common visualization and reporting utilities

**Key Methods**:
```python
def add_model(self, name, model, **kwargs)
```
- Registers a model for evaluation
- `name`: String identifier for the model
- `model`: Model instance or callable
- `kwargs`: Additional model-specific parameters

```python
def setup(self)
```
- Prepares the benchmark environment
- Generates test problems, graphs, and datasets
- Must be called before `run()`

```python
def run(self, models=None)
```
- Evaluates all registered models (or specified subset)
- `models`: Optional list of model names to evaluate
- Returns: Dictionary of results by model name

```python
def plot_results(self, metrics=None, **kwargs)
```
- Generates visualization of benchmark results
- `metrics`: List of metrics to include in visualization
- `kwargs`: Additional plotting parameters

```python
def save_results(self, file_path)
```
- Serializes results to disk
- `file_path`: Target path for saved results

**Usage Example**:
```python
benchmark = ConcreteBenchmark(name="example", seed=42)
benchmark.add_model("method1", Method1())
benchmark.add_model("method2", Method2())
benchmark.setup()
results = benchmark.run()
benchmark.plot_results(metrics=["metric1", "metric2"])
benchmark.save_results("results.json")
```

### `CausalDiscoveryBenchmark`

**Purpose**: Evaluates causal discovery/structure learning methods on their ability to recover true graph structure.

**Inheritance**: Extends `Benchmark`

**Parameters**:
- `name`: String identifier for the benchmark
- `seed`: Random seed for reproducibility
- `num_nodes`: Number of nodes in test graphs
- `num_graphs`: Number of test graphs to generate
- `num_samples`: Number of samples per dataset
- `graph_type`: Type of graphs to generate (e.g., "random", "scale-free")
- `edge_prob`: Probability of edge creation (for random graphs)
- `measure_memory`: Whether to track memory usage
- `timeout`: Maximum runtime allowed per method (seconds)

**Key Methods** (in addition to inherited methods):
```python
def add_baseline(self, name, method=None)
```
- Adds a baseline method (e.g., random, true graph)
- `name`: String identifier for the baseline
- `method`: Optional method implementation (if None, uses built-in baselines)

```python
def evaluate_model(self, model, test_case)
```
- Evaluates a single model on a specific test case
- `model`: Model instance to evaluate
- `test_case`: Test case dictionary with graph, data, etc.
- Returns: Dictionary of metrics

```python
def compute_metrics(self, true_graph, pred_graph)
```
- Computes structural metrics between graphs
- `true_graph`: Ground truth causal graph
- `pred_graph`: Predicted causal graph
- Returns: Dictionary of metrics (SHD, precision, recall, etc.)

**Metrics Provided**:
- Structural Hamming Distance (SHD)
- Precision (true edges / predicted edges)
- Recall (true edges / total true edges)
- F1 score (harmonic mean of precision and recall)
- Accuracy (correctly identified edges and non-edges)
- Runtime (seconds)
- Memory usage (if enabled)

**Usage Example**:
```python
# Create benchmark for 10-node graphs
benchmark = CausalDiscoveryBenchmark(
    name="cd_benchmark",
    seed=42,
    num_nodes=10,
    num_graphs=20,
    num_samples=1000,
    graph_type="random",
    edge_prob=0.3
)

# Add models to evaluate
benchmark.add_model("pc_algorithm", PCAlgorithm())
benchmark.add_model("notears", NOTEARS())
benchmark.add_model("neural_discovery", NeuralDiscoveryModel())

# Add baselines
benchmark.add_baseline("random")
benchmark.add_baseline("true_graph")

# Run benchmark
benchmark.setup()
results = benchmark.run()

# Visualize results
benchmark.plot_results(
    metrics=["shd", "precision", "recall"],
    figsize=(12, 8),
    save_path="discovery_results.png"
)
```

### `CBOBenchmark`

**Purpose**: Evaluates causal Bayesian optimization methods on their ability to identify optimal interventions.

**Inheritance**: Extends `Benchmark`

**Parameters**:
- `name`: String identifier for the benchmark
- `seed`: Random seed for reproducibility
- `num_nodes`: Number of nodes in test graphs
- `num_graphs`: Number of test graphs to generate
- `num_samples`: Number of samples per dataset
- `graph_type`: Type of graphs to generate
- `intervention_budget`: Number of interventions allowed
- `maximize`: Whether to maximize or minimize the objective
- `measure_memory`: Whether to track memory usage
- `timeout`: Maximum runtime allowed per method (seconds)

**Key Methods** (in addition to inherited methods):
```python
def add_baseline(self, name, method=None)
```
- Adds a baseline method (e.g., random search, optimal)
- `name`: String identifier for the baseline
- `method`: Optional method implementation (if None, uses built-in baselines)

```python
def evaluate_model(self, model, test_case)
```
- Evaluates a single model on a specific test case
- `model`: Model instance to evaluate
- `test_case`: Test case dictionary with SCM, graph, etc.
- Returns: Dictionary of metrics

```python
def create_objective(self, scm, target_node)
```
- Creates an objective function for optimization
- `scm`: Structural causal model instance
- `target_node`: Target node to optimize
- Returns: Callable objective function

**Metrics Provided**:
- Best value found
- Regret (gap from theoretical optimum)
- Improvement ratio over baseline
- Sample efficiency (value per evaluation)
- Convergence rate
- Runtime (seconds)
- Memory usage (if enabled)

**Usage Example**:
```python
# Create benchmark
benchmark = CBOBenchmark(
    name="cbo_benchmark",
    seed=42,
    num_nodes=10,
    num_graphs=15,
    num_samples=1000,
    graph_type="random",
    intervention_budget=20,
    maximize=True
)

# Add optimization methods
benchmark.add_model("bo_known_graph", BOWithKnownGraph())
benchmark.add_model("bo_unknown_graph", BOWithUnknownGraph())
benchmark.add_model("amortized_cbo", AmortizedCBO())

# Add baselines
benchmark.add_baseline("random_search")
benchmark.add_baseline("optimal")

# Run benchmark
benchmark.setup()
results = benchmark.run()

# Visualize results
benchmark.plot_results(
    metrics=["best_value", "improvement_ratio"],
    figsize=(12, 8),
    save_path="cbo_results.png"
)
```

### `ScalabilityBenchmark`

**Purpose**: Evaluates how methods scale with increasing problem size, measuring runtime, memory usage, and accuracy.

**Inheritance**: Extends `Benchmark`

**Parameters**:
- `name`: String identifier for the benchmark
- `seed`: Random seed for reproducibility
- `min_nodes`: Minimum number of nodes to test
- `max_nodes`: Maximum number of nodes to test
- `step_size`: Increment between node counts
- `num_graphs_per_size`: Number of test graphs per size
- `num_samples`: Number of samples per dataset
- `graph_type`: Type of graphs to generate
- `measure_mode`: What to measure ("discovery", "cbo", or "both")
- `timeout`: Maximum runtime allowed per method (seconds)

**Key Methods** (in addition to inherited methods):
```python
def plot_scaling_curves(self, metric="runtime", log_scale=True, **kwargs)
```
- Plots scaling curves for the specified metric
- `metric`: Metric to visualize scaling for
- `log_scale`: Whether to use logarithmic scale
- `kwargs`: Additional plotting parameters

```python
def generate_scaling_report(self)
```
- Analyzes scaling behavior and generates report
- Returns: Dictionary with scaling analysis for each model

```python
def fit_complexity_model(self, sizes, values)
```
- Fits a complexity model to observed data
- `sizes`: List of problem sizes
- `values`: Corresponding metric values
- Returns: Tuple of (complexity class, parameters)

**Metrics Provided**:
- Runtime scaling
- Memory usage scaling
- Accuracy scaling (SHD or regret)
- Estimated complexity class
- Maximum feasible problem size

**Usage Example**:
```python
# Create scalability benchmark
benchmark = ScalabilityBenchmark(
    name="scalability_benchmark",
    seed=42,
    min_nodes=5,
    max_nodes=50,
    step_size=5,
    num_graphs_per_size=3,
    num_samples=1000,
    graph_type="random",
    measure_mode="discovery"
)

# Add models to evaluate
benchmark.add_model("fast_method", FastMethod())
benchmark.add_model("accurate_method", AccurateMethod())

# Run benchmark
benchmark.setup()
results = benchmark.run()

# Visualize scaling
benchmark.plot_scaling_curves(
    metric="runtime",
    log_scale=True,
    save_path="scaling_curves.png"
)

# Get scaling analysis
scaling_report = benchmark.generate_scaling_report()
```

### `BenchmarkRunner`

**Purpose**: Orchestrates multiple benchmarks, facilitating comprehensive evaluation across different scenarios.

**Key Features**:
- Manages multiple benchmark instances
- Provides standard benchmark suite creation
- Aggregates results across benchmarks
- Generates comparative reports

**Parameters**:
- `name`: String identifier for the benchmark runner
- `seed`: Random seed for reproducibility
- `output_dir`: Directory to save results and reports

**Key Methods**:
```python
def add_model(self, name, model, **kwargs)
```
- Registers a model for evaluation across all benchmarks
- `name`: String identifier for the model
- `model`: Model instance or callable
- `kwargs`: Additional model-specific parameters

```python
def create_benchmark(self, benchmark_type, **kwargs)
```
- Creates a new benchmark instance
- `benchmark_type`: Class or string identifier for benchmark type
- `kwargs`: Parameters for benchmark initialization
- Returns: Benchmark ID

```python
def create_standard_suite(self, graph_sizes=None, **kwargs)
```
- Creates a standard suite of benchmarks
- `graph_sizes`: List of graph sizes to test
- `kwargs`: Additional parameters for benchmarks
- Returns: List of created benchmark IDs

```python
def run_all(self, parallel=False)
```
- Runs all registered benchmarks
- `parallel`: Whether to run benchmarks in parallel
- Returns: Dictionary of results by benchmark ID

```python
def generate_summary_report(self, file_path=None)
```
- Generates a summary report of all benchmark results
- `file_path`: Optional path to save the report
- Returns: Path to the generated report

```python
def compare_models(self, metrics=None, significance_level=0.05)
```
- Performs statistical comparison between models
- `metrics`: List of metrics to compare
- `significance_level`: P-value threshold for significance
- Returns: Dictionary of comparison results

**Usage Example**:
```python
# Create benchmark runner
runner = BenchmarkRunner(
    name="comprehensive_evaluation",
    seed=42,
    output_dir="benchmark_results"
)

# Add models to evaluate in all benchmarks
runner.add_model("method1", Method1())
runner.add_model("method2", Method2())
runner.add_model("neural_method", NeuralMethod())

# Create standard benchmark suite
benchmark_ids = runner.create_standard_suite(
    graph_sizes=[10, 20, 30],
    num_graphs=10,
    num_samples=1000
)

# Run all benchmarks
results = runner.run_all()

# Generate summary report
report_path = runner.generate_summary_report()

# Compare models statistically
comparison = runner.compare_models(
    metrics=["shd", "best_value", "runtime"],
    significance_level=0.05
)
```

### Dependency Graph

The benchmarking framework has the following primary dependencies:

1. **Core Dependencies**:
   - `causal_meta.graph.causal_graph.CausalGraph`: For graph representation
   - `causal_meta.graph.generators.factory.GraphFactory`: For generating test graphs
   - `causal_meta.environments.scm.StructuralCausalModel`: For data generation

2. **Support Dependencies**:
   - Standard libraries: `numpy`, `pandas`, `matplotlib`, `networkx`
   - Performance monitoring: `time`, `resource`, `psutil`
   - Statistical analysis: `scipy.stats`

3. **Optional Integration Points**:
   - `causal_meta.meta_learning.AmortizedCausalDiscovery`: For neural causal discovery
   - `causal_meta.optimization.AmortizedCBO`: For neural intervention optimization

The benchmarking framework is designed to be modular, allowing individual components to be used independently or together as needed.