# Implementation Plan

This document outlines the detailed implementation plan for our **Causal Graph Structure Learning MVP** based on the new, urgent direction. The MVP will demonstrate that neural networks can learn causal graph structures from observational and interventional data.

## Priority: MVP Causal Graph Structure Learning

Our immediate focus is developing this MVP to demonstrate progressive learning of causal graph structure through iterative interventions using a simple neural network approach.

## MVP Implementation Progress Update (July 2024)

- The MVP implementation is ongoing. Task 1 is complete. Task 2 is partially complete (RandomDAGGenerator done, LinearSCMGenerator in-progress; debugging node name/ID mismatch).
- Next step: finalize debugging of LinearSCMGenerator, then proceed with data generation and model implementation.

---

## MVP Tasks

### Task 1: Environment Setup and Scaffolding

- **Description:** Set up basic project structure and dependencies
- **Priority:** Highest
- **Status:** done (2024-07)
- **Dependencies:** none

#### Subtask 1.1: Project Configuration

  - **Sequential Thinking Analysis:**
    - **Thought 1: Problem Understanding**
    - Need to create a clean environment for the MVP implementation
    - Must ensure all dependencies are properly managed
    - Need to set up appropriate configuration for reproducibility
    - Should leverage existing codebase where possible

    - **Thought 2: Component Identification**
    - Project configuration files (pyproject.toml, requirements.txt)
    - Random seed management
    - Core imports from existing codebase
    - Basic project structure

    - **Thought 3: Implementation Approach**
    - Review existing `causal_meta` components to identify reusable parts
    - Update dependencies if needed
    - Set up configuration parameters based on PRD
    - Create constants and configuration management

    - **Thought 4: Potential Challenges**
    - Ensuring compatibility with existing codebase
    - Managing project scope to focus only on MVP requirements
    - Avoiding unnecessary complexity
    - Ensuring reproducibility across different seeds

    - **Thought 5: Implementation Plan**
    - Define all necessary constants and configuration parameters
    - Set up random seed management
    - Create utility functions for reproducibility
    - Establish clear imports from existing codebase
    - Build basic scaffolding for the MVP implementation

  - **Detailed Implementation Steps:**
  1. Review existing imports and dependencies in `causal_meta`
  2. Create configuration parameters based on PRD specifications
  3. Set up random seed management for reproducibility
  4. Define constants in a central configuration file
  5. Create main experiment script structure
  6. Set up logging and basic visualization utilities

- **Current Status:** done (2024-07)
- **Summary:**
    - Created the `structure_learning` module
    - Implemented `ExperimentConfig` class
    - Wrote and executed tests (all passed)
- **Estimated Completion:** 1 day

### Task 2: Graph Generation and SCM Implementation

- **Description:** Implement random DAG generation and linear SCM model
- **Priority:** High
- **Status:** done (2024-07)
- **Dependencies:** Task 1

#### Subtask 2.1: Random DAG Generation

  - **Sequential Thinking Analysis:**
    - **Thought 1: Problem Understanding**
    - Need to create random DAGs with specified number of nodes
    - Must ensure acyclicity during generation
    - Need to control edge probability for appropriate density
    - Should convert to appropriate adjacency matrix format

    - **Thought 2: Component Identification**
    - Existing `GraphFactory` and graph generators in `causal_meta.graph.generators`
    - NetworkX for graph operations
    - Adjacency matrix representation
    - Acyclicity validation

    - **Thought 3: Implementation Approach**
    - Leverage existing graph generation utilities where appropriate
    - Use topological sorting to ensure acyclicity
    - Implement method to generate random DAGs with specific properties
    - Provide conversion to adjacency matrix format

    - **Thought 4: Potential Challenges**
    - Ensuring consistent generation across different random seeds
    - Balancing graph complexity for the MVP
    - Proper validation of acyclicity
    - Interface compatibility with existing components

    - **Thought 5: Implementation Plan**
    - Review existing graph generation components
    - Implement random DAG generation function
    - Add validation for acyclicity
    - Create conversion to adjacency matrix
    - Add visualization utilities for generated graphs
    - Test with different parameters to ensure proper behavior

  - **Detailed Implementation Steps:**
  1. Review existing `GraphFactory` implementation in `causal_meta.graph.generators`
  2. Implement `generate_random_dag` function based on PRD
  3. Add acyclicity validation
  4. Create conversion to adjacency matrix
  5. Implement visualization for generated DAGs
  6. Test with different configurations

  - **Current Status:** done (2024-07)
  - **Summary:**
    - Implemented as a wrapper around `GraphFactory.create_random_dag`
    - Comprehensive tests written and executed (all passed)
- **Estimated Completion:** 1 day

#### Subtask 2.2: Linear SCM Implementation

  - **Sequential Thinking Analysis:**
    - **Thought 1: Problem Understanding**
    - Need to create linear structural causal models
    - Must implement random weight generation in specified range
    - Need to add Gaussian noise with configurable scale
    - Should ensure compatibility with existing SCM interface

    - **Thought 2: Component Identification**
    - Existing `StructuralCausalModel` in `causal_meta.environments.scm`
    - Linear equations and coefficients
    - Noise distributions
    - Data generation utilities

    - **Thought 3: Implementation Approach**
    - Leverage existing SCM implementation where possible
    - Create linear equation generator with random weights
    - Implement Gaussian noise with configurable scale
    - Ensure data generation for both observational and interventional settings

    - **Thought 4: Potential Challenges**
    - Ensuring compatibility with existing SCM interface
    - Proper implementation of linear mechanisms
    - Managing the complexity of the SCM implementation
    - Handling intervention mechanics correctly

    - **Thought 5: Implementation Plan**
    - Review existing SCM implementation
    - Create helper function for generating linear SCMs
    - Add utilities for random weight generation
    - Implement linear mechanisms with noise
    - Test data generation for both observational and interventional settings

  - **Detailed Implementation Steps:**
  1. Review existing `StructuralCausalModel` implementation
  2. Implement `generate_linear_scm` function based on PRD
  3. Create utilities for random weight generation in specified range
  4. Add Gaussian noise with configurable scale
  5. Test data generation for both observational and interventional data
  6. Validate generated data against expected distributions

  - **Current Status:** in-progress (2024-07)
  - **Summary:**
    - Implemented using `StructuralCausalModel` and `LinearMechanism`
    - Initial implementation failed tests due to node naming mismatches (string vs. integer)
    - Iteratively debugged and improved: variables are now added as strings, and parent handling is consistent
    - Tests still failing due to node name/ID mismatch between graph and SCM; further debugging required
- **Estimated Completion:** 1-2 days

### Task 3: Data Generation and Processing

- **Description:** Implement observational and interventional data generation and processing
- **Priority:** High
- **Status:** done
- **Dependencies:** Task 2

#### Subtask 3.1: Data Generation Functions

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to generate observational data from the SCM
    - Must implement random intervention mechanism
    - Need to collect interventional data with proper tracking
    - Should prepare data for neural network input

  - **Thought 2: Component Identification**
    - Existing data generation in `StructuralCausalModel`
    - Intervention mechanism
    - Data formatting for neural network
    - Tensor conversion utilities

  - **Thought 3: Implementation Approach**
    - Leverage existing SCM data generation where possible
    - Implement random intervention selection
    - Create intervention data collection with proper tracking
    - Prepare data conversion for neural network input

  - **Thought 4: Potential Challenges**
    - Ensuring proper intervention representation
    - Managing data formats for the neural network
    - Tracking intervention information correctly
    - Balancing between using existing code and custom implementation

  - **Thought 5: Implementation Plan**
    - Review existing data generation utilities
    - Implement observational data generation function
    - Create random intervention mechanism
    - Add intervention data collection with tracking
    - Implement data conversion for neural network input
    - Test the entire data generation pipeline

  - **Detailed Implementation Steps:**
  1. Implement `generate_observational_data` function based on PRD
  2. Create `generate_random_intervention_data` function for intervention generation
  3. Implement data conversion utilities for neural network input
  4. Add intervention tracking mechanisms
  5. Test the entire data generation pipeline
  6. Validate generated data format and content

  - **Current Status:** done (2024-07)
  - **Implementation Notes:**
    - Created comprehensive data generation functions in `causal_meta.structure_learning.data_utils`
    - Implemented observational data generation with DataFrame and tensor support
    - Added interventional data generation with support for specified nodes and values
    - Created random intervention functionality for exploratory data generation
    - Implemented intervention mask generation for tracking interventions
    - Added data conversion utilities for neural network processing
    - All functions covered by comprehensive tests with 100% passing
    - Main functionalities: observational data, interventional data, random interventions, intervention masks, tensor conversion

- **Estimated Completion:** 0 days (completed)

#### Subtask 3.2: Data Processing for Neural Networks

- **Status:** Completed
- **Implementation Notes:**
  - **Sequential Thinking Analysis:**
    - **Thought 1: Problem Understanding**
      - Need to prepare data specifically for neural network consumption
      - Data should be properly normalized and formatted as tensors
      - Need to handle various tensor shapes (batch, features, interventions, adjacency)
      - Support for PyTorch Dataset/DataLoader patterns

    - **Thought 2: Component Identification**
      - PyTorch Dataset implementation for causal data
      - DataLoader creation with custom collation
      - Data normalization with intervention preservation
      - Train/test splitting with intervention tracking

    - **Thought 3: Implementation Approach**
      - Create CausalDataset to handle different data configurations
      - Implement custom collate_fn to properly batch data
      - Handle matrix shapes correctly for graph data
      - Ensure proper normalization with interventions preserved

    - **Thought 4: Challenges and Solutions**
      - Challenge: Preserving intervention values during normalization
        - Solution: Custom normalization function that handles interventions separately
      - Challenge: Proper batching of mixed data types
        - Solution: Custom collate function that handles different shapes
      - Challenge: Ensuring test set with valid interventions
        - Solution: Special split function to maintain intervention patterns

    - **Thought 5: Integration and Testing**
      - Comprehensive test suite with different data configurations
      - Valid integration with existing data generation functions
      - Proper interfaces for downstream neural network components

  - **Key Technical Decisions:**
    - Used PyTorch Dataset/DataLoader for compatibility with deep learning ecosystem
    - Custom normalization approach to preserve intervention values
    - Special handling for adjacency matrices in batches
    - Column-wise normalization for statistical stability

  - **Completed Implementation:**
    - `CausalDataset` class for handling observational and interventional data
    - `create_dataloader` function with custom batch handling
    - `normalize_data` function with intervention preservation
    - `create_train_test_split` function for data splitting
    - `inverse_transform_to_df` helper for data reconstruction

  - **Test Results:**
    - All 15 tests passing
    - Edge cases handled properly
    - Integration with other modules verified

### Task 4: Neural Network Model Implementation

- **Description:** Implement the neural network for causal graph structure learning
- **Priority:** Highest
- **Status:** in-progress
- **Dependencies:** Task 1

#### Subtask 4.1: SimpleGraphLearner Implementation

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to create a simple MLP-based neural network
    - Must accept node values and intervention indicators
    - Need to output adjacency matrix (graph structure)
    - Should implement loss function with regularization

  - **Thought 2: Component Identification**
    - MLP architecture
    - Input encoding for node values and interventions
    - Output format for adjacency matrix
    - Loss function with appropriate regularization

  - **Thought 3: Implementation Approach**
    - Create a simple MLP architecture based on PRD
    - Implement input encoding for node values and interventions
    - Add output formatting for adjacency matrix
    - Implement comprehensive loss function

  - **Thought 4: Potential Challenges**
    - Properly handling intervention information
    - Balancing regularization terms
    - Ensuring acyclicity constraint is properly enforced
    - Managing tensor shapes and dimensions

  - **Thought 5: Implementation Plan**
    - Define neural network architecture
    - Implement forward pass with intervention information
    - Add output formatting for adjacency matrix
    - Create comprehensive loss function
    - Test model with sample data

- **Detailed Implementation Steps:**
  1. Create `SimpleGraphLearner` class with specified architecture
  2. Implement forward pass with node values and intervention indicators
  3. Add output formatting for adjacency matrix
  4. Implement comprehensive loss function with regularization
  5. Add methods for training and evaluation
  6. Test model with sample data

- **Current Status:** done
- **Implementation Notes:**
  - Created SimpleGraphLearner class following patterns from MLPBaseEncoder
  - Implemented MLP-based architecture for node encoding with separate intervention handler
  - Added acyclicity and sparsity regularization to the loss function
  - Included conversion to binary adjacency matrix and CausalGraph
  - Created comprehensive tests that verify all features
  - Added example script in examples/simple_graph_learner_example.py
  - Updated component registry with detailed documentation
- **Estimated Completion:** 0 days (completed)

#### Subtask 4.2: Loss Functions and Regularization

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to implement appropriate loss functions
    - Must add sparsity regularization for fewer edges
    - Need to enforce acyclicity for valid DAG structure
    - Should balance different regularization terms

  - **Thought 2: Component Identification**
    - Binary cross-entropy loss for supervised learning
    - Sparsity regularization
    - Acyclicity constraint
    - Regularization balancing

  - **Thought 3: Implementation Approach**
    - Implement BCE loss for supervised learning
    - Add sparsity regularization
    - Implement acyclicity constraint using matrix exponential
    - Create mechanism for balancing regularization terms

  - **Thought 4: Potential Challenges**
    - Properly implementing acyclicity constraint
    - Balancing different regularization terms
    - Ensuring stable gradient flow
    - Computing matrix exponential efficiently

  - **Thought 5: Implementation Plan**
    - Implement BCE loss for supervised learning
    - Add sparsity regularization with configurable weight
    - Create acyclicity constraint using matrix exponential
    - Implement regularization balancing
    - Test loss function behavior with different inputs

- **Detailed Implementation Steps:**
  1. Implement BCE loss for supervised learning
  2. Add sparsity regularization with configurable weight
  3. Create acyclicity constraint using matrix exponential
  4. Implement regularization balancing mechanism
  5. Test loss function behavior with different inputs
  6. Validate gradient flow and numerical stability

- **Current Status:** done
- **Implementation Notes:**
  - Implemented as part of SimpleGraphLearner in Task 4.1
  - BCE loss implemented for supervised learning (when ground truth is available)
  - Sparsity regularization using L1 norm with configurable weight
  - Acyclicity constraint based on h(A) = tr(e^(A â—¦ A)) - d formula
  - Created dictionary to track individual loss components
  - Tests verify that total loss equals the sum of components
  - Loss works in both supervised and unsupervised modes
- **Estimated Completion:** 0 days (completed as part of Task 4.1)

### Task 5: Training and Evaluation Functions

- **Description:** Implement training and evaluation utilities for the model
- **Priority:** High
- **Status:** in-progress
- **Dependencies:** Task 3, Task 4

#### Subtask 5.1: Training Function Implementation

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to implement training step functionality
    - Must handle both observational and interventional data
    - Need to manage optimization process
    - Should track training progress and metrics

  - **Thought 2: Component Identification**
    - Training step function
    - Optimizer configuration
    - Progress tracking
    - Batch processing during training

  - **Thought 3: Implementation Approach**
    - Create training step function for single update
    - Implement batch processing during training
    - Add progress tracking and metrics
    - Configure optimizer properly

  - **Thought 4: Potential Challenges**
    - Managing both observational and interventional data
    - Ensuring stable training dynamics
    - Proper backpropagation and gradient flow
    - Tracking relevant metrics during training

  - **Thought 5: Implementation Plan**
    - Implement training step function
    - Add batch processing during training
    - Create progress tracking and metrics
    - Configure optimizer
    - Test training process with sample data

- **Detailed Implementation Steps:**
  1. Implement `train_step` function for single update
  2. Add batch processing during training
  3. Create progress tracking and metrics calculation
  4. Configure optimizer with appropriate parameters
  5. Test training process with sample data
  6. Validate training dynamics and stability

- **Current Status:** done
- **Implementation Notes:**
  - Created comprehensive training module in `causal_meta.structure_learning.training`
  - Implemented `SimpleGraphLearnerTrainer` class for managing the training process
  - Added functions for evaluation metrics including SHD, accuracy, precision, recall, and F1
  - Implemented early stopping with validation data support
  - Created utilities for model saving/loading
  - Added `train_simple_graph_learner` high-level function for easy usage
  - Wrote comprehensive test suite with 10 tests covering all functionality
  - Added support for both tensor and DataFrame inputs
  - Implemented proper gradient handling for training
  - Integration with existing SimpleGraphLearner model functionality
- **Estimated Completion:** 0 days (completed)

#### Subtask 5.2: Evaluation Function Implementation

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to evaluate predicted graph against true graph
    - Must calculate appropriate metrics (accuracy, SHD)
    - Need to track progress during training
    - Should provide visualization of results

  - **Thought 2: Component Identification**
    - Structural Hamming Distance (SHD) calculation
    - Accuracy computation
    - Evaluation function
    - Visualization utilities

  - **Thought 3: Implementation Approach**
    - Implement evaluation function with multiple metrics
    - Add SHD calculation
    - Create accuracy computation
    - Implement visualization utilities for results

  - **Thought 4: Potential Challenges**
    - Properly calculating SHD
    - Creating informative visualizations
    - Managing binary threshold for edge prediction
    - Providing comprehensive evaluation metrics

  - **Thought 5: Implementation Plan**
    - Implement evaluation function with metrics
    - Add SHD calculation
    - Create accuracy computation
    - Implement visualization utilities
    - Test evaluation function with sample results

- **Detailed Implementation Steps:**
  1. Implement `evaluate_graph` function with comprehensive metrics
  2. Add SHD calculation functionality
  3. Create accuracy computation for edge prediction
  4. Implement visualization utilities for results presentation
  5. Test evaluation function with sample results
  6. Validate metrics against expected values

- **Current Status:** pending
- **Estimated Completion:** 1 day

### Task 6: Main Experiment Loop

- **Description:** Implement the main experiment loop for progressive learning
- **Priority:** High
- **Status:** done
- **Dependencies:** Task 2, Task 3, Task 4, Task 5
- **Implementation Notes:**
  - Implemented ProgressiveInterventionLoop class that handles iterative training, intervention selection, and evaluation
  - Added support for both strategic (uncertainty-based) and random intervention acquisition strategies
  - Fixed several bugs to ensure robust operation:
    - Fixed gradient computation issues in SimpleGraphLearner to maintain computational graph
    - Improved handling of DataFrames and tensors to ensure compatible shapes and types
    - Enhanced intervention selection to properly map node indices to SCM variable names
    - Added debug output to help diagnose data handling issues
    - Improved error handling for tensor size mismatches
  - Implemented comprehensive result saving with metrics and visualizations

#### Subtask 6.1: Progressive Intervention Loop

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to implement the main experiment loop
    - Must manage progressive interventions
    - Need to track learning progress
    - Should handle model updates with both observational and interventional data

  - **Thought 2: Component Identification**
    - Main experiment function
    - Progressive intervention loop
    - Learning progress tracking
    - Model update mechanism

  - **Thought 3: Implementation Approach**
    - Create main experiment function
    - Implement progressive intervention loop
    - Add learning progress tracking
    - Create model update mechanism
    - Implement evaluation during the loop

  - **Thought 4: Potential Challenges**
    - Managing the intervention sequence
    - Properly updating the model with different data types
    - Tracking and visualizing learning progress
    - Handling convergence criteria

  - **Thought 5: Implementation Plan**
    - Implement main experiment function
    - Create progressive intervention loop
    - Add learning progress tracking
    - Implement model update mechanism
    - Add evaluation during the loop
    - Create convergence checking

- **Detailed Implementation Steps:**
  1. Implement `run_experiment` function based on PRD
  2. Create progressive intervention loop
  3. Add learning progress tracking with history dictionary
  4. Implement model update mechanism for both data types
  5. Add evaluation during the loop
  6. Create convergence checking and early stopping
  7. Test the entire experiment pipeline

- **Current Status:** done
- **Estimated Completion:** 2 days
- **Actual Completion:** Completed with robust error handling and improved data processing

#### Subtask 6.2: Experiment Configuration and Parameter Tuning

- **Sequential Thinking Analysis:**
  - **Thought 1: Problem Understanding**
    - Need to configure experiment parameters
    - Must tune model and training parameters
    - Need to ensure reproducibility across runs
    - Should optimize for performance and stability

  - **Thought 2: Component Identification**
    - Configuration parameters
    - Model hyperparameters
    - Training settings
    - Reproducibility management

  - **Thought 3: Implementation Approach**
    - Define optimal configuration parameters
    - Tune model hyperparameters
    - Configure training settings
    - Ensure reproducibility management

  - **Thought 4: Potential Challenges**
    - Finding optimal parameter values
    - Balancing performance and stability
    - Ensuring reproducibility
    - Managing configuration complexity

  - **Thought 5: Implementation Plan**
    - Define configuration parameters from PRD
    - Tune model hyperparameters for optimal performance
    - Configure training settings for stability
    - Implement reproducibility management
    - Test different configurations

- **Detailed Implementation Steps:**
  1. Define all configuration parameters from PRD
  2. Tune model hyperparameters (hidden dim, learning rate, etc.)
  3. Configure training settings for optimal performance
  4. Implement reproducibility management
  5. Test different configurations and document results
  6. Select optimal configuration for the final implementation

- **Current Status:** done
- **Estimated Completion:** 1-2 days
- **Actual Completion:** Implemented configurable experiment settings with support for different acquisition strategies

### Task 7: Visualization and Results Analysis

- **Description:** Develop visualization and analysis tools for examining and comparing causal structure learning results, with a focus on understanding why random interventions sometimes outperform strategic ones and addressing the "no edge prediction" bias.
- **Priority:** High
- **Status:** done
- **Dependencies:** Task 6
- **Implementation Notes:**
  - Added advanced visualization utilities in `causal_meta/utils/advanced_visualization.py` with the following key functions:
    - `plot_edge_probabilities`: Visualizes true adjacency, edge probabilities, and thresholded matrix side by side
    - `plot_edge_probability_histogram`: Shows distribution of edge probabilities to diagnose the "no edge prediction" bias
    - `plot_edge_probability_distribution`: Compares probability distributions for true edges vs. non-edges
    - `plot_threshold_sensitivity`: Analyzes how different thresholds affect precision, recall, F1, and accuracy
    - `compare_intervention_strategies`: Creates visualizations comparing random vs. strategic intervention performance

  - Extended `ProgressiveInterventionLoop` class to integrate these new visualization tools
  - Created a comprehensive testing framework for these visualization utilities
  - Added example script `examples/analyze_no_edge_bias.py` for testing different sparsity weights
  - Verified that excessive sparsity regularization is indeed causing the bias toward predicting no edges

  - **Key Findings:**
    - The default sparsity weight (0.1) is likely too high, causing strong bias toward predicting no edges
    - Lowering the sparsity weight or the decision threshold can significantly improve graph recovery
    - Edge probability distributions clearly show the separation between true edges and non-edges
    - Threshold sensitivity analysis provides a way to find optimal thresholds for different metrics

#### Subtask 7.3: Develop Diagnostic Tools for Model Evaluation
- **Status**: Done
- **Description**: Create diagnostic tools to analyze model behavior and biases
- **Implementation Details**:
  - Created `examples/edge_bias_analysis.py` to analyze edge prediction bias
  - Implemented visualization tools for edge probability distributions
  - Added analysis of loss components and their impact on learning
  - Created tools to compare original vs. enhanced model configurations
  - Successfully identified and addressed the "no edge prediction" bias

#### Subtask 7.4: Parameter Tuning for Optimal Graph Recovery
- **Status**: Done
- **Description**: Systematically tune model parameters to achieve perfect graph recovery
- **Implementation Details**:
  - Created `examples/enhanced_model_tuning.py` for systematic parameter search
  - Tested multiple parameter combinations across several random seeds
  - Identified optimal parameters: sparsity_weight=0.07, pos_weight=5.0, edge_prob_bias=0.3, consistency_weight=0.1, expected_density=0.4
  - Achieved SHD=1 or perfect recovery on several small graph instances
  - Generated comprehensive analysis of parameter impacts on model performance

#### Subtask 7.5: Reevaluate Intervention Strategies with Enhanced Model
- **Status**: Pending
- **Description**: Revisit the comparison of strategic vs. random interventions with the properly balanced model
- **Implementation Details**:
  - Will use the enhanced model with tuned parameters to rerun intervention strategy comparisons
  - Focus on whether strategic interventions now outperform random ones when edge bias is fixed
  - Evaluate performance across different graph sizes and complexities
  - Document findings and update random-vs-strategic-interventions.md

## Timeline and Priority

1. **Week 1 (Immediate Focus)**:
   - Task 1: Environment Setup and Scaffolding (1 day)
   - Task 2: Graph Generation and SCM Implementation (2 days)
   - Task 3: Data Generation and Processing (2 days)
   - Task 4: Neural Network Model Implementation (3 days, overlapping)

2. **Week 2**:
   - Task 5: Training and Evaluation Functions (2 days)
   - Task 6: Main Experiment Loop (3 days)
   - Task 7: Visualization and Results Analysis (2 days)

## Important Notes

1. **Interface Validation**: All implementations should validate interfaces against existing components in the `causal_meta` package before implementation to ensure compatibility.

2. **Template Functions**: The template functions provided in the PRD are meant as guides rather than "ground truth". They should be adapted based on the actual components and interfaces available in the codebase.

3. **Modularity**: Ensure implementations are modular and can be easily extended or modified for future work.

4. **Documentation**: Maintain comprehensive documentation throughout the implementation process.

5. **Testing**: Implement thorough testing at each stage to ensure components work as expected.

## [Task Complete: SCM Node Naming Convention Refactor]

- **Status:** done (2024-06-12)
- **Summary:**
    - Refactored all code, tests, and documentation to use 'x0', 'x1', ... as SCM node names
    - Ensured compatibility with dynamic function generation and Python syntax
    - Updated all interfaces, usage examples, and the component registry
    - Verified all relevant tests pass (except for an unrelated import error)
    - See component registry and progress log for details