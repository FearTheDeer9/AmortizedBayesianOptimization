#!/usr/bin/env python3
"""Simple Training Speed Benchmark.

This script provides realistic speed estimates based on component benchmarks
rather than full training loops which have interface mismatches.
"""

import sys
import time
from pathlib import Path
from typing import Dict, Any
import json

import jax
import jax.numpy as jnp
import jax.random as random

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from causal_bayes_opt.training.grpo_config import (
    create_debug_grpo_config,
    create_standard_grpo_config,
    create_production_grpo_config
)
from causal_bayes_opt.training.experience_management import (
    create_experience_manager,
    Experience
)
from causal_bayes_opt.acquisition.reward_rubric import create_training_rubric
from causal_bayes_opt.jax_native.state import create_test_state
from causal_bayes_opt.acquisition.reward_rubric import RewardResult
from causal_bayes_opt.environments.intervention_env import EnvironmentInfo
import pyrsistent as pyr


class SimpleSpeedBenchmark:
    """Simple speed benchmarks based on component performance."""
    
    def benchmark_component_speeds(self):
        """Benchmark individual component speeds."""
        print("Component Speed Benchmarks")
        print("=" * 50)
        
        results = {}
        
        # 1. State creation speed
        start = time.time()
        for _ in range(1000):
            state = create_test_state()
        state_creation_time = (time.time() - start) / 1000
        results['state_creation_per_op'] = state_creation_time
        print(f"State creation: {state_creation_time*1000:.3f}ms per operation")
        
        # 2. Reward computation speed
        rubric = create_training_rubric()
        test_state = create_test_state()
        test_action = pyr.pmap({'X': 1.0})
        test_outcome = pyr.pmap({'values': pyr.pmap({'X': 1.0, 'Y': 2.0})})
        
        start = time.time()
        for _ in range(100):
            reward = rubric.compute_reward(test_state, test_action, test_outcome)
        reward_computation_time = (time.time() - start) / 100
        results['reward_computation_per_op'] = reward_computation_time
        print(f"Reward computation: {reward_computation_time*1000:.3f}ms per operation")
        
        # 3. Experience management speed
        grpo_config = create_debug_grpo_config()
        exp_manager = create_experience_manager(grpo_config)
        
        # Create test experience
        test_experience = Experience(
            state=test_state,
            action=test_action,
            next_state=test_state,
            reward=RewardResult(1.0, {'test': 1.0}, {}),
            done=False,
            log_prob=-0.5,
            value=0.8,
            env_info=EnvironmentInfo(1, 10.0, False, False, False, 0.8, {}),
            timestamp=time.time()
        )
        
        start = time.time()
        for _ in range(1000):
            exp_manager.add_experience(test_experience)
        experience_add_time = (time.time() - start) / 1000
        results['experience_add_per_op'] = experience_add_time
        print(f"Experience addition: {experience_add_time*1000:.3f}ms per operation")
        
        # 4. Batch sampling speed
        start = time.time()
        for _ in range(100):
            batch = exp_manager.sample_batch()
        batch_sample_time = (time.time() - start) / 100
        results['batch_sampling_per_op'] = batch_sample_time
        print(f"Batch sampling: {batch_sample_time*1000:.3f}ms per operation")
        
        return results
    
    def estimate_training_speeds(self, component_results: Dict[str, float]):
        """Estimate training speeds based on component benchmarks."""
        print("\nTraining Speed Estimates")
        print("=" * 50)
        
        configs = {
            'debug': create_debug_grpo_config(),
            'standard': create_standard_grpo_config(), 
            'production': create_production_grpo_config()
        }
        
        estimates = {}
        
        for config_name, config in configs.items():
            # Estimate operations per training step
            experiences_per_step = 5  # Conservative estimate
            
            step_time = (
                component_results['state_creation_per_op'] * 2 +  # Current + next state
                component_results['reward_computation_per_op'] * experiences_per_step +
                component_results['experience_add_per_op'] * experiences_per_step +
                component_results['batch_sampling_per_op'] * 1 +  # One batch per step
                0.010  # JAX compilation and other overhead
            )
            
            total_steps = config.max_training_steps
            total_time = step_time * total_steps
            total_hours = total_time / 3600
            
            meets_24h = total_hours < 24
            
            estimates[config_name] = {
                'step_time': step_time,
                'total_steps': total_steps,
                'total_time_seconds': total_time,
                'total_hours': total_hours,
                'meets_24h_requirement': meets_24h
            }
            
            status = "âœ…" if meets_24h else "âŒ"
            print(f"{config_name:12}: {total_hours:8.2f}h ({total_steps:6} steps) {status}")
        
        return estimates
    
    def run_jax_compilation_benchmark(self):
        """Benchmark JAX compilation speedup."""
        print("\nJAX Compilation Speedup")
        print("=" * 50)
        
        # Simple function for benchmarking
        def simple_computation(x):
            return jnp.sum(x ** 2) + jnp.mean(x)
        
        # Compile the function
        compiled_fn = jax.jit(simple_computation)
        
        # Test data
        test_data = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0] * 100)
        
        # Warmup
        _ = compiled_fn(test_data)
        
        # Benchmark uncompiled
        start = time.time()
        for _ in range(1000):
            _ = simple_computation(test_data)
        uncompiled_time = time.time() - start
        
        # Benchmark compiled
        start = time.time()
        for _ in range(1000):
            _ = compiled_fn(test_data)
        compiled_time = time.time() - start
        
        speedup = uncompiled_time / compiled_time
        
        print(f"Uncompiled: {uncompiled_time:.3f}s")
        print(f"Compiled:   {compiled_time:.3f}s")
        print(f"Speedup:    {speedup:.1f}x")
        
        return speedup
    
    def generate_final_assessment(self, estimates: Dict[str, Dict]):
        """Generate final assessment of training speed requirements."""
        print("\nFinal Assessment")
        print("=" * 50)
        
        meeting_requirements = [
            config for config, data in estimates.items()
            if data['meets_24h_requirement']
        ]
        
        if meeting_requirements:
            print("âœ… TRAINING SPEED REQUIREMENTS CAN BE MET")
            print(f"   {len(meeting_requirements)}/{len(estimates)} configurations under 24h")
            print("   Recommended configurations:")
            for config in meeting_requirements:
                hours = estimates[config]['total_hours']
                print(f"     - {config}: {hours:.1f} hours")
        else:
            print("âŒ TRAINING SPEED REQUIREMENTS NOT MET")
            print("   No configurations complete within 24 hours")
            print("   Recommended optimizations:")
            print("     - Reduce max_training_steps")
            print("     - Enable JAX compilation")
            print("     - Use production optimization level")
        
        return len(meeting_requirements) > 0
    
    def run_all_benchmarks(self):
        """Run complete benchmark suite."""
        print("Training Speed Benchmark Suite")
        print("=" * 60)
        print("Estimating training speeds based on component performance\n")
        
        # Component benchmarks
        component_results = self.benchmark_component_speeds()
        
        # Training estimates
        estimates = self.estimate_training_speeds(component_results)
        
        # JAX speedup
        jax_speedup = self.run_jax_compilation_benchmark()
        
        # Final assessment
        requirements_met = self.generate_final_assessment(estimates)
        
        # Save results
        results = {
            'component_benchmarks': component_results,
            'training_estimates': estimates,
            'jax_speedup': jax_speedup,
            'requirements_met': requirements_met,
            'timestamp': time.time()
        }
        
        results_file = Path(__file__).parent.parent / "training_speed_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nðŸ“Š Results saved to {results_file}")
        
        return requirements_met


def main():
    """Run simple speed benchmarks."""
    benchmark = SimpleSpeedBenchmark()
    success = benchmark.run_all_benchmarks()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())