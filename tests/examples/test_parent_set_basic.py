#!/usr/bin/env python3
"""
Enhanced validation test showcasing ParentSetPosterior API.

This test validates the complete parent set prediction system using the clean
ParentSetPosterior API with rich analysis capabilities.

Tests the standard SCM: X ‚Üí Y ‚Üê Z
"""

import sys
from pathlib import Path

# Add src to path
project_root = Path(__file__).parent.parent.parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

import jax
import jax.numpy as jnp
import jax.random as random
import optax

from causal_bayes_opt.experiments.test_scms import create_simple_test_scm
from causal_bayes_opt.mechanisms.linear import sample_from_linear_scm
# Clean ParentSetPosterior API
from causal_bayes_opt.avici_integration import (
    create_training_batch,
    create_parent_set_model,
    predict_parent_posterior,          # Main prediction API
    summarize_posterior,               # Rich analysis
    get_marginal_parent_probabilities, # Marginal analysis
    get_most_likely_parents,           # Clean extraction
    get_parent_set_probability,        # Direct queries
    compare_posteriors,                # Comparison utilities
    create_parent_set_posterior,       # Manual creation
    compute_loss,
    create_train_step
)
from causal_bayes_opt.avici_integration.testing.debug_tools import (
    debug_parent_set_enumeration,
    debug_training_step,
    debug_logits_and_probabilities
)


def create_simple_config():
    """Simple config for quick testing."""
    return {
        'model_kwargs': {
            'layers': 2,  # Smaller for quick testing
            'dim': 32,    # Smaller for quick testing
            'key_size': 8,
            'num_heads': 2,
            'dropout': 0.0,  # No dropout for testing
        },
        'learning_rate': 1e-3,  # Fixed learning rate
        'batch_size': 16,
        'gradient_clip_norm': 1.0,
        'max_parent_size': 3,  # Allow up to 3 parents for full SCM
    }


def create_improved_optimizer(config):
    """Create optimizer with gradient clipping."""
    return optax.chain(
        optax.clip_by_global_norm(config['gradient_clip_norm']),
        optax.adam(learning_rate=config['learning_rate'])
    )


def test_posterior_api_features():
    """Demonstrate the comprehensive ParentSetPosterior API features."""
    print("üîÆ PARENT SET POSTERIOR API FEATURES")
    print("=" * 50)
    print("Demonstrating comprehensive ParentSetPosterior capabilities")
    print("=" * 50)
    
    # Setup
    config = create_simple_config()
    scm = create_simple_test_scm(noise_scale=1.0, target="Y")
    from causal_bayes_opt.data_structures.scm import get_variables
    variables = sorted(get_variables(scm))
    
    net = create_parent_set_model(
        model_kwargs=config['model_kwargs'],
        max_parent_size=config['max_parent_size']
    )
    
    # Generate test data
    samples = sample_from_linear_scm(scm, n_samples=config['batch_size'], seed=42)
    batch = create_training_batch(scm, samples, "Y")
    params = net.init(random.PRNGKey(42), batch['x'], variables, "Y", True)
    
    print(f"\nüéØ CORE API:")
    print("-" * 30)
    
    # Main prediction API
    posterior = predict_parent_posterior(net, params, batch['x'], variables, "Y")
    print(f"‚úÖ predict_parent_posterior() -> ParentSetPosterior")
    print(f"  Target: {posterior.target_variable}")
    print(f"  Parent sets: {len(posterior.parent_set_probs)}")
    print(f"  Uncertainty: {posterior.uncertainty:.3f} nats")
    
    # Rich analysis capabilities
    print(f"\n‚ú® ANALYSIS FEATURES:")
    
    # 1. Rich summary
    summary = summarize_posterior(posterior)
    print(f"üìã summarize_posterior():")
    print(f"  Most likely: {summary['most_likely_parents']}")
    print(f"  Confidence: {summary['most_likely_probability']:.3f}")
    print(f"  Uncertainty: {summary['uncertainty_bits']:.2f} bits")
    print(f"  Concentration: {summary['concentration']:.3f}")
    
    # 2. Marginal probabilities
    marginals = get_marginal_parent_probabilities(posterior, variables)
    print(f"\nüìà get_marginal_parent_probabilities():")
    for var, prob in marginals.items():
        print(f"  P({var} is parent of Y) = {prob:.3f}")
    
    # 3. Top-k extraction
    top_3 = get_most_likely_parents(posterior, k=3)
    print(f"\nüèÜ get_most_likely_parents(k=3):")
    for i, ps in enumerate(top_3):
        ps_str = set(ps) if ps else "{}"
        prob = posterior.parent_set_probs[ps]
        print(f"  {i+1}. {ps_str}: {prob:.3f}")
    
    # 4. Direct probability queries
    empty_prob = get_parent_set_probability(posterior, frozenset())
    both_prob = get_parent_set_probability(posterior, frozenset(['X', 'Z']))
    print(f"\nüîç get_parent_set_probability():")
    print(f"  P(parents = {{}}) = {empty_prob:.3f}")
    print(f"  P(parents = {{X,Z}}) = {both_prob:.3f}")
    
    print(f"\n‚úÖ All ParentSetPosterior features demonstrated!")
    assert posterior is not None
    assert posterior.target_variable == "Y"
    assert len(posterior.parent_set_probs) > 0
    assert isinstance(posterior.uncertainty, float)


def test_enhanced_training_workflow():
    """Test the enhanced training workflow with posterior analysis."""
    print(f"\nüéØ ENHANCED TRAINING WITH POSTERIOR ANALYSIS")
    print("=" * 60)
    
    # Create SCM and expected results
    config = create_simple_config()
    scm = create_simple_test_scm(noise_scale=1.0, target="Y")
    from causal_bayes_opt.data_structures.scm import get_variables
    variables = sorted(get_variables(scm))
    
    print(f"Testing SCM: X ‚Üí Y ‚Üê Z")
    print(f"Expected parents for Y: {{X, Z}}")
    
    # Test cases with ground truth
    test_cases = [
        ('X', frozenset()),              # Root variable
        ('Z', frozenset()),              # Root variable  
        ('Y', frozenset(['X', 'Z']))     # Target with parents
    ]
    
    net = create_parent_set_model(
        model_kwargs=config['model_kwargs'],
        max_parent_size=config['max_parent_size']
    )
    
    for target_var, expected_parents in test_cases:
        print(f"\n" + "="*40)
        print(f"üéØ TARGET: {target_var}")
        print(f"Expected: {set(expected_parents) if expected_parents else '{}'}")
        print("="*40)
        
        # Generate data and initialize
        samples = sample_from_linear_scm(scm, n_samples=config['batch_size'], seed=42)
        batch = create_training_batch(scm, samples, target_var)
        params = net.init(random.PRNGKey(42), batch['x'], variables, target_var, True)
        
        # Initial prediction
        print(f"\nüîÆ INITIAL PREDICTION:")
        initial_posterior = predict_parent_posterior(net, params, batch['x'], variables, target_var)
        initial_summary = summarize_posterior(initial_posterior)
        
        print(f"  Most likely: {initial_summary['most_likely_parents']}")
        print(f"  Confidence: {initial_summary['most_likely_probability']:.3f}")
        print(f"  Uncertainty: {initial_summary['uncertainty_bits']:.2f} bits")
        
        # Quick training
        print(f"\nüèÉ TRAINING (5 steps):")
        optimizer = create_improved_optimizer(config)
        opt_state = optimizer.init(params)
        train_step_fn = create_train_step(net, optimizer)
        
        for step in range(5):
            step_samples = sample_from_linear_scm(scm, n_samples=config['batch_size'], seed=42+step)
            step_batch = create_training_batch(scm, step_samples, target_var)
            
            params, opt_state, loss = train_step_fn(
                params, opt_state, step_batch['x'], variables, target_var, expected_parents
            )
            print(f"  Step {step}: loss = {loss:.4f}")
        
        # Final prediction with analysis
        print(f"\nüéØ FINAL PREDICTION:")
        final_posterior = predict_parent_posterior(net, params, batch['x'], variables, target_var)
        final_summary = summarize_posterior(final_posterior)
        
        print(f"  Most likely: {final_summary['most_likely_parents']}")
        print(f"  Confidence: {final_summary['most_likely_probability']:.3f}")
        print(f"  Uncertainty: {final_summary['uncertainty_bits']:.2f} bits")
        
        # Check improvement
        if final_summary['most_likely_parents'] == set(expected_parents):
            print(f"  ‚úÖ SUCCESS: Correct prediction!")
        else:
            print(f"  ‚ö†Ô∏è PARTIAL: Expected {set(expected_parents)}, got {final_summary['most_likely_parents']}")
        
        # Compare initial vs final
        print(f"\nüìä IMPROVEMENT ANALYSIS:")
        try:
            comparison = compare_posteriors(initial_posterior, final_posterior)
            print(f"  KL divergence: {comparison['symmetric_kl_divergence']:.3f}")
            print(f"  Change in uncertainty: {final_summary['uncertainty_bits'] - initial_summary['uncertainty_bits']:.2f} bits")
            
            conf_improvement = final_summary['most_likely_probability'] - initial_summary['most_likely_probability']
            print(f"  Confidence change: {conf_improvement:+.3f}")
            
        except Exception as e:
            print(f"  Comparison skipped: {e}")
        
        # Show marginal probabilities for complex cases
        if target_var == 'Y':
            marginals = get_marginal_parent_probabilities(final_posterior, variables)
            print(f"\nüìà MARGINAL PARENT PROBABILITIES:")
            for var, prob in marginals.items():
                is_true_parent = var in expected_parents
                status = "‚úÖ" if is_true_parent else "  "
                print(f"  {status} P({var} is parent) = {prob:.3f}")
    
    print(f"\n‚úÖ Enhanced training workflow completed!")


def test_ground_truth_comparison():
    """Test comparing predictions against ground truth."""
    print(f"\nüéØ GROUND TRUTH COMPARISON")
    print("=" * 40)
    
    # Create a ground truth posterior manually
    true_parent_sets = [
        frozenset(),              # Empty set - low probability
        frozenset(['X']),         # Single parent - medium
        frozenset(['Z']),         # Single parent - medium  
        frozenset(['X', 'Z'])     # Correct answer - high probability
    ]
    
    # Ground truth: Y should have parents {X, Z} with high confidence
    true_probs = jnp.array([0.05, 0.15, 0.15, 0.65])  # Correct answer gets 65%
    
    ground_truth = create_parent_set_posterior(
        target_variable="Y",
        parent_sets=true_parent_sets,
        probabilities=true_probs,
        metadata={'source': 'ground_truth', 'scm': 'X‚ÜíY‚ÜêZ'}
    )
    
    print(f"üìã GROUND TRUTH POSTERIOR:")
    gt_summary = summarize_posterior(ground_truth)
    print(f"  Most likely: {gt_summary['most_likely_parents']}")
    print(f"  Confidence: {gt_summary['most_likely_probability']:.3f}")
    print(f"  Uncertainty: {gt_summary['uncertainty_bits']:.2f} bits")
    
    # Generate a prediction from our model
    config = create_simple_config()
    scm = create_simple_test_scm(noise_scale=1.0, target="Y")
    variables = sorted(scm['variables'])
    
    net = create_parent_set_model(max_parent_size=config['max_parent_size'])
    samples = sample_from_linear_scm(scm, n_samples=config['batch_size'], seed=42)
    batch = create_training_batch(scm, samples, "Y")
    params = net.init(random.PRNGKey(42), batch['x'], variables, "Y", True)
    
    predicted = predict_parent_posterior(net, params, batch['x'], variables, "Y")
    
    print(f"\nüîÆ MODEL PREDICTION:")
    pred_summary = summarize_posterior(predicted)
    print(f"  Most likely: {pred_summary['most_likely_parents']}")
    print(f"  Confidence: {pred_summary['most_likely_probability']:.3f}")
    print(f"  Uncertainty: {pred_summary['uncertainty_bits']:.2f} bits")
    
    # Compare predictions
    print(f"\nüìä COMPARISON METRICS:")
    try:
        comparison = compare_posteriors(predicted, ground_truth)
        print(f"  KL divergence (pred ‚Üí truth): {comparison['kl_divergence_1_to_2']:.3f}")
        print(f"  KL divergence (truth ‚Üí pred): {comparison['kl_divergence_2_to_1']:.3f}")
        print(f"  Symmetric KL divergence: {comparison['symmetric_kl_divergence']:.3f}")
        print(f"  Total variation distance: {comparison['total_variation_distance']:.3f}")
        print(f"  Overlap: {comparison['overlap']:.3f}")
        
        # Interpretation
        if comparison['symmetric_kl_divergence'] < 0.5:
            print(f"  ‚úÖ GOOD: Predictions are close to ground truth")
        elif comparison['symmetric_kl_divergence'] < 1.0:
            print(f"  ‚ö†Ô∏è FAIR: Predictions are somewhat close")
        else:
            print(f"  ‚ùå POOR: Predictions are far from ground truth")
            
    except Exception as e:
        print(f"  Comparison failed: {e}")
    
    print(f"\n‚úÖ Ground truth comparison completed!")


def main():
    """Run all enhanced tests with ParentSetPosterior API."""
    print("üöÄ PARENT SET PREDICTION TESTS")
    print("=" * 60)
    print("Clean ParentSetPosterior API with rich analysis capabilities")
    print("=" * 60)
    
    try:
        # Test 1: ParentSetPosterior API features
        posterior = test_posterior_api_features()
        
        # Test 2: Enhanced training workflow  
        test_enhanced_training_workflow()
        
        # Test 3: Ground truth comparison
        test_ground_truth_comparison()
        
        print(f"\n" + "="*60)
        print(f"üéâ ALL TESTS PASSED! üéâ")
        print(f"\nKey Achievements:")
        print(f"‚úÖ ParentSetPosterior API works seamlessly")
        print(f"‚úÖ Clean, single API design")
        print(f"‚úÖ Rich analysis capabilities (summaries, marginals, comparisons)")
        print(f"‚úÖ Enhanced training workflow with uncertainty tracking")
        print(f"‚úÖ Ground truth comparison and validation metrics")
        
        print(f"\nüìö Clean API Usage:")
        print(f"  posterior = predict_parent_posterior(net, params, data, vars, 'Y')")
        print(f"  summary = summarize_posterior(posterior)")
        print(f"  marginals = get_marginal_parent_probabilities(posterior, vars)")
        print(f"  most_likely = get_most_likely_parents(posterior, k=3)")
        print(f"  comparison = compare_posteriors(pred, truth)")
        
        print(f"\nüîÆ Next Steps:")
        print(f"  ‚Ä¢ Use ParentSetPosterior in Phase 3 (Acquisition Model)")
        print(f"  ‚Ä¢ Leverage uncertainty for exploration strategies")  
        print(f"  ‚Ä¢ Build evaluation framework using comparison utilities")
        print(f"  ‚Ä¢ Integrate with GRPO for causal intervention selection")
        
        print(f"\n‚ú® ParentSetPosterior is production-ready! ‚ú®")
        
    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
