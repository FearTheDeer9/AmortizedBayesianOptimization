#!/usr/bin/env python3
"""
Adversarial Tests for Reward System Exploits

Tests designed to find potential gaming strategies and exploits in the continuous
reward system. These tests simulate adversarial scenarios where a policy might
try to game the reward signal instead of genuinely optimizing for the intended
objectives.
"""

import pytest
import jax.numpy as jnp
import pyrsistent as pyr
from unittest.mock import Mock
from typing import List, Dict, Any

# Import the continuous reward system
from causal_bayes_opt.acquisition.rewards import (
    RewardComponents,
    compute_verifiable_reward,
    _compute_optimization_reward,
    _compute_scm_objective_reward,
    _compute_structure_discovery_reward,
    _compute_parent_intervention_reward,
    _compute_exploration_bonus,
    create_default_reward_config,
    validate_reward_consistency,
)


class TestAdversarialRewardExploits:
    """Test suite for finding potential gaming exploits in the reward system."""
    
    def setup_method(self):
        """Set up adversarial test scenarios."""
        # Create base mock state
        self.mock_posterior = Mock()
        self.mock_posterior.uncertainty = 0.5
        self.mock_posterior.target_variable = "Y"
        
        self.mock_buffer = Mock()
        self.mock_buffer.samples = []
        self.mock_buffer.get_interventions.return_value = []
        
        self.base_state = Mock()
        self.base_state.current_target = "Y"
        self.base_state.step = 10
        self.base_state.best_value = 0.0
        self.base_state.posterior = self.mock_posterior
        self.base_state.buffer = self.mock_buffer
        self.base_state.marginal_parent_probs = {"X": 0.5, "Z": 0.5}
        self.base_state.uncertainty_bits = 0.5
        self.base_state.buffer_statistics = Mock()
        self.base_state.buffer_statistics.total_samples = 50


    def test_exploit_infinite_values(self):
        """Test that infinite or NaN values don't break reward computation."""
        state_before = self.base_state
        state_after = Mock()
        state_after.current_target = "Y"
        state_after.step = 11
        state_after.best_value = float('inf')  # Adversarial infinite value
        state_after.posterior = self.mock_posterior
        state_after.uncertainty_bits = 0.4
        state_after.buffer_statistics = Mock()
        state_after.buffer_statistics.total_samples = 51
        
        # Adversarial outcome with extreme values
        adversarial_outcome = pyr.m(
            values={"Y": float('inf'), "X": float('nan'), "Z": 1e100}
        )
        
        intervention = pyr.m(type="perfect", targets={"X"})
        config = create_default_reward_config()
        
        # Should handle gracefully without crashing
        rewards = compute_verifiable_reward(
            state_before, intervention, adversarial_outcome, state_after, config
        )
        
        # All rewards should be finite
        assert jnp.isfinite(rewards.optimization_reward)
        assert jnp.isfinite(rewards.structure_discovery_reward)
        assert jnp.isfinite(rewards.parent_intervention_reward)
        assert jnp.isfinite(rewards.exploration_bonus)
        assert jnp.isfinite(rewards.total_reward)


    def test_exploit_negative_uncertainty_reduction(self):
        """Test exploitation where uncertainty increases instead of decreases."""
        # Create scenario where posterior uncertainty increases (bad)
        mock_posterior_before = Mock()
        mock_posterior_before.uncertainty = 0.3  # Low uncertainty
        mock_posterior_before.target_variable = "Y"
        
        mock_posterior_after = Mock()
        mock_posterior_after.uncertainty = 0.8  # Higher uncertainty (bad!)
        mock_posterior_after.target_variable = "Y"
        
        state_before = Mock()
        state_before.current_target = "Y"
        state_before.step = 5
        state_before.posterior = mock_posterior_before
        
        state_after = Mock()
        state_after.current_target = "Y"
        state_after.step = 6
        state_after.posterior = mock_posterior_after
        
        # Should give zero or negative structure discovery reward
        structure_reward = _compute_structure_discovery_reward(
            mock_posterior_before, mock_posterior_after
        )
        
        # Should not reward uncertainty increase
        assert structure_reward == 0.0


    def test_exploit_repeated_optimal_interventions(self):
        """Test if repeatedly finding optimal interventions can be gamed."""
        # Set up SCM-objective reward scenario
        mock_mechanism = Mock()
        mock_mechanism.coefficients = {"X": 1.0}
        mock_mechanism.intercept = 0.0
        
        state_before = self.base_state
        state_before.mechanism_predictions = {"Y": mock_mechanism}
        state_before.intervention_bounds = {"X": (-5.0, 5.0)}
        
        # Optimal intervention (X=5.0 gives Y=5.0)
        optimal_outcome = pyr.m(values={"Y": 5.0, "X": 5.0})
        
        # Repeatedly apply optimal intervention - should consistently get high reward
        rewards = []
        for _ in range(10):
            reward = _compute_optimization_reward(state_before, optimal_outcome, "Y")
            rewards.append(reward)
        
        # All rewards should be high and consistent (no degradation)
        assert all(r >= 0.9 for r in rewards), f"Rewards degraded: {rewards}"
        
        # Variance should be low (consistent high performance)
        reward_variance = float(jnp.var(jnp.array(rewards)))
        assert reward_variance < 0.01, f"High variance in optimal rewards: {reward_variance}"


    def test_exploit_parent_probability_gaming(self):
        """Test if parent intervention rewards can be gamed with extreme probabilities."""
        # Extreme parent probabilities (adversarial)
        extreme_probs = {"X": 1.0, "Y": 0.0, "Z": 0.999}
        
        # Intervention targeting highest probability parent
        intervention = pyr.m(type="perfect", targets={"X"})
        reward = _compute_parent_intervention_reward(intervention, extreme_probs)
        
        # Should still be bounded to [0, 1]
        assert 0.0 <= reward <= 1.0
        assert reward == 1.0  # Should match the probability
        
        # Test with invalid probabilities
        invalid_probs = {"X": 1.5, "Y": -0.3, "Z": float('inf')}
        invalid_reward = _compute_parent_intervention_reward(intervention, invalid_probs)
        
        # Should handle gracefully
        assert jnp.isfinite(invalid_reward)
        assert 0.0 <= invalid_reward <= 1.0


    def test_exploit_exploration_frequency_manipulation(self):
        """Test if exploration bonus can be gamed by manipulating intervention history."""
        # Create adversarial intervention history with strategic patterns
        adversarial_history = []
        
        # Strategy: alternate between two interventions to game frequency
        for i in range(100):
            if i % 2 == 0:
                adversarial_history.append((pyr.m(targets={"X"}), Mock()))
            else:
                adversarial_history.append((pyr.m(targets={"Z"}), Mock()))
        
        self.mock_buffer.get_interventions.return_value = adversarial_history
        
        # New intervention on alternating pattern
        new_intervention_x = pyr.m(type="perfect", targets={"X"})
        new_intervention_z = pyr.m(type="perfect", targets={"Z"})
        
        bonus_x = _compute_exploration_bonus(new_intervention_x, self.mock_buffer, 0.1)
        bonus_z = _compute_exploration_bonus(new_intervention_z, self.mock_buffer, 0.1)
        
        # Both should get low bonuses due to high frequency
        assert bonus_x <= 0.05  # Should be low due to high frequency
        assert bonus_z <= 0.05  # Should be low due to high frequency
        
        # Novel intervention should still get higher bonus
        novel_intervention = pyr.m(type="perfect", targets={"W"})
        novel_bonus = _compute_exploration_bonus(novel_intervention, self.mock_buffer, 0.1)
        assert novel_bonus > max(bonus_x, bonus_z)


    def test_exploit_mechanism_prediction_gaming(self):
        """Test if mechanism predictions can be gamed with adversarial coefficients."""
        # Create adversarial mechanism with extreme coefficients
        adversarial_mechanism = Mock()
        adversarial_mechanism.coefficients = {
            "X": 1e6,    # Extremely large coefficient
            "Z": -1e6    # Extremely large negative coefficient
        }
        adversarial_mechanism.intercept = 1e9  # Huge intercept
        
        state_before = self.base_state
        state_before.mechanism_predictions = {"Y": adversarial_mechanism}
        state_before.intervention_bounds = {"X": (-1e3, 1e3), "Z": (-1e3, 1e3)}
        
        # Try to exploit with extreme target value
        extreme_outcome = pyr.m(values={"Y": 1e15, "X": 1000, "Z": -1000})
        
        # Should handle gracefully and return bounded reward
        reward = _compute_optimization_reward(state_before, extreme_outcome, "Y")
        
        assert jnp.isfinite(reward)
        assert 0.0 <= reward <= 1.0


    def test_exploit_zero_variance_gaming_detection(self):
        """Test if the gaming detection can be fooled with near-zero variance."""
        # Create reward pattern with minimal but non-zero variance
        gaming_rewards = []
        base_value = 0.5
        tiny_variance = 1e-8
        
        for i in range(50):
            # Add tiny variations to try to fool variance detection
            noise = tiny_variance * (i % 3 - 1)
            gaming_rewards.append(RewardComponents(
                optimization_reward=base_value + noise,
                structure_discovery_reward=base_value + noise * 2,
                parent_intervention_reward=base_value + noise * 3,
                exploration_bonus=base_value + noise * 4,
                total_reward=(base_value + noise) * 4,
                metadata=pyr.m()
            ))
        
        validation_result = validate_reward_consistency(gaming_rewards)
        
        # Should still detect this as suspicious due to very low variance
        if not validation_result['valid']:
            issues = validation_result['gaming_issues']
            assert any("variance" in issue.lower() for issue in issues)


    def test_exploit_state_inconsistency(self):
        """Test exploitation through inconsistent state transitions."""
        # Create inconsistent states
        state_before = Mock()
        state_before.current_target = "Y"
        state_before.step = 10
        
        state_after = Mock()
        state_after.current_target = "X"  # Different target (inconsistent!)
        state_after.step = 9  # Step went backwards (inconsistent!)
        
        intervention = pyr.m(type="perfect", targets={"Z"})
        outcome = pyr.m(values={"Y": 1.0, "X": 1.0, "Z": 1.0})
        config = create_default_reward_config()
        
        # Should raise error for inconsistent states
        with pytest.raises(ValueError, match="target mismatch"):
            compute_verifiable_reward(
                state_before, intervention, outcome, state_after, config
            )


    def test_exploit_missing_target_variable(self):
        """Test exploitation when target variable is missing from outcome."""
        state_before = self.base_state
        state_after = Mock()
        state_after.current_target = "Y"
        state_after.step = 11
        state_after.posterior = self.mock_posterior
        state_after.uncertainty_bits = 0.4
        state_after.buffer_statistics = Mock()
        state_after.buffer_statistics.total_samples = 51
        
        # Outcome missing target variable
        incomplete_outcome = pyr.m(values={"X": 1.0, "Z": 1.0})  # No Y!
        
        intervention = pyr.m(type="perfect", targets={"X"})
        config = create_default_reward_config()
        
        # Should handle gracefully
        rewards = compute_verifiable_reward(
            state_before, intervention, incomplete_outcome, state_after, config
        )
        
        # Optimization reward should be zero due to missing target
        assert rewards.optimization_reward == 0.0
        # Other components should still work
        assert jnp.isfinite(rewards.total_reward)


    def test_exploit_weight_manipulation(self):
        """Test if extreme weight configurations can break the system."""
        # Adversarial weight configuration should be rejected
        with pytest.raises(ValueError, match="Invalid reward configuration"):
            adversarial_config = create_default_reward_config(
                optimization_weight=-1000.0,  # Negative weight
                structure_weight=1e10,        # Huge weight
                parent_weight=float('inf'),   # Infinite weight
                exploration_weight=float('nan')  # NaN weight
            )
        
        # This is expected behavior - validation should catch invalid weights


    def test_exploit_temporal_consistency(self):
        """Test if temporal reward consistency can be gamed."""
        # Create scenario where rewards improve artificially over time
        reward_history = []
        
        # Simulate gradually increasing rewards that might indicate gaming
        for i in range(20):
            artificial_boost = i * 0.05  # Linearly increasing
            reward_history.append(RewardComponents(
                optimization_reward=0.1 + artificial_boost,
                structure_discovery_reward=0.1 + artificial_boost,
                parent_intervention_reward=0.1 + artificial_boost,
                exploration_bonus=0.1 + artificial_boost,
                total_reward=(0.1 + artificial_boost) * 4,
                metadata=pyr.m()
            ))
        
        validation_result = validate_reward_consistency(reward_history)
        
        # This pattern might or might not be flagged as suspicious
        # The key is that the system doesn't crash with temporal patterns
        assert 'statistics' in validation_result
        assert jnp.isfinite(validation_result['statistics']['mean_total_reward'])


    def test_exploit_buffer_overflow_simulation(self):
        """Test behavior with extremely large intervention histories."""
        # Simulate very large intervention history (potential memory attack)
        large_history = []
        for i in range(1000):  # Large but not infinite
            large_history.append((pyr.m(targets={"X"}), Mock()))
        
        self.mock_buffer.get_interventions.return_value = large_history
        
        intervention = pyr.m(type="perfect", targets={"X"})
        
        # Should handle large histories efficiently
        bonus = _compute_exploration_bonus(intervention, self.mock_buffer, 0.1)
        
        # Should still return valid result
        assert jnp.isfinite(bonus)
        assert 0.0 <= bonus <= 0.1  # Should be very low due to high frequency


if __name__ == "__main__":
    pytest.main([__file__, "-v"])